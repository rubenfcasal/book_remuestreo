% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Técnicas de Remuestreo},
  pdfauthor={Ricardo Cao Abad (rcao@udc.es) y Rubén Fernández Casal (rfcasal@udc.es)},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{amsthm}
\usepackage[a4paper, top=3.25cm, bottom=2.5cm, left=3cm, right=2.5cm]{geometry}
%\usepackage{fontspec}
%\setmainfont{Arial}
% Espacio después de teorema
% https://tex.stackexchange.com/questions/37797/theorem-environment-line-break-after-label
\newtheoremstyle{break}
  {\topsep}{\topsep}%
  {\itshape}{}%
  {\bfseries}{}%
  {\newline}%
  {}%

\theoremstyle{break}

\ifxetex
  \usepackage{polyglossia}
  \setmainlanguage{spanish}
  % Tabla en lugar de cuadro
  \gappto\captionsspanish{\renewcommand{\tablename}{Tabla}
          \renewcommand{\listtablename}{Índice de tablas}}

\else
  \usepackage[spanish,es-tabla]{babel}
\fi
\makeatletter
\def\thm@space@setup{
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{Técnicas de Remuestreo}
\author{Ricardo Cao Abad (\href{mailto:rcao@udc.es}{\nolinkurl{rcao@udc.es}}) y Rubén Fernández Casal (\href{mailto:rfcasal@udc.es}{\nolinkurl{rfcasal@udc.es}})}
\date{2021-11-25}

\usepackage{amsthm}
\newtheorem{theorem}{Teorema}[chapter]
\newtheorem{lemma}{Lema}[chapter]
\newtheorem{corollary}{Corolario}[chapter]
\newtheorem{proposition}{Proposición}[chapter]
\newtheorem{conjecture}{Algoritmo}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definición}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Ejemplo}[chapter]
\theoremstyle{definition}
\newtheorem{exercise}{Ejercicio}[chapter]
\theoremstyle{definition}
\newtheorem{hypothesis}{Hypothesis}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Nota: }
\newtheorem*{solution}{Solución}
\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{pruxf3logo}{%
\chapter*{Prólogo}\label{pruxf3logo}}
\addcontentsline{toc}{chapter}{Prólogo}

Este libro contiene los apuntes de la asignatura de \href{http://eamo.usc.es/pub/mte/index.php/es/?option=com_content\&view=article\&id=2202\&idm=22\&a\%C3\%B1o=2019}{Técnicas de Remuestreo} del \href{http://eio.usc.es/pub/mte}{Máster en Técnicas Estadísticas}.

Este libro ha sido escrito en \href{http://rmarkdown.rstudio.com}{R-Markdown} empleando el paquete \href{https://bookdown.org/yihui/bookdown/}{\texttt{bookdown}} y está disponible en el repositorio Github: \href{https://github.com/rubenfcasal/book_remuestreo}{rubenfcasal/book\_remuestreo}.
Se puede acceder a la versión en línea a través del siguiente enlace:

\url{https://rubenfcasal.github.io/book_remuestreo}.

donde puede descargarse en formato \href{https://rubenfcasal.github.io/book_remuestreo/book_remuestreo.pdf}{pdf}.

Para ejecutar los ejemplos mostrados en el libro será necesario tener instalados los siguientes paquetes:
\href{https://CRAN.R-project.org/package=boot}{\texttt{boot}}, \href{https://CRAN.R-project.org/package=bootstrap}{\texttt{bootstrap}}, \href{https://CRAN.R-project.org/package=survival}{\texttt{survival}}, \href{https://CRAN.R-project.org/package=forecast}{\texttt{forecast}}, \href{https://CRAN.R-project.org/package=MASS}{\texttt{MASS}}, \href{https://CRAN.R-project.org/package=sm}{\texttt{sm}}, \href{https://CRAN.R-project.org/package=snow}{\texttt{snow}}.
Por ejemplo mediante el comando:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"boot"}\NormalTok{, }\StringTok{"bootstrap"}\NormalTok{, }\StringTok{"survival"}\NormalTok{, }\StringTok{"forecast"}\NormalTok{, }\StringTok{"MASS"}\NormalTok{, }\StringTok{"sm"}\NormalTok{, }\StringTok{"snow"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Para generar el libro (compilar) serán necesarios paquetes adicionales,
para lo que se recomendaría consultar el libro de \href{https://rubenfcasal.github.io/bookdown_intro}{``Escritura de libros con bookdown''} en castellano.

Este obra está bajo una licencia de \href{https://creativecommons.org/licenses/by-nc-nd/4.0/deed.es_ES}{Creative Commons Reconocimiento-NoComercial-SinObraDerivada 4.0 Internacional}
(esperamos poder liberarlo bajo una licencia menos restrictiva más adelante\ldots).

\includegraphics[width=1.22in]{by-nc-nd-88x31}

\hypertarget{intro}{%
\chapter{Motivación del principio Bootstrap}\label{intro}}

Etimología: bootstrap = cinta de la bota (oreja lateral para calzarse
las botas). Modismo anglosajón: to pull oneself up by one's bootstraps.

\hypertarget{introducciuxf3n}{%
\section{Introducción}\label{introducciuxf3n}}

El bootstrap es un procedimiento estadístico que sirve para aproximar la
distribución en el muestreo (normalmente) de un estadístico. Para ello
procede mediante remuestreo, es decir, obteniendo muestras mediante
algún procedimiento aleatorio que utilice la muestra original.

Su ventaja principal es que no requiere hipótesis sobre el mecanismo
generador de los datos. Sí las requiere, aunque suelen ser más
relajadas, para obtener propiedades asintóticas del mismo. Por otra
parte, su implementación en ordenador suele ser sencilla, en comparación
con otros métodos. Su principal inconveniente es la necesidad de
computación intensiva, debido a la fuerza bruta del método de Monte
Carlo. Con la capacidad computacional actual, esta mayor carga
computacional del bootstrap no suele ser un problema hoy en día. En
raras ocasiones el bootstrap no necesita del uso de técnicas de Monte
Carlo.

\hypertarget{breve-nota-histuxf3rica}{%
\subsection{Breve nota histórica}\label{breve-nota-histuxf3rica}}

Precursores teóricos remotos:

\begin{itemize}
\item
  Laplace (1810). Teoría límite de primer orden.
\item
  Chebychev (final siglo XIX). Teoría límite de segundo orden.
\end{itemize}

Primeras contribuciones:

\begin{itemize}
\item
  Hubback (1878-1968). Esquemas de muestreo espacial para ensayos
  agrícolas.
\item
  Mahalanobis (años 1930 y segunda guerra mundial). Precursor del
  bootstrap por bloques.
\end{itemize}

Otras contribuciones:

\begin{itemize}
\item
  Gurney, McCarthy, Hartigan (años 1960, 1970). Métodos de
  half-sampling para estimación de varianzas (U.S. Bureau of the
  Census).
\item
  Maritz, Jarret, Simon (años 1970, 1980). Métodos de permutaciones
  relacionados con el bootstrap.
\end{itemize}

En la actualidad:

\begin{itemize}
\item
  Bradley Efron (Stanford University, 1979). Creador oficial del
  método. Acuñó su nombre. Fusionó la potencia de Monte Carlo con la
  resolución de problemas planteados de forma muy general.
\item
  Peter Hall (1951-2016). Fue uno de los estadísticos contemporáneos
  más prolíficos. Dedicó al bootstrap gran parte de su producción a
  partir de los años 1980.
\end{itemize}

\hypertarget{paradigma-inferencial-y-anuxe1logo-bootstrap}{%
\subsection{Paradigma inferencial y análogo bootstrap}\label{paradigma-inferencial-y-anuxe1logo-bootstrap}}

\textbf{\emph{Paradigma inferencial}}

Suponemos que \(\mathbf{X}=\left( X_1,\ldots ,X_n \right)\) es una m.a.s.
de una población con distribución \(F\) y que
estamos interesados en hacer inferencia sobre \(\theta =\theta \left(F \right)\).
Para ello nos gustaría conocer la distribución en el muestreo de
\(R\left( \mathbf{X},F \right)\), cierto estadístico función
de la muestra y de la distribución poblacional.
Por ejemplo:
\[R=R\left( \mathbf{X},F \right) =\theta \left( F_n \right) 
-\theta \left( F \right) = \hat \theta - \theta,\]
siendo \(F_n\) la función de distribución empírica.

A veces podemos calcular directamente la distribución de \(R\left( \mathbf{X},F \right)\),
aunque suele depender de cantidades poblacionales,
no conocidas en la práctica.
Por ejemplo, bajo normalidad \(X_i \overset{i.i.d.}{\sim} \mathcal{N}\left( \mu ,\sigma^2 \right)\), si estamos interesados en
\[\theta \left( F \right) =\mu =\int x~dF\left( x \right) =\int xf\left( x \right) ~dx\]
como \(\theta \left( F_n \right) = \int x~dF_n\left( x \right) = \sum \frac{1}{n}X_i = \bar{X}\), podríamos considerar el estadístico:
\[R=R\left( \mathbf{X},F \right) = \bar{X} - \mu \sim \mathcal{N}\left( 0 ,\frac{\sigma^2}{n} \right).\]
Aunque en la práctica la varianza no es normalmente conocida y habría que aproximarla
(sería preferible considerar como estadístico la media estudentizada).

Otras veces sólo podemos llegar a aproximar la distribución de
\(R\left( \mathbf{X},F \right)\) cuando \(n \rightarrow \infty\).
Por ejemplo, cuando estamos interesados en la media pero desconocemos la
distribución de los datos.

\textbf{\emph{Análogo bootstrap}}

El primer paso es reemplazar la distribución poblacional (desconocida) \(F\) por una
estimación, \(\hat{F}\), de la misma. Por ejemplo, podríamos considerar la
distribución empírica \(\hat{F}=F_n\) (bootstrap uniforme; Sección \ref{intro-unif}),
o una aproximación paramétrica \(\hat{F}=F_{\hat \theta}\) (bootstrap paramétrico; Sección \ref{modunif-boot-par}).

Como ejemplo ilustrativo consideramos los datos simulados {[}Figura \ref{fig:muestra-sim}{]}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{muestra }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\FunctionTok{hist}\NormalTok{(muestra, }\AttributeTok{freq =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{),}
     \AttributeTok{main =} \StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{, }\AttributeTok{xlab =} \StringTok{\textquotesingle{}x\textquotesingle{}}\NormalTok{, }\AttributeTok{ylab =} \StringTok{\textquotesingle{}densidad\textquotesingle{}}\NormalTok{)}
\FunctionTok{curve}\NormalTok{(dnorm, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{, }\AttributeTok{add =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[!htb]

{\centering \includegraphics[width=0.7\linewidth]{01-intro_files/figure-latex/muestra-sim-1} 

}

\caption{Distribución de la muestra simulada.}\label{fig:muestra-sim}
\end{figure}

Como aproximación de la distribución poblacional, desconocida en la práctica,
siempre podemos considerar la distribución empírica
(o una versión suavizada: bootstrap suavizado; Sección \ref{modunif-boot-suav}).
Alternativamente podríamos asumir un modelo paramétrico y estimar los parámetros a partir de la muestra {[}Figura \ref{fig:muestra-sim-aprox}{]}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Distribución bootstrap uniforme}
\FunctionTok{curve}\NormalTok{(}\FunctionTok{ecdf}\NormalTok{(muestra)(x), }\AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{), }\AttributeTok{ylab =} \StringTok{"F(x)"}\NormalTok{, }\AttributeTok{type =} \StringTok{"s"}\NormalTok{)}
\CommentTok{\# Distribución bootstrap paramétrico (asumiendo normalidad)}
\FunctionTok{curve}\NormalTok{(}\FunctionTok{pnorm}\NormalTok{(x, }\FunctionTok{mean}\NormalTok{(muestra), }\FunctionTok{sd}\NormalTok{(muestra)), }\AttributeTok{lty =} \DecValTok{2}\NormalTok{, }\AttributeTok{add =} \ConstantTok{TRUE}\NormalTok{)}
\CommentTok{\# Distribución teórica}
\FunctionTok{curve}\NormalTok{(pnorm, }\AttributeTok{lty =} \DecValTok{3}\NormalTok{, }\AttributeTok{add =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}\StringTok{"bottomright"}\NormalTok{, }\AttributeTok{legend =} \FunctionTok{c}\NormalTok{(}\StringTok{"Empírica"}\NormalTok{, }\StringTok{"Aprox. paramétrica"}\NormalTok{, }\StringTok{"Teórica"}\NormalTok{), }\AttributeTok{lty =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[!htb]

{\centering \includegraphics[width=0.7\linewidth]{01-intro_files/figure-latex/muestra-sim-aprox-1} 

}

\caption{Distribución teórica de la muestra simulada y distintas aproximaciones.}\label{fig:muestra-sim-aprox}
\end{figure}

A partir de la aproximación \(\hat{F}\) podríamos generar, condicionalmente a la muestra observada,
remuestras
\[\mathbf{X}^{\ast}=\left( X_1^{\ast},\ldots ,X_n^{\ast} \right)\]
con distribución \(X_i^{\ast} \sim \hat{F}\), que demoninaremos remuestras bootstrap.
Por lo que podemos hablar de la distribución en el remuestreo de
\[R^{\ast}=R\left( \mathbf{X}^{\ast},\hat{F} \right),\]
llamada distribución bootstrap.

La idea original (Efron, 1979) es que la distribución
de \(\hat{\theta}_{b}^{\ast }\) en torno a \(\hat{\theta}\) aproxima la
distribución de \(\hat{\theta}\) en torno a \(\theta\).
Por tanto se pretende aproximar la distribución en el muestreo de \(R\) por la
distribución bootstrap de \(R^{\ast}\).

En raras ocasiones la distribución bootstrap de \(R^{\ast}\) es
calculable directamente, pero siempre suele poder aproximarse por
Monte Carlo.

\hypertarget{intro-implementacion}{%
\subsection{Implementación en la práctica}\label{intro-implementacion}}

En el caso i.i.d., si empleamos como aproximación la distribución empírica \(\hat{F}=F_n\),
la generación de las muestras bootstrap puede hacerse mediante remuestreo
(manteniendo el tamaño muestral). Habría que simular una muestra de tamaño \(n\)
de una variable aleatoria discreta que toma los valores
\(X_1,\ldots ,X_n\) todos ellos con probabilidad \(\frac{1}{n}\):

\begin{itemize}
\tightlist
\item
  Para cada \(i=1,\ldots, n\),
  \(P^{\ast}\left( X_i^{\ast}=X_j \right) = \frac{1}{n}\), \(j=1,\ldots ,n\).
\end{itemize}

Existen multitud de algoritmos para simular variables discretas, pero en
este caso de equiprobabilidad hay un procedimiento muy eficiente (método
de la transformación cuantil con búsqueda directa) que se reduce a
simular un número aleatorio \(U\), con distribución \(\mathcal{U}\left( 0,1 \right)\),
y hacer \(X^{\ast}=X_{\left\lfloor nU\right\rfloor +1}\), donde \(\left\lfloor x\right\rfloor\)
representa la parte entera de \(x\), es decir, el mayor número entero que
sea menor o igual que \(x\).
Empleando ese método, el procedimiento para generar la muestra bootstrap sería:

\begin{itemize}
\tightlist
\item
  Para cada \(i=1,\ldots ,n\)
  generar \(U_i\sim \mathcal{U}\left( 0,1 \right)\) y
  hacer \(X_i^{\ast}=X_{\left\lfloor nU_i\right\rfloor +1}\).
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{n }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(muestra)}
\NormalTok{u }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(n)}
\NormalTok{muestra\_boot }\OtherTok{\textless{}{-}}\NormalTok{ muestra[}\FunctionTok{floor}\NormalTok{(n}\SpecialCharTok{*}\NormalTok{u) }\SpecialCharTok{+} \DecValTok{1}\NormalTok{]}
\FunctionTok{head}\NormalTok{(muestra\_boot)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -0.1557955 -0.0593134 -1.0441346 -0.5425200  0.9189774  0.2670988
\end{verbatim}

En \texttt{R} es recomendable\footnote{De esta forma se evitan posibles problemas numéricos
  al emplear el método de la transformación cuantil cuando \(n\) es extremadamente grande
  (e.g.~\url{https://stat.ethz.ch/pipermail/r-devel/2018-September/076817.html}).}
emplear la función \texttt{sample} para generar muestras aleatorias con reemplazamiento
del conjunto de datos original:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{muestra\_boot }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(muestra, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{head}\NormalTok{(muestra\_boot)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -1.4707524  0.7685329  0.3876716 -0.6887557  0.9189774  1.3586796
\end{verbatim}

En el caso multidimensional, cuando trabajamos con un conjunto de datos
con múltiples variables,
podríamos emplear un procedimiento análogo, a partir de remuestras del
vector de índices. Por ejemplo:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(iris)}
\FunctionTok{str}\NormalTok{(iris)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    150 obs. of  5 variables:
##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...
##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...
##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...
##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...
##  $ Species     : Factor w/ 3 levels "setosa","versicolor",..: 1 1 1 1 1 1 1 1 1 1 ...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(iris)}
\CommentTok{\# i\_boot \textless{}{-} floor(n*runif(n)) + 1}
\CommentTok{\# i\_boot \textless{}{-} sample.int(n, replace = TRUE)}
\NormalTok{i\_boot }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(n, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{data\_boot }\OtherTok{\textless{}{-}}\NormalTok{ iris[i\_boot, ]}
\FunctionTok{str}\NormalTok{(data\_boot)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    150 obs. of  5 variables:
##  $ Sepal.Length: num  6.9 7.2 4.3 6.4 5.7 5.8 7.2 5.6 5.8 5.4 ...
##  $ Sepal.Width : num  3.1 3.2 3 3.2 4.4 4 3 2.9 2.7 3.9 ...
##  $ Petal.Length: num  5.4 6 1.1 5.3 1.5 1.2 5.8 3.6 5.1 1.3 ...
##  $ Petal.Width : num  2.1 1.8 0.1 2.3 0.4 0.2 1.6 1.3 1.9 0.4 ...
##  $ Species     : Factor w/ 3 levels "setosa","versicolor",..: 3 3 1 3 1 1 3 2 3 1 ...
\end{verbatim}

Esta forma de proceder es la que emplea por defecto el paquete \texttt{boot} que
describiremos más adelante (Sección \ref{intro-pkgboot}).

\begin{example}[Inferencia sobre la media con varianza conocida]
\protect\hypertarget{exm:media-dt-conocida}{}{\label{exm:media-dt-conocida} \iffalse (Inferencia sobre la media con varianza conocida) \fi{} }
\vspace{0.5cm}

Hemos observado 15 tiempos de vida de microorganismos:
0.143, 0.182, 0.256, 0.260, 0.270, 0.437, 0.509,
0.611, 0.712, 1.04, 1.09, 1.15, 1.46, 1.88, 2.08.
A partir de los cuales queremos
obtener una estimación por intervalo de confianza de su vida media,
suponiendo que la desviación típica es conocida e igual a 0.6
(en el Capítulo \ref{icboot} se tratará con más detalle la construcción de intervalos de confianza).
\end{example}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{muestra }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{0.143}\NormalTok{, }\FloatTok{0.182}\NormalTok{, }\FloatTok{0.256}\NormalTok{, }\FloatTok{0.26}\NormalTok{, }\FloatTok{0.27}\NormalTok{, }\FloatTok{0.437}\NormalTok{, }\FloatTok{0.509}\NormalTok{, }
    \FloatTok{0.611}\NormalTok{, }\FloatTok{0.712}\NormalTok{, }\FloatTok{1.04}\NormalTok{, }\FloatTok{1.09}\NormalTok{, }\FloatTok{1.15}\NormalTok{, }\FloatTok{1.46}\NormalTok{, }\FloatTok{1.88}\NormalTok{, }\FloatTok{2.08}\NormalTok{)}
\NormalTok{sigma }\OtherTok{\textless{}{-}} \FloatTok{0.6}
\FunctionTok{summary}\NormalTok{(muestra)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  0.1430  0.2650  0.6110  0.8053  1.1200  2.0800
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sd}\NormalTok{(muestra)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.6237042
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(muestra)}
\FunctionTok{rug}\NormalTok{(muestra)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[!htb]

{\centering \includegraphics[width=0.7\linewidth]{01-intro_files/figure-latex/microorganismos-1} 

}

\caption{Distribución del tiempo de vida de microorganismos.}\label{fig:microorganismos}
\end{figure}

{[}Figura \ref{fig:microorganismos}{]}

\textbf{\emph{Contexto clásico}}

Suponemos que los datos \(\mathbf{X}=\left( X_1,\ldots ,X_n \right)\) son una m.a.s.
de una población con distribución \(F\), con \(\mu\) desconocida y \(\sigma\) conocida,
y que estamos interesados en hacer inferencia sobre:
\[\theta \left( F \right) =\mu =\int x~dF\left( x \right)\]
Para ello, un estadístico adecuado para este caso es:
\[R=R\left( \mathbf{X},F \right) =\sqrt{n}\frac{\bar{X}-\mu }{\sigma},\]
con \(\theta \left( F_n \right) =\int x~dF_n\left( x \right) = \bar{X}\).

Bajo normalidad \(\left( X\sim \mathcal{N}\left( \mu ,\sigma^2 \right) \right)\),
\(R\sim N\left( 0,1 \right)\). Si \(F\) no es normal, tan sólo sabemos que,
bajo ciertas condiciones,
\(R\overset{d}{\rightarrow }\mathcal{N}\left( 0, 1 \right)\).

A partir de esta última aproximación, se obtiene el intervalo de
confianza asintótico (de nivel \(1-\alpha\)) para la media \(\mu\):
\[\hat{IC}_{1-\alpha}\left(  \mu\right)  = 
\left(  \overline{X}-z_{1-\alpha/2}\dfrac{\sigma}{\sqrt{n}},\ \overline{X} 
+ z_{1-\alpha/2}\dfrac{\sigma}{\sqrt{n}} \right).\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{alfa }\OtherTok{\textless{}{-}} \FloatTok{0.05}
\NormalTok{x\_barra }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(muestra)}
\NormalTok{z }\OtherTok{\textless{}{-}} \FunctionTok{qnorm}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{)}
\NormalTok{n }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(muestra)}
\NormalTok{ic\_inf }\OtherTok{\textless{}{-}}\NormalTok{ x\_barra }\SpecialCharTok{{-}}\NormalTok{ z}\SpecialCharTok{*}\NormalTok{sigma}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(n)}
\NormalTok{ic\_sup }\OtherTok{\textless{}{-}}\NormalTok{ x\_barra }\SpecialCharTok{+}\NormalTok{ z}\SpecialCharTok{*}\NormalTok{sigma}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(n)}
\NormalTok{IC }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(ic\_inf, ic\_sup)}
\NormalTok{IC}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.501697 1.108970
\end{verbatim}

\textbf{\emph{Contexto bootstrap}}

Consideramos la función de distribución empírica \(\hat{F}=F_n\)
como aproximación de la distribución poblacional (bootstrap uniforme).
Para aproximar la distribución bootstrap del estadístico por Monte Carlo,
se generan \(B=1000\) muestras bootstrap
\(\mathbf{X}^{\ast (b)}=\left( X_1^{\ast (b)},\ldots ,X_n^{\ast (b)} \right)\)
de forma que
\(P^{\ast (b)}\left( X_i^{\ast}=X_j \right) = \frac{1}{n}\), \(j=1,\ldots ,n\), para \(i=1,\ldots, n\) y \(b=1,\ldots, B\).
A partir de las cuales se obtienen las \(B\) réplicas bootstrap del estadístico:
\[R^{\ast (b)}=R\left( \mathbf{X}^{\ast (b)},\hat{F} \right) =\sqrt{n}\frac{
\bar{X}^{\ast  (b)}-\bar{X}}{\sigma }, \ b=1,\ldots, B, \]
con \(\bar{X}^{\ast (b)} = \frac{1}{n}\sum X_i^{\ast (b)}\).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{B }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{estadistico\_boot }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(B)}
\ControlFlowTok{for}\NormalTok{ (k }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{B) \{}
\NormalTok{    remuestra }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(muestra, n, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{    x\_barra\_boot }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(remuestra)}
\NormalTok{    estadistico\_boot[k] }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(n) }\SpecialCharTok{*}\NormalTok{ (x\_barra\_boot }\SpecialCharTok{{-}}\NormalTok{ x\_barra)}\SpecialCharTok{/}\NormalTok{sigma}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Las características de interés de la distribución en el muestreo de \(R\)
se aproximan por las correspondientes de la distribución bootstrap de \(R^{\ast}\).
En este caso nos interesa aproximar los puntos críticos \(x_{\alpha /2}\) y
\(x_{1-\alpha /2}\), tales que:
\[P\left( x_{\alpha /2} < R < x_{1-\alpha /2} \right) = 1-\alpha.\]
Para lo que podemos emplear los cuantiles muestrales\footnote{
  Se podrían considerar distintos estimadores del cuantil \(x_{\alpha}\)
  (ver p.e. la ayuda de la función \texttt{quantile()}).
  Si empleamos directamente la distribución empírica, el cuantil se
  correspondería con la observación ordenada en la posición \(B \alpha\)
  (se suele hacer una interpolación lineal si este valor no es entero),
  lo que equivale a emplear la función \texttt{quantile()} de \texttt{R} con el parámetro
  \texttt{type\ =\ 1}. Esta función considera por defecto la posición
  \(1 + (B - 1) \alpha\) (\texttt{type\ =\ 7}).
  En el libro de Davison y Hinkley (1997), y en el paquete \texttt{boot}, se emplea \((B + 1) \alpha\) (equivalente a \texttt{type\ =\ 6}; lo que justifica que
  consideren habitualmente 99, 199 ó 999 réplicas bootstrap).}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Empleando la distribución empírica del estadístico bootstrap: }
\NormalTok{estadistico\_boot\_ordenado }\OtherTok{\textless{}{-}} \FunctionTok{sort}\NormalTok{(estadistico\_boot)}
\NormalTok{indice\_inf }\OtherTok{\textless{}{-}} \FunctionTok{floor}\NormalTok{(B }\SpecialCharTok{*}\NormalTok{ alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{)}
\NormalTok{indice\_sup }\OtherTok{\textless{}{-}} \FunctionTok{floor}\NormalTok{(B }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{))}
\NormalTok{pto\_crit }\OtherTok{\textless{}{-}}\NormalTok{ estadistico\_boot\_ordenado[}\FunctionTok{c}\NormalTok{(indice\_inf, indice\_sup)]}
\CommentTok{\# Empleando la función \textasciigrave{}quantile\textasciigrave{}:}
\CommentTok{\# pto\_crit \textless{}{-} quantile(estadistico\_boot, c(alfa/2, 1 {-} alfa/2), type = 1)}
\NormalTok{pto\_crit }\OtherTok{\textless{}{-}} \FunctionTok{quantile}\NormalTok{(estadistico\_boot, }\FunctionTok{c}\NormalTok{(alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{))}
\NormalTok{pto\_crit}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      2.5%     97.5% 
## -1.918622  2.075984
\end{verbatim}

A partir de los cuales obtenemos la correspondiente estimación por IC
boostrap:
\[\hat{IC}^{boot}_{1-\alpha}\left(  \mu\right)  = 
\left(  \overline{X}-x_{1-\alpha/2}\dfrac{\sigma}{\sqrt{n}},\ \overline{X} 
- x_{\alpha/2}\dfrac{\sigma}{\sqrt{n}} \right).\]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Construcción del IC}
\NormalTok{ic\_inf\_boot }\OtherTok{\textless{}{-}}\NormalTok{ x\_barra }\SpecialCharTok{{-}}\NormalTok{ pto\_crit[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ sigma}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(n)}
\NormalTok{ic\_sup\_boot }\OtherTok{\textless{}{-}}\NormalTok{ x\_barra }\SpecialCharTok{{-}}\NormalTok{ pto\_crit[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ sigma}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(n)}
\NormalTok{IC\_boot }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(ic\_inf\_boot, ic\_sup\_boot)}
\FunctionTok{names}\NormalTok{(IC\_boot) }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\DecValTok{100}\SpecialCharTok{*}\FunctionTok{c}\NormalTok{(alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{), }\StringTok{"\%"}\NormalTok{) }\CommentTok{\# rev(names(IC\_boot))}
\NormalTok{IC\_boot}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      2.5%     97.5% 
## 0.4837233 1.1025650
\end{verbatim}

Nótese que este intervalo de confianza no está centrado en la media,
al contrario que el obtenido con la aproximación tradicional.
Aunque en este caso no se observan grandes diferencias ya que
la distribución bootstrap obtenida es muy similar a la aproximación normal
(ver Figura \ref{fig:estad-boot}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(estadistico\_boot, }\AttributeTok{freq =} \ConstantTok{FALSE}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(}\FunctionTok{density}\NormalTok{(estadistico\_boot))}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{v =}\NormalTok{ pto\_crit)}
\FunctionTok{curve}\NormalTok{(dnorm, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{, }\AttributeTok{add =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{v =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{z, z), }\AttributeTok{lty =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[!htb]

{\centering \includegraphics[width=0.7\linewidth]{01-intro_files/figure-latex/estad-boot-1} 

}

\caption{Distribución del estadístico boostrap y aproximaciones de los cuantiles. Con línea discontinua se muestra la distribución normal asintótica.}\label{fig:estad-boot}
\end{figure}

\hypertarget{intro-unif}{%
\section{El Bootstrap uniforme}\label{intro-unif}}

Como ya se comentó anteriormente el bootstrap uniforme es aquel en el que
se reemplaza la distribución poblacional (desconocida) por la distribución
empírica:
\[F_n\left( x \right) =\frac{1}{n}\sum_{i=1}^{n}\mathbf{1}\left\{ X_i\leq x\right\}.\]

Es decir \(\hat{F}=F_n\) y, por lo tanto,
\(R^{\ast}=R\left( \mathbf{X}^{\ast},F_n \right)\).

Conviene recordar algunas propiedades de la distribución empírica:
\[\begin{aligned}
nF_n\left( x \right) &= \sum_{i=1}^{n}\mathbf{1}\left\{ X_i\leq x\right\}
\sim \mathcal{B}\left( n,F\left( x \right) \right), \\
E\left( nF_n\left( x \right) \right) &= nF\left( x \right) \implies E\left(
F_n\left( x \right) \right) =F\left( x \right), \\
Var\left( nF_n\left( x \right) \right) &=  nF\left( x \right) \left(
1-F\left( x \right) \right) \\
&\implies  Var\left( F_n\left( x \right) \right) =\frac{F\left( x \right) \left( 1-F\left( x \right) \right)}{n}
\end{aligned}\]

Así pues, en este caso el algoritmo bootstrap uniforme (también llamado
bootstrap naïve) es el siguiente:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Para cada \(i=1,\ldots ,n\) arrojar \(X_i^{\ast}\) a partir de
  \(F_n\), es decir
  \(P^{\ast}\left( X_i^{\ast}=X_j \right) =\frac{1}{n}\), \(j=1,\ldots ,n\)
\item
  Obtener \(\mathbf{X}^{\ast}=\left( X_1^{\ast},\ldots ,X_n^{\ast} \right)\)
\item
  Calcular \(R^{\ast}=R\left( \mathbf{X}^{\ast},F_n \right)\)
\end{enumerate}

Como veremos más adelante, a veces (muy poco frecuentemente) es posible
calcular exactamente la distribución bootstrap de \(R^{\ast}\). Cuando
eso no es posible, esa distribución es fácilmente aproximable por Monte
Carlo, arrojando una gran cantidad, \(B\), de réplicas de \(R^{\ast}\). En
ese caso, el algoritmo se convierte en:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Para cada \(i=1,\ldots ,n\) arrojar \(X_i^{\ast}\) a partir de \(F_n\)
\item
  Obtener \(\mathbf{X}^{\ast}=\left( X_1^{\ast},\ldots ,X_n^{\ast} \right)\)
\item
  Calcular \(R^{\ast}=R\left( \mathbf{X}^{\ast},F_n \right)\)
\item
  Repetir \(B\) veces los pasos 1-3 para obtener las réplicas bootstrap
  \(R^{\ast (1)}, \ldots, R^{\ast (B)}\)
\item
  Utilizar esas réplicas bootstrap para aproximar la distribución en el
  muestreo de \(R\)
\end{enumerate}

\hypertarget{ejemplos}{%
\subsection{Ejemplos}\label{ejemplos}}

\begin{example}[Inferencia sobre la media con varianza conocida, continuación]
\protect\hypertarget{exm:media-dt-conocida-perturbando}{}{\label{exm:media-dt-conocida-perturbando} \iffalse (Inferencia sobre la media con varianza conocida, continuación) \fi{} }
\end{example}

En el Ejemplo \ref{exm:media-dt-conocida} anteriormente visto de inferencia para la media con
varianza conocida, el algoritmo bootstrap (basado en Monte Carlo) para
aproximar la distribución en el muestreo de \(R\) empleado fue:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Para cada \(i=1,\ldots ,n\) arrojar \(U_i\sim \mathcal{U}\left( 0,1 \right)\) y
  hacer \(X_i^{\ast}=X_{\left\lfloor nU_i\right\rfloor +1}\)
\item
  Obtener \(\bar{X}^{\ast}=\frac{1}{n}\sum X_i^{\ast}\)
\item
  Calcular
  \(R^{\ast}=\sqrt{n}\frac{\bar{X}^{\ast}-\bar{X}}{ \sigma }\)
\item
  Repetir \(B\) veces los pasos 1-3 para obtener las réplicas bootstrap
  \(R^{\ast (1)}, \ldots, R^{\ast (B)}\)
\item
  Aproximar la distribución en el muestreo de \(R\) mediante la empírica
  de \(R^{\ast (1)}, \ldots, R^{\ast (B)}\)
\end{enumerate}

Como curiosidad podemos calcular la esperanza y la varianza de \(R\) y la
esperanza y varianza bootstrap de \(R^{\ast}\). Para \(R\) tenemos:
\[\begin{aligned}
E\left( R \right) &=\sqrt{n}\frac{E\left( \bar{X} \right) -\mu }{\sigma }
=0, \\
Var\left( R \right) &=n\frac{Var\left( \bar{X} \right)}{\sigma^2}=n
\frac{\frac{1}{n}\sigma^2}{\sigma^2}=1.
\end{aligned}\]

Para calcular esos mismos momentos de \(R^{\ast}\), resultará útil
obtener previamente la esperanza y varianza bootstrap de
\(\bar{X}^{\ast}\):
\[\begin{aligned}
E^{\ast}\left( \bar{X}^{\ast} \right) &= \frac{1}{n}
\sum_{i=1}^{n}E^{\ast}\left( X_i^{\ast} \right) =\frac{1}{n}
\sum_{i=1}^{n}E^{\ast}\left( X_1^{\ast} \right) =E^{\ast}\left(
X_1^{\ast} \right) =\bar{X}, \\
Var^{\ast}\left( \bar{X}^{\ast} \right) &= \frac{1}{n^2}
\sum_{i=1}^{n}Var^{\ast}\left( X_i^{\ast} \right) =\frac{1}{n^2}
\sum_{i=1}^{n}Var^{\ast}\left( X_1^{\ast} \right) =\frac{1}{n}Var^{\ast
}\left( X_1^{\ast} \right) =\frac{S_n^2}{n},
\end{aligned}\]
ya que
\[\begin{aligned}
E^{\ast}\left( X_1^{\ast} \right) &= \sum_{j=1}^{n}X_jP^{\ast}\left(
X_1^{\ast}=X_j \right) =\sum_{j=1}^{n}\frac{1}{n}X_j=\bar{X}, \\
Var^{\ast}\left( X_1^{\ast} \right) &= E^{\ast}\left( X_1^{\ast
2} \right) -\left[ E^{\ast}\left( X_1^{\ast} \right) \right]
^2=\sum_{j=1}^{n}X_j^2P^{\ast}\left( X_1^{\ast}=X_j \right) -\bar{X}
^2 \\
&= \frac{1}{n}\sum_{j=1}^{n}X_j^2-\bar{X}^2=\frac{1}{n}
\sum_{j=1}^{n}\left( X_j-\bar{X} \right)^2=S_n^2
\end{aligned}\]

Así pues, la esperanza y la varianza bootstrap de \(R^{\ast}\)
resultan:
\[\begin{aligned}
E^{\ast}\left( R^{\ast} \right) &= \sqrt{n}\frac{E^{\ast}\left( \bar{X}^{\ast} \right) -\bar{X}}{\sigma }=0, \\
Var^{\ast}\left( R^{\ast} \right) &= n\frac{Var^{\ast}\left( \bar{X}^{\ast} \right)}{\sigma^2}=n\frac{\frac{1}{n}S_n^2}{\sigma^2}=
\frac{S_n^2}{\sigma^2}.
\end{aligned}\]

Es curioso observar que la esperanza de \(R\) y la esperanza bootstrap de
\(R^{\ast}\) coinciden (son ambas cero), pero no ocurre lo mismo con sus
varianzas: la de \(R\) es \(1\) y la varianza bootstrap de \(R^{\ast}\) es
\(S_n^2/\sigma^2\), que, aunque tiende a \(1\) (en probabilidad o de
forma casi segura, bajo las condiciones adecuadas) cuando \(n\rightarrow \infty\), no es igual a \(1\). Eso nos lleva a intuir que el método de
remuestreo bootstrap propuesto quizá podría modificarse ligeramente para
que imitase exactamente al caso no bootstrap también en la varianza.
Puede comprobarse que eso se consigue remuestreando \(X^{\ast}\) de la
distribución empírica de la muestra modificada:
\(\left( \tilde{X}_1,\ldots ,\tilde{X}_n \right)\), siendo
\[\tilde{X}_i=\bar{X}+\frac{\sigma }{S_n}\left( X_i-\bar{X}
 \right) \text{, }i=1,\ldots ,n.\]

Efectivamente, bajo ese nuevo remuestreo, se tiene
\[\begin{aligned}
E^{\ast}\left( \bar{X}^{\ast} \right) &= E^{\ast}\left( X_1^{\ast
} \right) =\overline{\tilde{X}}=\frac{1}{n}\sum_{i=1}^{n}\left[ \bar{X}+
\frac{\sigma }{S_n}\left( X_i-\bar{X} \right) \right] \\
&= \bar{X}+\frac{1}{n}\frac{\sigma }{S_n}\sum_{i=1}^{n}\left( X_i-
\bar{X} \right) =\bar{X}, \\
Var^{\ast}\left( \bar{X}^{\ast} \right) &= \frac{1}{n}Var^{\ast
}\left( X_1^{\ast} \right) =\frac{\sigma^2}{n},
\end{aligned}\]
ya que
\[\begin{aligned}
Var^{\ast}\left( X_1^{\ast} \right) &= E^{\ast}\left( X_1^{\ast
2} \right) -\left[ E^{\ast}\left( X_1^{\ast} \right) \right]
^2=\sum_{j=1}^{n}\tilde{X}_j^2P^{\ast}\left( X_1^{\ast}=\tilde{X}
_j \right) -\overline{\tilde{X}}^2 \\
&= \frac{1}{n}\sum_{j=1}^{n}\tilde{X}_j^2-\overline{\tilde{X}}^2=\frac{
1}{n}\sum_{j=1}^{n}\left( \tilde{X}_j-\overline{\tilde{X}} \right)^2=
\frac{1}{n}\sum_{j=1}^{n}\left[ \frac{\sigma }{S_n}\left( X_j-\bar{X} \right) \right]^2 \\
&= \frac{\sigma^2}{S_n^2}\frac{1}{n}\sum_{j=1}^{n}\left( X_i-
\bar{X} \right)^2=\frac{\sigma^2}{S_n^2}S_n^2=\sigma^2.
\end{aligned}\]
Como consecuencia
\[\begin{aligned}
E^{\ast}\left( R^{\ast} \right) &= \sqrt{n}\frac{E^{\ast}\left( 
\bar{X}^{\ast} \right) -\bar{X}}{\sigma }=0, \\
Var^{\ast}\left( R^{\ast} \right) &= n\frac{Var^{\ast}\left( 
\bar{X}^{\ast} \right)}{\sigma^2}=n\frac{\frac{\sigma^2}{n}}{\sigma^2}
=1.
\end{aligned}\]

Esto es muy coherente con lo que nos diría la intuición pues, si la
varianza poblacional, \(\sigma^2\), es conocida (ese es el motivo de
que podamos usarla directamente en la definición del estadístico \(R\)),
el plan de remuestreo bootstrap también ha de conocer \(\sigma^2\), es
decir ha de diseñarse de modo que la distribución bootstrap de
\(X^{\ast}\) tenga también varianza bootstrap \(\sigma^2\). Eso ocurre
con el remuestreo uniforme de la muestra transformada
\(\left( \tilde{X}_1,\ldots ,\tilde{X}_n \right)\), pero no ocurre con
el remuestreo naïve (a partir de la distribución empírica de la muestra
original). Esto da pie a una de las consideraciones más importantes a la
hora de diseñar un buen método de remuestreo bootstrap: ha de procurarse
que \textbf{el bootstrap imite todas las condiciones que cumple la población
original}.

El código para realizar remuestreo bootstrap uniforme sobre la empírica de la
muestra perturbando es análogo:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Remuestreo}
\NormalTok{B }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{estadistico\_boot }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(B)}
\NormalTok{coeficiente }\OtherTok{\textless{}{-}}\NormalTok{ sigma}\SpecialCharTok{/}\FunctionTok{sd}\NormalTok{(muestra)}
\NormalTok{muestra\_perturbada }\OtherTok{\textless{}{-}}\NormalTok{ x\_barra }\SpecialCharTok{+}\NormalTok{ coeficiente }\SpecialCharTok{*}\NormalTok{ (muestra }\SpecialCharTok{{-}}\NormalTok{ x\_barra)}
\ControlFlowTok{for}\NormalTok{ (k }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{B) \{}
\NormalTok{  remuestra }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(muestra\_perturbada, n, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{  x\_barra\_boot }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(remuestra)}
\NormalTok{  estadistico\_boot[k] }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(n) }\SpecialCharTok{*}\NormalTok{ (x\_barra\_boot }\SpecialCharTok{{-}}\NormalTok{ x\_barra)}\SpecialCharTok{/}\NormalTok{sigma}
\NormalTok{\}}

\CommentTok{\# Aproximación bootstrap de los ptos críticos}
\NormalTok{pto\_crit }\OtherTok{\textless{}{-}} \FunctionTok{quantile}\NormalTok{(estadistico\_boot, }\FunctionTok{c}\NormalTok{(alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{))}
\CommentTok{\# Construcción del IC}
\NormalTok{ic\_inf\_boot }\OtherTok{\textless{}{-}}\NormalTok{ x\_barra }\SpecialCharTok{{-}}\NormalTok{ pto\_crit[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ sigma}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(n)}
\NormalTok{ic\_sup\_boot }\OtherTok{\textless{}{-}}\NormalTok{ x\_barra }\SpecialCharTok{{-}}\NormalTok{ pto\_crit[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ sigma}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(n)}
\NormalTok{IC\_boot }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(ic\_inf\_boot, ic\_sup\_boot)}
\FunctionTok{names}\NormalTok{(IC\_boot) }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\DecValTok{100}\SpecialCharTok{*}\FunctionTok{c}\NormalTok{(alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{), }\StringTok{"\%"}\NormalTok{)}
\NormalTok{IC\_boot}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      2.5%     97.5% 
## 0.5024398 1.0827052
\end{verbatim}

\begin{example}[Inferencia sobre la mediana]
\protect\hypertarget{exm:mediana}{}{\label{exm:mediana} \iffalse (Inferencia sobre la mediana) \fi{} } \vspace{0.5cm}

Continuando con el ejemplo de los tiempos de vida de microorganismos,
supongamos que queremos obtener una estimación por intervalo de confianza
de su vida mediana a partir de los 15 valores observados.
\end{example}

Consideramos la mediana poblacional como parámetro de interés:
\[\theta = \theta \left( F \right) = F^{-1}\left( \frac{1}{2} \right) 
= \inf \left\{ x\in \mathbb{R} : F\left( x \right) \geq \frac{1}{2}\right\}.\]
Dada una muestra \(\mathbf{X}=\left( X_1,\ldots ,X_n \right) \sim F\), \(\theta\) puede estimarse mediante la mediana muestral
\[\begin{aligned}
\hat{\theta} &= \theta \left( F_n \right) =F_n^{-1}\left( \frac{1}{2} \right) 
=\inf \left\{ x\in \mathbb{R} : F_n\left( x \right) \geq \frac{1}{2}
\right\} \\
&= \left\{ 
\begin{array}{ll}
X_{(m)} & \text{si } n=2m-1 \text{ es impar} \\ 
\frac{X_{(m)}+X_{\left( m+1 \right)}}{2} & \text{si } n=2m \text{ es par}
\end{array}
\right.
\end{aligned}\]
siendo \(X_{(1)},\ldots ,X_{(n)}\) los estadísticos ordenados.

El estadístico interesante para realizar inferencia en este contexto es
\(R=\sqrt{n}\left( \hat{\theta}-\theta \right)\). Si la población de
partida es continua, puede demostrarse que su distribución asintótica
(i.e., cuando \(n \rightarrow \infty\)) viene dada por
\[R=\sqrt{n}\left( \hat{\theta}-\theta \right) \overset{d}{\rightarrow }
\mathcal{N}\left( 0,\frac{1}{f\left( \theta \right)^2} \right),\]donde \(f\) es
la función de densidad de la población. Como consecuencia, la
utilización de esta distribución límite,
\(\mathcal{N}\left( 0, 1/f\left( \theta \right)^2 \right)\), para realizar
inferencia sobre la mediana, además de comportar una aproximación de la
distribución en el muestreo real, no puede utilizarse directamente
porque la densidad (desconocida) aparece en la expresión de la varianza
asintótica. Para ser utilizable en la práctica deberíamos estimar \(f\),
lo cual es un problema añadido.

Esta es pues una situación muy natural en la que usar un método
bootstrap para aproximar la distribución de \(R\). Consideremos como
estimador de \(F\) la distribución empírica, \(F_n\), y procedamos según
un bootstrap uniforme (supongamos \(n=2m-1\), impar, por simplicidad):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Para cada \(i=1,\ldots ,n\) arrojar \(U_i\sim \mathcal{U}\left( 0,1 \right)\) y
  hacer \(X_i^{\ast}=X_{\left\lfloor nU_i\right\rfloor +1}\)
\item
  Obtener \(X_{(1)}^{\ast},\ldots ,X_{(n)}^{\ast}\)
  los estadísticos ordenados de la remuestra bootstrap y quedarse con
  el que ocupa lugar central:
  \(\hat{\theta}^{\ast}=\theta \left( F_n^{\ast} \right) =X_{(m)}^{\ast}\)
\item
  Calcular
  \(R^{\ast}=\sqrt{n}\left( X_{(m)}^{\ast}-X_{\left(m \right)} \right)\)
\item
  Repetir \(B\) veces los pasos 1-3 para obtener las réplicas bootstrap
  \(R^{\ast (1)}, \ldots, R^{\ast (B)}\)
\item
  Aproximar la distribución en el muestreo de \(R\) mediante la empírica
  de \(R^{\ast (1)}, \ldots, R^{\ast (B)}\)
\end{enumerate}

El código implementando este algoritmo sería muy similar al de los casos anteriores:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x\_mediana}\OtherTok{\textless{}{-}} \FunctionTok{median}\NormalTok{(muestra)}

\CommentTok{\# Remuestreo}
\NormalTok{B }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{estadistico\_boot }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(B)}
\NormalTok{coeficiente }\OtherTok{\textless{}{-}}\NormalTok{ sigma}\SpecialCharTok{/}\FunctionTok{sd}\NormalTok{(muestra)}
\ControlFlowTok{for}\NormalTok{ (k }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{B) \{}
\NormalTok{  remuestra }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(muestra, n, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{  x\_mediana\_boot }\OtherTok{\textless{}{-}} \FunctionTok{median}\NormalTok{(remuestra)}
\NormalTok{  estadistico\_boot[k] }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(n) }\SpecialCharTok{*}\NormalTok{ (x\_mediana\_boot }\SpecialCharTok{{-}}\NormalTok{ x\_mediana)}
\NormalTok{\}}

\CommentTok{\# Aproximación bootstrap de los ptos críticos}
\NormalTok{pto\_crit }\OtherTok{\textless{}{-}} \FunctionTok{quantile}\NormalTok{(estadistico\_boot, }\FunctionTok{c}\NormalTok{(alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{))}
\CommentTok{\# Construcción del IC}
\NormalTok{ic\_inf\_boot }\OtherTok{\textless{}{-}}\NormalTok{ x\_mediana }\SpecialCharTok{{-}}\NormalTok{ pto\_crit[}\DecValTok{2}\NormalTok{]}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(n)}
\NormalTok{ic\_sup\_boot }\OtherTok{\textless{}{-}}\NormalTok{ x\_mediana }\SpecialCharTok{{-}}\NormalTok{ pto\_crit[}\DecValTok{1}\NormalTok{]}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(n)}
\NormalTok{IC\_boot }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(ic\_inf\_boot, ic\_sup\_boot)}
\FunctionTok{names}\NormalTok{(IC\_boot) }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\DecValTok{100}\SpecialCharTok{*}\FunctionTok{c}\NormalTok{(alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{), }\StringTok{"\%"}\NormalTok{)}
\NormalTok{IC\_boot}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  2.5% 97.5% 
## 0.132 0.962
\end{verbatim}

Sin embargo, como veremos más adelante, este caso de inferencia de la
mediana es uno de los pocos casos en los que la distribución bootstrap
se puede calcular de forma exacta, siendo dicha expresión utilizable en
la práctica.

\hypertarget{cuxe1lculo-de-la-distribuciuxf3n-bootstrap-exacta-y-aproximada}{%
\section{Cálculo de la distribución Bootstrap: exacta y aproximada}\label{cuxe1lculo-de-la-distribuciuxf3n-bootstrap-exacta-y-aproximada}}

\hypertarget{distribuciuxf3n-bootstrap-exacta}{%
\subsection{Distribución bootstrap exacta}\label{distribuciuxf3n-bootstrap-exacta}}

En principio siempre es posible calcular la distribución en el
remuestreo del estadístico bootstrap de forma exacta. Al menos para el
bootstrap uniforme, que es el más habitual. El motivo es que la
distribución de probabilidad de la que se remuestrea en el universo
bootstrap es discreta y con un número finito de valores: \(X_1,\ldots ,X_n\). Así pues, cada observación bootstrap, \(X_i^{\ast}\), ha de
tomar necesariamente alguno de esos \(n\) valores y, por tanto, el número
de posibles remuestras, \(\mathbf{X}^{\ast}=\left( X_1^{\ast },\ldots ,X_n^{\ast} \right)\), obtenibles mediante el bootstrap
uniforme es finito, concretamente \(n^{n}\). Aún siendo finito, este
número es gigantescamente grande incluso para tamaños muestrales
pequeños (salvo casos extremos del tipo \(n=2,\ldots ,9\)). Por ejemplo,
para \(n=10\), tenemos \(10^{10}\) (diez mil millones de) posibles
remuestras bootstrap y para \(n=20\), tendríamos
\(20^{20}\simeq 10.4857\cdot 10^{25}\) (algo más de cien
cuatrillones). Incluso para estos tamaños muestrales el problema de
cálculo de la distribución bootstrap exacta de
\(\mathbf{X}^{\ast}\) es inabordable.

\hypertarget{vectores-de-remuestreo}{%
\subsection{Vectores de remuestreo}\label{vectores-de-remuestreo}}

Una forma alternativa de representar las posibles remuestras bootstrap
es mediante los llamados vectores de remuestreo. Son utilizables en el
caso de que el estadístico de interés sea funcional, es decir, cuando
\(R\) depende de la muestra sólo a través de la distribución empírica o,
lo que es lo mismo, el valor de \(R\) no cambia cuando realizamos una
permutación arbitraria sobre los elementos de la muestra (los cambiamos
de orden). Consideremos la remuestra bootstrap
\(\mathbf{X}^{\ast} =\left( X_1^{\ast},\ldots ,X_n^{\ast} \right)\) y denotemos por
\[N_j=\#\left\{ i\in \left\{ 1,\ldots ,n\right\} : 
X_i^{\ast}=X_j\right\}.\]
Obviamente, si el orden en el que se han obtenido los
elementos de la muestra no es importante, entonces el vector
\(\mathbf{N}=\left( N_1,\ldots ,N_n \right)\) contiene la misma
información que la remuestra bootstrap \(\mathbf{X}^{\ast}\).
Esencialmente lo que hace el vector \(\mathbf{N}\) es contabilizar
cuantas veces se repite cada elemento de la muestra original en la
remuestra bootstrap. Con esta notación, el vector de remuestreo
bootstrap,
\(\mathbf{P}^{\ast}=\left( P_1^{\ast},\ldots ,P_n^{\ast} \right)\),
se define como \(P_i^{\ast}=\frac{N_i}{n}\), \(i=1,\ldots ,n\).

La distribución en el remuestreo de \(\mathbf{N}\), bajo el
bootstrap uniforme, es multinomial: \(\mathbf{N}\sim \mathcal{M}_n\left( n,\left( \frac{1}{n},\ldots ,\frac{1}{n} \right) \right)\). Así que su
masa de probabilidad, y por tanto la de \(\mathbf{P}^{\ast}\), es
fácilmente calculable:
\[\begin{aligned}
P\left( N_1=m_1,\ldots ,N_n=m_n \right) &= \frac{n!}{m_1!\cdots
m_n!}\left( \frac{1}{n} \right)^{m_1}\cdots \left( \frac{1}{n} \right)
^{m_n} \\
&= \frac{n!}{m_1!\cdots m_n!n^{n}}\text{, } \\
\end{aligned}\]
para \(m_1,\ldots ,m_n\) enteros con \(\sum_{i=1}^{n}m_i = n\),
donde el número de átomos de probabilidad de \(\mathbf{N}\) es ahora
\(\binom{n+n-1}{n}=\binom{2n-1}{n}\).

En general \(\binom{2n-1}{n}<n^{n}\),
pues el hecho de que no importe el orden de las componentes de las
remuestras bootstrap provoca un menor número de átomos de probabilidad.
Aún así dicho cardinal es prohibitivamente grande incluso para tamaños
muestrales pequeños: para \(n=10\), resultaría abordable pues
\(\binom{19}{10}= 92\,378\), pero para \(n=20\) tendríamos
\(\binom{39}{20}= 68\, 923\,264\,410\). De toda esa
enorme cantidad de átomos, el de más grande probabilidad resulta tener
una probabilidad de \(\frac{n!}{n^{n}}\), que es insignificantemente
pequeña para tamaños pequeños como \(n=20\), con
\(\frac{20!}{20^{20}}\sim 2.\, 320\,2\times 10^{-8}\). De todas
formas, existen raras ocasiones en las que el número de átomos de
probabilidad de \(R^{\ast}\) resulta ser mucho menor que el de
\(\mathbf{P}^{\ast}\).

Para tamaños muestrales realmente pequeños es posible encontrar todos
los átomos de probabilidad de la distribución bootstrap. Un ejemplo es
la media muestral con, por ejemplo, \(n=3\).

\begin{example}[Media muestral para una muestra de tamaño 3]
\protect\hypertarget{exm:media3}{}{\label{exm:media3} \iffalse (Media muestral para una muestra de tamaño 3) \fi{} } \vspace{0.5cm}

Consideremos una muestra aleatoria simple de tamaño \(n=3\) de una
población con distribución \(F\) y tomemos como parámetro de interés la
media poblacional
\(\theta \left( F \right) =\mu =\int xdF\left( x \right)\).
Tomemos como estadístico de interés
\(R=R\left( \mathbf{X},F \right) =\bar{X}\).
El análogo bootstrap de esta estadístico es
\(R^{\ast}=R\left( \mathbf{X}^{\ast},F_n \right) =\bar{X}^{\ast}\),
cuya distribución en el remuestreo se puede calcular de forma exacta
debido al reducido número de átomos de probabilidad que tiene.
Esta es una distribución discreta con 10 posibles valores, cuyo
valor más probable es precisamente \(\bar{X}\) que tiene una
probabilidad bootstrap de \(\frac{2}{9}\), como puede verse en la
siguiente tabla.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.21}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.19}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.19}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.20}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.21}}@{}}
\toprule
\(\mathbf{X}^{\ast}\) (salvo permutaciones) & \(\mathbf{N}\) = \(\left( m_1,m_2,m_3 \right)\) & \(\mathbf{P}^{\ast}\) = \(\left(p_1,p_2,p_3 \right)\) & \(\frac{3!}{m_1!m_2!m_3!3^{3}}\) & \(\bar{X}^{\ast}\) \\
\midrule
\endhead
\(\left( X_1,X_1,X_1 \right)\) & \(\left( 3,0,0 \right)\) & \(\left(1,0,0 \right)\) & \(\frac{1}{27}\) & \(X_1\) \\
\(\left( X_2,X_2,X_2 \right)\) & \(\left( 0,3,0 \right)\) & \(\left(0,1,0 \right)\) & \(\frac{1}{27}\) & \(X_2\) \\
\(\left( X_3,X_3,X_3 \right)\) & \(\left( 0,0,3 \right)\) & \(\left(0,0,1 \right)\) & \(\frac{1}{27}\) & \(X_3\) \\
\(\left( X_1,X_1,X_2 \right)\) & \(\left( 2,1,0 \right)\) & \(\left( \frac{2}{3},\frac{1}{3},0 \right)\) & \(\frac{1}{9}\) & \(\frac{2X_1+X_2}{3}\) \\
\(\left( X_1,X_1,X_3 \right)\) & \(\left( 2,0,1 \right)\) & \(\left( \frac{2}{3},0,\frac{1}{3} \right)\) & \(\frac{1}{9}\) & \(\frac{2X_1+X_3}{3}\) \\
\(\left( X_1,X_2,X_2 \right)\) & \(\left( 1,2,0 \right)\) & \(\left( \frac{1}{3},\frac{2}{3},0 \right)\) & \(\frac{1}{9}\) & \(\frac{X_1+2X_2}{3}\) \\
\(\left( X_2,X_2,X_3 \right)\) & \(\left( 0,2,1 \right)\) & \(\left( 0,\frac{2}{3},\frac{1}{3} \right)\) & \(\frac{1}{9}\) & \(\frac{2X_2+X_3}{3}\) \\
\(\left( X_1,X_3,X_3 \right)\) & \(\left( 1,0,2 \right)\) & \(\left( \frac{1}{3},0,\frac{2}{3} \right)\) & \(\frac{1}{9}\) & \(\frac{X_1+2X_3}{3}\) \\
\(\left( X_2,X_3,X_3 \right)\) & \(\left( 0,1,2 \right)\) & \(\left( 0,\frac{1}{3},\frac{2}{3} \right)\) & \(\frac{1}{9}\) & \(\frac{X_2+2X_3}{3}\) \\
\(\left( X_1,X_2,X_3 \right)\) & \(\left( 1,1,1 \right)\) & \(\left( \frac{1}{3},\frac{1}{3},\frac{1}{3} \right)\) & \(\frac{2}{9}\) & \(\frac{X_1+X_2+X_3}{3}\) \\
\bottomrule
\end{longtable}
\end{example}

En algunas ocasiones es factible encontrar expresiones cerradas
para la distribución de \(R^{\ast}\), más allá de las obvias que
consisten en enumerar el ingente número de átomos de probabilidad de
\(\mathbf{P}^{\ast}\):
\[P^{\ast}\left( R^{\ast}=R\left( \left( m_1,\ldots ,m_n \right)
,F_n \right) \right)\]
Veamos un ejemplo.

\hypertarget{inferencia-sobre-la-mediana}{%
\subsection{Inferencia sobre la mediana}\label{inferencia-sobre-la-mediana}}

En el caso de la mediana, consideremos, por simplicidad el caso de
tamaño muestral impar, \(n=2m-1\). Supongamos también que no hay empates
en los valores de la muestra (si los hubiese las expresiones serían más
farragosas pero también calculables). La versión bootstrap del
estadístico sobre el cual pivota la inferencia es \(R^{\ast}=X_{\left( m \right)}^{\ast}-X_{(m)}\). Su distribución bootstrap
podría calcularse si se obtuviese la de \(X_{(m)}^{\ast}\).
Pero ésta es factible de calcular por los pocos posibles valores que
puede tomar el estadístico \(X_{(m)}^{\ast}\) (tan sólo los
valores de la muestra original) y por la sencillez del bootstrap
uniforme.
Veámoslo:
\[P^{\ast}\left( X_{(m)}^{\ast}>X_{(j)} \right)
=P^{\ast}\left( \#\left\{ X_i^{\ast}\leq X_{(j)}\right\}
\leq m-1 \right),\]
pero
\[\#\left\{ X_i^{\ast}\leq X_{(j)}\right\} \sim \mathcal{B}\left(
n,\frac{j}{n} \right),\]
con lo cual
\[P^{\ast}\left( X_{(m)}^{\ast}>X_{(j)} \right)
=\sum_{k=0}^{m-1}\binom{n}{k}\left( \frac{j}{n} \right)^{k}
\left( \frac{n-j}{n} \right)^{n-k}\]
y, por lo tanto, si \(j\geq 2\),
\[\begin{aligned}
P^{\ast}\left( X_{(m)}^{\ast}=X_{(j)} \right)
=&\ P^{\ast}\left( X_{(m)}^{\ast}>X_{\left( j-1 \right)} \right)
-P^{\ast}\left( X_{(m)}^{\ast}>X_{(j)} \right) \\
=&\ \sum_{k=0}^{m-1}\binom{n}{k}\left( \frac{j-1}{n} \right)^{k}\left( \frac{
n-j+1}{n} \right)^{n-k} \\
&-\sum_{k=0}^{m-1}\binom{n}{k}\left( \frac{j}{n} \right)^{k}\left( \frac{n-j}{
n} \right)^{n-k} \\
=&\ \sum_{k=0}^{m-1}\binom{n}{k}\left[ \left( \frac{j-1}{n} \right)^{k}\left( 
\frac{n-j+1}{n} \right)^{n-k}-\left( \frac{j}{n} \right)^{k}\left( \frac{n-j
}{n} \right)^{n-k}\right] .
\end{aligned}\]

Cuando \(j=1\), entonces
\[\begin{aligned}
P^{\ast}\left( X_{(m)}^{\ast} = X_{(1)} \right)
&= 1-P^{\ast}\left( X_{(m)}^{\ast}>X_{(1)} \right) \\
&=  1-\sum_{k=0}^{m-1}\binom{n}{k}\left( \frac{1}{n} \right)^{k}
\left( \frac{n-1}{n} \right)^{n-k}.
\end{aligned}\]

\hypertarget{distribuciuxf3n-bootstrap-aproximada-por-monte-carlo}{%
\subsection{Distribución Bootstrap aproximada por Monte Carlo}\label{distribuciuxf3n-bootstrap-aproximada-por-monte-carlo}}

Como ya se comentó anteriormente,
al conocer el mecanismo que genera los datos en el bootstrap,
siempre se podrá simular dicho mecanismo mediante el método de Monte
Carlo. Por lo que el algoritmo general para la aproximación de Monte Carlo del
bootstrap uniforme es:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Para cada \(i=1,\ldots ,n\) arrojar \(X_i^{\ast}\) a partir de \(F_n\)
\item
  Obtener \(\mathbf{X}^{\ast}=\left( X_1^{\ast},\ldots ,X_n^{\ast} \right)\)
\item
  Calcular \(R^{\ast}=R\left( \mathbf{X}^{\ast},F_n \right)\)
\item
  Repetir \(B\) veces los pasos 1-3 para obtener las réplicas bootstrap
  \(R^{\ast (1)}\), \(\ldots\), \(R^{\ast (B)}\)
\item
  Utilizar esas réplicas bootstrap para aproximar la distribución en el
  muestreo de \(R\)
\end{enumerate}

Como se mostró en la Sección \ref{intro-implementacion}, el paso 1 se puede llevar a cabo simulando una distribución uniforme discreta
mediante el método de la transformación cuantil:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Para cada \(i=1,\ldots ,n\) arrojar \(U_i\sim \mathcal{U}\left( 0,1 \right)\) y
  hacer \(X_i^{\ast}=X_{\left\lfloor nU_i\right\rfloor +1}\)
\end{enumerate}

Aunque en \texttt{R} se recomienda emplear la función \texttt{sample}.

\begin{example}[Inferencia sobre la media con varianza desconocida]
\protect\hypertarget{exm:media-dt-desconocida}{}{\label{exm:media-dt-desconocida} \iffalse (Inferencia sobre la media con varianza desconocida) \fi{} } \vspace{0.5cm}

Continuando con el ejemplo de los tiempos de vida de microorganismos,
supongamos que queremos obtener una estimación por intervalo de confianza
de su vida media a partir de los 15 valores observados pero en la
situación mucho más realista de que la varianza sea desconocida.
\end{example}

Tenemos pues
\(\mathbf{X}=\left( X_1,\ldots ,X_n \right) \sim F\,\), con
\(\mu\) y \(\sigma\) desconocidas

El parámetro de interés es
\[\theta \left( F \right) =\mu =\int x~dF\left( x \right)\]
que se estima mediante
\[\theta \left( F_n \right) =\int x~dF_n\left( x \right) =\bar{X}.\]
Así pues, el estadístico en el que basar la inferencia es
\[R=R\left( \mathbf{X},F \right) =\sqrt{n}\frac{\bar{X}-\mu }{S_{n-1}},\]
donde \(S_{n-1}^2\) es la cuasivarianza muestral:
\[S_{n-1}^2=\frac{1}{n-1}\sum_{j=1}^{n}\left( X_j-\bar{X} \right)^2.\]

Bajo normalidad \(\left( X\sim \mathcal{N}\left( \mu ,\sigma^2 \right) \right)\),
se sabe que \(R\sim t_{n-1}\) y, en particular,
\(R\overset{d}{\rightarrow } \mathcal{N}\left( 0,1 \right)\) cuando \(n\rightarrow \infty\).
Si \(F\) no es normal entonces la distribución de \(R\) ya no es una \(t_{n-1}\),
pero también es cierto que, bajo ciertas condiciones,
\(R\overset{d}{\rightarrow}\mathcal{N}\left(0,1 \right)\).

En el contexto bootstrap elegimos \(\hat{F}=F_n\,\), con lo cual se
trata de un bootstrap naïve o uniforme. El análogo bootstrap del
estadístico \(R\) será
\[R^{\ast}=R\left( \mathbf{X}^{\ast},F_n \right) =\sqrt{n}\frac{
\bar{X}^{\ast}-\bar{X}}{S_{n-1}^{\ast}},\]
siendo
\[\begin{aligned}
\bar{X}^{\ast} &= \frac{1}{n}\sum_{i=1}^{n}X_i^{\ast}, \\
S_{n-1}^{\ast 2} &= \frac{1}{n-1}\sum_{i=1}^{n}\left( X_i^{\ast}-
\bar{X}^{\ast} \right)^2.
\end{aligned}\]

El algoritmo bootstrap (aproximado por Monte Carlo) procedería así:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Para cada \(i=1,\ldots ,n\) arrojar \(U_i\sim \mathcal{U}\left( 0,1 \right)\) y
  hacer \(X_i^{\ast}=X_{\left\lfloor nU_i\right\rfloor +1}\)
\item
  Obtener \(\bar{X}^{\ast}\) y \(S_{n-1}^{\ast 2}\)
\item
  Calcular
  \(R^{\ast}=\sqrt{n}\frac{\bar{X}^{\ast}-\bar{X}}{ S_{n-1}^{\ast}}\)
\item
  Repetir \(B\) veces los pasos 1-3 para obtener las réplicas bootstrap
  \(R^{\ast (1)}, \ldots, R^{\ast (B)}\)
\item
  Aproximar la distribución en el muestreo de \(R\) mediante la
  distribución empírica de \(R^{\ast (1)}, \ldots, R^{\ast (B)}\)
\end{enumerate}

El código para implementar este método es similar al del caso de varianza conocida
del Ejemplo \ref{exm:media-dt-conocida}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{muestra }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{0.143}\NormalTok{, }\FloatTok{0.182}\NormalTok{, }\FloatTok{0.256}\NormalTok{, }\FloatTok{0.26}\NormalTok{, }\FloatTok{0.27}\NormalTok{, }\FloatTok{0.437}\NormalTok{, }\FloatTok{0.509}\NormalTok{, }
             \FloatTok{0.611}\NormalTok{, }\FloatTok{0.712}\NormalTok{, }\FloatTok{1.04}\NormalTok{, }\FloatTok{1.09}\NormalTok{, }\FloatTok{1.15}\NormalTok{, }\FloatTok{1.46}\NormalTok{, }\FloatTok{1.88}\NormalTok{, }\FloatTok{2.08}\NormalTok{)}
\NormalTok{n }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(muestra)}
\NormalTok{alfa }\OtherTok{\textless{}{-}} \FloatTok{0.05}
\NormalTok{x\_barra }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(muestra)}
\NormalTok{cuasi\_dt }\OtherTok{\textless{}{-}} \FunctionTok{sd}\NormalTok{(muestra)}

\CommentTok{\# Remuestreo}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{B }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{remuestra }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(n)}
\NormalTok{estadistico\_boot }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(B)}
\ControlFlowTok{for}\NormalTok{ (k }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{B) \{}
\NormalTok{  remuestra }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(muestra, n, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{  x\_barra\_boot }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(remuestra)}
\NormalTok{  cuasi\_dt\_boot }\OtherTok{\textless{}{-}} \FunctionTok{sd}\NormalTok{(remuestra)}
\NormalTok{  estadistico\_boot[k] }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(n) }\SpecialCharTok{*}\NormalTok{ (x\_barra\_boot }\SpecialCharTok{{-}}\NormalTok{ x\_barra)}\SpecialCharTok{/}\NormalTok{cuasi\_dt\_boot}
\NormalTok{\}}

\CommentTok{\# Aproximación bootstrap de los ptos críticos}
\NormalTok{pto\_crit }\OtherTok{\textless{}{-}} \FunctionTok{quantile}\NormalTok{(estadistico\_boot, }\FunctionTok{c}\NormalTok{(alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{))}

\CommentTok{\# Construcción del IC}
\NormalTok{ic\_inf\_boot }\OtherTok{\textless{}{-}}\NormalTok{ x\_barra }\SpecialCharTok{{-}}\NormalTok{ pto\_crit[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ cuasi\_dt}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(n)}
\NormalTok{ic\_sup\_boot }\OtherTok{\textless{}{-}}\NormalTok{ x\_barra }\SpecialCharTok{{-}}\NormalTok{ pto\_crit[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ cuasi\_dt}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(n)}
\NormalTok{IC\_boot }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(ic\_inf\_boot, ic\_sup\_boot)}
\FunctionTok{names}\NormalTok{(IC\_boot) }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\DecValTok{100}\SpecialCharTok{*}\FunctionTok{c}\NormalTok{(alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{), }\StringTok{"\%"}\NormalTok{)}
\NormalTok{IC\_boot}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      2.5%     97.5% 
## 0.5030131 1.2888063
\end{verbatim}

Este procedimiento para la construcción de intervalos de confianza
se denomina \emph{método percentil-t} y se tratará en la Sección \ref{icboot-perc-t}.

Como ejemplo adicional podemos comparar la aproximación de la distribución bootstrap del estadístico con la aproximación \(t_{n-1}\) basada en normalidad.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(estadistico\_boot, }\AttributeTok{freq =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.4}\NormalTok{))}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{v =}\NormalTok{ pto\_crit)}
\FunctionTok{curve}\NormalTok{(}\FunctionTok{dt}\NormalTok{(x, n}\DecValTok{{-}1}\NormalTok{), }\AttributeTok{add=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{)}
\NormalTok{pto\_crit\_t }\OtherTok{\textless{}{-}} \FunctionTok{qt}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, n}\DecValTok{{-}1}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{v =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{pto\_crit\_t, pto\_crit\_t), }\AttributeTok{lty =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{01-intro_files/figure-latex/unnamed-chunk-11-1} \end{center}

En este caso la distribución bootstrap del estadístico es más asimétrica, lo que se traducirá en diferencias entre las estimaciones por intervalos de confianza.
Por ejemplo, podemos obtener la estimación basada en normalidad mediante la función \texttt{t.test()}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(muestra)}\SpecialCharTok{$}\NormalTok{conf.int}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.4599374 1.1507292
## attr(,"conf.level")
## [1] 0.95
\end{verbatim}

\hypertarget{elecciuxf3n-del-nuxfamero-de-ruxe9plicas-monte-carlo}{%
\subsection{Elección del número de réplicas Monte Carlo}\label{elecciuxf3n-del-nuxfamero-de-ruxe9plicas-monte-carlo}}

Normalmente el valor de \(B\) se toma del orden de varias centenas o
incluso millares. En los casos en los que el bootstrap se utiliza para
estimar el sesgo o la varianza de un estimador, bastará tomar un número,
\(B\), de réplicas bootstrap del orden de \(B = 100, 200, 500\). Sin embargo,
cuando se trata de utilizar el bootstrap para realizar contrastes de
hipótesis o construir intervalos de confianza son necesarios valores
mayores, del tipo \(B = 500, 1000, 2000, 5000\).

Evidentemente, la función de distribución del estadístico de interés,
\(\psi \left( u \right) =P\left( R\leq u \right)\), se estimaría mediante
la distribución empírica de las \(B\) realizaciones de la aproximación de
Monte Carlo,
\[\hat{\psi}_{B}\left( u \right) =
\frac{1}{B}\sum_{i=1}^{B}\mathbf{1}\left\{ R^{\ast (i)}\leq u\right\},\]
de la verdadera distribución bootstrap exacta:
\(\hat{\psi}\left(u \right) =P^{\ast}\left( R^{\ast}\leq u \right)\).
El error de MonteCarlo de \(\hat{\psi}_{B}\left( u \right)\) con respecto
a \(\hat{\psi}\left( u \right)\) viene dado por su varianza Monte Carlo,
pues su sesgo Monte Carlo es cero:

\[\begin{aligned}
E^{MC}\left( \hat{\psi}_{B}\left( u \right) \right) &= \frac{1}{B}
\sum_{i=1}^{B}E^{MC}\left( \mathbf{1}\left\{ R^{\ast (i)}\leq
u\right\} \right) =\frac{1}{B}\sum_{i=1}^{B}P^{\ast}\left( R^{\ast \left(
i \right)}\leq u \right) \\
&= \frac{1}{B}\sum_{i=1}^{B}\hat{\psi}\left( u \right) =\hat{\psi}\left(
u \right), \\
Var^{MC}\left( \hat{\psi}_{B}\left( u \right) \right) &= \frac{1}{B^2}
\sum_{i=1}^{B}Var^{MC}\left( \mathbf{1}\left\{ R^{\ast (i)}\leq
u\right\} \right) \\
&= \frac{1}{B^2}\sum_{i=1}^{B}P^{\ast}\left( R^{\ast (i)
}\leq u \right) \left[ 1-P^{\ast}\left( R^{\ast (i)}\leq
u \right) \right] = \\
&= \frac{1}{B^2}\sum_{i=1}^{B}\hat{\psi}\left( u \right) \left( 1-\hat{\psi}
\left( u \right) \right) =\frac{1}{B}\hat{\psi}\left( u \right) \left( 1-\hat{
\psi}\left( u \right) \right) \leq \frac{1}{4B}
\end{aligned}\]

Así, el error de la aproximación de Monte Carlo al bootstrap exacto
(raíz cuadrada de la varianza del Monte Carlo), puede acotarse por
\(\frac{1}{2\sqrt{B}}\)
(para más detalles sobre la convergencia de una aproximación Monte Carlo ver p.e. el \href{https://rubenfcasal.github.io/simbook/cap4.html}{Capítulo 4} de Fernández-Casal y Cao, 2020).

\hypertarget{intro-paquetes}{%
\section{Herramientas disponibles en R sobre bootstrap}\label{intro-paquetes}}

En \texttt{R} hay una gran cantidad de paquetes que implementan métodos bootstrap.
Por ejemplo, al ejecutar el comando \texttt{??bootstrap} (o \texttt{help.search(\textquotesingle{}bootstrap\textquotesingle{})})
se mostrarán las funciones de los paquetes instalados que incluyen este término
en su documentación (se puede realizar la búsqueda en todos los paquetes disponibles
de \texttt{R} a través de \url{https://www.rdocumentation.org}).

De entre todos estas herramientas destacan dos librerías
como las más empleadas:

\begin{itemize}
\item
  \texttt{bootstrap}: contiene las rutinas (bootstrap, cross-validation,
  jackknife) y los datos del libro ``An Introduction to the Bootstrap'' de B.
  Efron y R. Tibshirani, 1993, Chapman and Hall. La librería fue
  desarrollada originalmente en \texttt{S} por Rob Tibshirani y exportada a \texttt{R} por
  Friedrich Leisch. Es útil para desarrollar los ejemplos que se citan en
  ese libro.
\item
  \texttt{boot}: incluye las funciones y conjuntos de datos utilizados en el libro
  ``Bootstrap Methods and Their Applications'' de A. C. Davison y D. V. Hinkley, 1997,
  Cambridge University Press. Esta librería fue desarrollada originalmente
  en \texttt{S} por Angelo J. Canty y posteriormente exportada a \texttt{R} (ver \href{http://cran.fhcrc.org/doc/Rnews/Rnews_2002-3.pdf}{Canty, 2002}).
  Este paquete es mucho más completo que el paquete \texttt{bootstrap}, forma parte de la distribución estándar de \texttt{R} y es el que emplearemos como referencia en este libro (ver Sección \ref{intro-pkgboot}).
\end{itemize}

Por otra parte existen numerosas rutinas (scripts) realizadas en \texttt{R} por
diversos autores, que están disponibles en Internet
(por ejemplo, puede ser interesante realizar una búsqueda en
\url{https://rseek.org}).

El bootstrap uniforme se puede implementar fácilmente. Por ejemplo,
una rutina general para el caso univariante sería la siguiente:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\textquotesingle{} @param x vector que contiene la muestra.}
\CommentTok{\#\textquotesingle{} @param B número de réplicas bootstrap.}
\CommentTok{\#\textquotesingle{} @param statistic función que calcula el estadístico.}
\NormalTok{boot.strap0 }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x, }\AttributeTok{B=}\DecValTok{1000}\NormalTok{, }\AttributeTok{statistic=}\NormalTok{mean)\{}
\NormalTok{  ndat }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(x)}
\NormalTok{  x.boot }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(x, ndat}\SpecialCharTok{*}\NormalTok{B, }\AttributeTok{replace=}\ConstantTok{TRUE}\NormalTok{)}
\NormalTok{  x.boot }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(x.boot, }\AttributeTok{ncol=}\NormalTok{B, }\AttributeTok{nrow=}\NormalTok{ndat)}
\NormalTok{  stat.boot }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(x.boot, }\DecValTok{2}\NormalTok{, statistic)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Podríamos aplicar esta función a la muestra de tiempos de vida de
microorganismos con el siguiente código:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fstatistic }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x)\{}
  \CommentTok{\#  mean(x)}
  \CommentTok{\#  mean(x, trim=0.2)}
  \FunctionTok{median}\NormalTok{(x)}
  \CommentTok{\#  max(x)}
\NormalTok{\}}

\NormalTok{B }\OtherTok{\textless{}{-}} \DecValTok{1000}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{stat.dat }\OtherTok{\textless{}{-}} \FunctionTok{fstatistic}\NormalTok{(muestra)}
\NormalTok{stat.boot }\OtherTok{\textless{}{-}} \FunctionTok{boot.strap0}\NormalTok{(muestra, B, fstatistic)}

\NormalTok{res.boot }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(stat.dat, }\FunctionTok{mean}\NormalTok{(stat.boot)}\SpecialCharTok{{-}}\NormalTok{stat.dat, }\FunctionTok{sd}\NormalTok{(stat.boot))}
\FunctionTok{names}\NormalTok{(res.boot) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Estadístico"}\NormalTok{, }\StringTok{"Sesgo"}\NormalTok{, }\StringTok{"Error Std."}\NormalTok{)}
\NormalTok{res.boot}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Estadístico       Sesgo  Error Std. 
##   0.6110000   0.0609260   0.2481362
\end{verbatim}

La función \texttt{boot.strap0()} anterior no es adecuada para el caso multivariante
(por ejemplo cuando estamos interesados en regresión).
Como se mostró en la Sección \ref{intro-implementacion}
sería preferible emplear remuestras del vector de índices. Por ejemplo:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\textquotesingle{} @param datos vector, matriz o data.frame que contiene los datos.}
\CommentTok{\#\textquotesingle{} @param B número de réplicas bootstrap.}
\CommentTok{\#\textquotesingle{} @param statistic función con al menos dos parámetros, }
\CommentTok{\#\textquotesingle{} los datos y el vector de índices de remuestreo, }
\CommentTok{\#\textquotesingle{} y que devuelve el vector de estadísticos.}
\CommentTok{\#\textquotesingle{} @param ... parámetros adicionales de la función statistic.}
\NormalTok{boot.strap }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(datos, }\AttributeTok{B=}\DecValTok{1000}\NormalTok{, statistic, ...) \{}
\NormalTok{  ndat }\OtherTok{\textless{}{-}} \FunctionTok{NROW}\NormalTok{(datos)}
\NormalTok{  i.boot }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(ndat, ndat}\SpecialCharTok{*}\NormalTok{B, }\AttributeTok{replace=}\ConstantTok{TRUE}\NormalTok{)}
\NormalTok{  i.boot }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(i.boot, }\AttributeTok{ncol=}\NormalTok{B, }\AttributeTok{nrow=}\NormalTok{ndat)}
\NormalTok{  stat.boot }\OtherTok{\textless{}{-}} \FunctionTok{drop}\NormalTok{(}\FunctionTok{apply}\NormalTok{(i.boot, }\DecValTok{2}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(i) }\FunctionTok{statistic}\NormalTok{(datos, i, ...)))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

El paquete \texttt{boot}, descrito a continuación, emplea una implementación similar.

\hypertarget{intro-pkgboot}{%
\subsection{\texorpdfstring{El paquete \texttt{boot}}{El paquete boot}}\label{intro-pkgboot}}

La función principal de este paquete es la función \texttt{boot()} que implementa distintos métodos de remuestreo para datos i.i.d..
En su forma más simple permite realizar bootstrap uniforme (que en la práctica también se denomina habitualmente \emph{bootstrap noparamétrico}):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{boot}\NormalTok{(data, statistic, R)}
\end{Highlighting}
\end{Shaded}

donde \texttt{data} es un vector, matriz o \texttt{data.frame} que contiene los datos,
\texttt{R} es el número de réplicas bootstrap, y \texttt{statistic} es una función
con al menos dos parámetros (con las opciones por defecto),
los datos y el vector de índices de remuestreo,
y que devuelve el vector de estadísticos.

Por ejemplo, para hacer inferencia sobre la mediana del tiempo de microorganismos,
podríamos emplear el siguiente código:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(boot)}
\NormalTok{muestra }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{0.143}\NormalTok{, }\FloatTok{0.182}\NormalTok{, }\FloatTok{0.256}\NormalTok{, }\FloatTok{0.26}\NormalTok{, }\FloatTok{0.27}\NormalTok{, }\FloatTok{0.437}\NormalTok{, }\FloatTok{0.509}\NormalTok{, }
             \FloatTok{0.611}\NormalTok{, }\FloatTok{0.712}\NormalTok{, }\FloatTok{1.04}\NormalTok{, }\FloatTok{1.09}\NormalTok{, }\FloatTok{1.15}\NormalTok{, }\FloatTok{1.46}\NormalTok{, }\FloatTok{1.88}\NormalTok{, }\FloatTok{2.08}\NormalTok{)}

\NormalTok{statistic }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, i)\{}
  \CommentTok{\# remuestra \textless{}{-} data[i]; median(remuestra)}
  \FunctionTok{median}\NormalTok{(data[i])}
\NormalTok{\}}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{res.boot }\OtherTok{\textless{}{-}} \FunctionTok{boot}\NormalTok{(muestra, statistic, }\AttributeTok{R =} \DecValTok{1000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

El resultado que devuelve esta función es un objeto de clase \texttt{boot}, una lista con los siguientes componentes:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(res.boot)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "t0"        "t"         "R"         "data"      "seed"      "statistic"
##  [7] "sim"       "call"      "stype"     "strata"    "weights"
\end{verbatim}

Además de los parámetros de entrada (incluyendo los valores por defecto), contiene tres componentes adicionales:

\begin{itemize}
\item
  \texttt{tO}: el valor observado del estadístico
  (su evaluación en los datos originales).
\item
  \texttt{t}: la matriz de réplicas bootstrap del estadístico
  (cada fila se corresponde con una remuestra).
\item
  \texttt{seed}: el valor inicial de la semilla (\texttt{.Random.seed})
  empleada para la generación de las réplicas.
\end{itemize}

Este tipo de objetos dispone de dos métodos principales:
el método \texttt{print()} que muestra un resumen de los resultados
(incluyendo aproximaciones bootstrap del sesgo y del error
estándar de los estadísticos; ver Capítulo \ref{prec-sesgo}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res.boot}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## ORDINARY NONPARAMETRIC BOOTSTRAP
## 
## 
## Call:
## boot(data = muestra, statistic = statistic, R = 1000)
## 
## 
## Bootstrap Statistics :
##     original   bias    std. error
## t1*    0.611 0.058523   0.2526519
\end{verbatim}

y el método \texttt{plot()} que genera gráficas básicas de diagnosis
de los resultados (correspondientes al estadístico determinado por el parámetro \texttt{index}, por defecto \texttt{=\ 1}): {[}Figura \ref{fig:plot-res-boot}{]}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(res.boot)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[!htb]

{\centering \includegraphics[width=0.7\linewidth]{01-intro_files/figure-latex/plot-res-boot-1} 

}

\caption{Gráficos de diagnóstico de los resultados bootstrap de la mediana de los tiempos de vida de microorganismos.}\label{fig:plot-res-boot}
\end{figure}

Es recomendable examinar la distribución bootstrap del estimador (o estadístico) para detectar posibles problemas.
Como en este caso puede ocurrir que el estadístico bootstrap tome pocos valores distintos, lo que indicaría que el número de réplicas bootstrap es insuficiente o que hay algún problema con método de remuestreo empleado (en este caso la distribución objetivo es continua).
Se darán más detalles sobre los posibles problemas del bootstrap uniforme en la Sección \ref{deficien-unif}.

Además de estos métodos, las principales funciones de interés serían:

\begin{itemize}
\item
  \texttt{jack.after.boot()}: genera un gráfico para diagnósticar la inluencia
  de las observaciones individuales en los resultados bootstrap
  (se representan los cuantiles frente a las diferencias en el estadístico
  al eliminar una observación; este gráfico también se puede obtener estableciendo
  \texttt{jack\ =\ TRUE} en \texttt{plot.boot()}).
\item
  \texttt{boot.array()}: genera la matriz de índices a partir de la que se obtuvieron las remuestras (permite reconstruir las remuestras bootstrap).
\item
  \texttt{boot.ci()}: construye distintos tipos de intervalos de confianza
  (se tratarán en el Capítulo \ref{icboot}) dependiendo del parámetro \texttt{type}:

  \begin{itemize}
  \item
    \texttt{"norm"}: utiliza la distribución asintótica normal considerando las
    aproximaciones bootstrap del sesgo y de la varianza.
  \item
    \texttt{"basic"}: emplea el estadístico \(R = \hat \theta - \theta\) para la
    construcción del intervalo de confianza.
  \item
    \texttt{"stud"}: calcula el intervalo a partir del estadístico estudentizado
    \(R = \left( \hat \theta - \theta \right) / \sqrt{Var(\hat \theta)}\).
  \item
    \texttt{"perc"}: utiliza directamente la distribución bootstrap del estadístico
    (\(R = \hat \theta\)).
  \item
    \texttt{"bca"}: emplea el método \(BCa\) (``bias-corrected and accelerated'')
    propuesto por Efron (1987) (ver Sección 5.3.2 de Davison y Hinkley, 1997).
  \item
    \texttt{"all"}: calcula los cinco tipos de intervalos anteriores.
  \end{itemize}
\end{itemize}

Como ya se comentó, la función \texttt{boot()} admite estadísticos multivariantes
(haciendo que la función \texttt{statistic} devuelva un vector en lugar de un escalar),
pero por defecto las funciones anteriores consideran el primer componente
como el estadístico principal.
Para obtener resultados de otros componentes del vector de estadísticos
habrá que establecer el parámetro \texttt{index} igual al índice deseado.
Además, en algunos casos (por ejemplo para la obtención de intevalos de confianza
estudentizados con la función \texttt{boot.ci()}) se supone, por defecto, que el segundo
componente del vector de estadísticos contiene estimaciones de la varianza del
estadístico para cada réplica boostrap.

\begin{example}[Inferencia sobre la media con varianza desconocida, continuación]
\protect\hypertarget{exm:media-dt-desconocida-boot}{}{\label{exm:media-dt-desconocida-boot} \iffalse (Inferencia sobre la media con varianza desconocida, continuación) \fi{} } \vspace{0.5cm}

Continuando con el Ejemplo \ref{exm:media-dt-desconocida} de
inferencia sobre la media con varianza desconocida.
Para obtener la estimación por intervalo de confianza del tiempo de vida medio
de los microorganismos con el paquete \texttt{boot}, podríamos emplear
el siguiente código:
\end{example}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(boot)}
\NormalTok{muestra }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{0.143}\NormalTok{, }\FloatTok{0.182}\NormalTok{, }\FloatTok{0.256}\NormalTok{, }\FloatTok{0.26}\NormalTok{, }\FloatTok{0.27}\NormalTok{, }\FloatTok{0.437}\NormalTok{, }\FloatTok{0.509}\NormalTok{, }
             \FloatTok{0.611}\NormalTok{, }\FloatTok{0.712}\NormalTok{, }\FloatTok{1.04}\NormalTok{, }\FloatTok{1.09}\NormalTok{, }\FloatTok{1.15}\NormalTok{, }\FloatTok{1.46}\NormalTok{, }\FloatTok{1.88}\NormalTok{, }\FloatTok{2.08}\NormalTok{)}

\NormalTok{statistic }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, i)\{}
\NormalTok{  remuestra }\OtherTok{\textless{}{-}}\NormalTok{ data[i]}
  \FunctionTok{c}\NormalTok{(}\FunctionTok{mean}\NormalTok{(remuestra), }\FunctionTok{var}\NormalTok{(remuestra)}\SpecialCharTok{/}\FunctionTok{length}\NormalTok{(remuestra))}
\NormalTok{\}}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{res.boot }\OtherTok{\textless{}{-}} \FunctionTok{boot}\NormalTok{(muestra, statistic, }\AttributeTok{R =} \DecValTok{1000}\NormalTok{)}
\NormalTok{res.boot}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## ORDINARY NONPARAMETRIC BOOTSTRAP
## 
## 
## Call:
## boot(data = muestra, statistic = statistic, R = 1000)
## 
## 
## Bootstrap Statistics :
##      original       bias    std. error
## t1* 0.8053333  0.003173267 0.158330646
## t2* 0.0259338 -0.002155755 0.007594682
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{boot.ci}\NormalTok{(res.boot)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
## Based on 1000 bootstrap replicates
## 
## CALL : 
## boot.ci(boot.out = res.boot)
## 
## Intervals : 
## Level      Normal              Basic             Studentized     
## 95%   ( 0.4918,  1.1125 )   ( 0.4825,  1.0980 )   ( 0.4715,  1.2320 )  
## 
## Level     Percentile            BCa          
## 95%   ( 0.5127,  1.1282 )   ( 0.5384,  1.1543 )  
## Calculations and Intervals on Original Scale
\end{verbatim}

El intervalo marcado como \texttt{Studentized} se obtuvo empleando el mismo estadístico
del Ejemplo \ref{exm:media-dt-desconocida}.

\textbf{\emph{Modificaciones del bootstrap uniforme}}

Establecenciendo parámetros adicionales de la función \texttt{boot} se pueden llevar
a cabo modificaciones del bootstrap uniforme.
Algunos de estos parámetros son los siguientes:

\begin{itemize}
\item
  \texttt{strata}: permite realizar remuestreo estratificado estableciendo este parámetro
  como un vector numérico o factor que defina los grupos.
\item
  \texttt{sim\ =\ c("ordinary"\ ,\ "parametric",\ "balanced",\ "permutation",\ "antithetic")}:
  permite establecer distintos tipos de remuestreo.
  Por defecto es igual a \texttt{"ordinary"} que se corresponde con el bootstrap uniforme,
  descrito anteriormente. Entre el resto de opciones destacaríamos
  \texttt{sim\ =\ "permutation"}, que permite realizar contrastes de
  permutaciones (remuestreo sin reemplazamiento), y \texttt{sim\ =\ "parametric"},
  que permite realizar bootstrap paramétrico (Sección \ref{modunif-boot-par}).
  En este último caso también habrá que establecer los parámetros \texttt{ran.gen} y
  \texttt{mle}, y la función \texttt{statistics} no empleará el segundo parámetro de índices.
\item
  \texttt{ran.gen}: función que genera los datos. El primer argumento será el conjunto de datos
  original y el segundo un vector de parámetros adicionales
  (normalmente los valores de los parámetros de la distribución).
\item
  \texttt{mle}: parámetros de la distribución (típicamente estimados por máxima verosimilitud)
  o parámetros adicionales para \texttt{ran.gen} ó \texttt{statistics}.
\end{itemize}

Además hay otros parámetros para el procesamiento en paralelo: \texttt{parallel\ =\ c("no",\ "multicore",\ "snow")}, \texttt{ncpus}, \texttt{cl}.
En el Apéndice \ref{intro-hpc} se incluye una pequeña introducción al procesamiento en paralelo y se muestran algunos ejemplos sobre el uso de estos parámetros.
También se puede consultar la ayuda de la función \texttt{boot()} (\texttt{?boot}).

El paquete \texttt{boot} también incluye otras funciones que implementan métodos
boostrap para otros tipos de datos, como la función \texttt{censboot()} para datos
censurados (Capítulo \ref{bootcen}) o la función \texttt{tsboot()} para series de tiempo (Capítulo \ref{bootdep}).

Finalmente destacar que hay numerosas extensiones implementadas en otros paquetes utilizando el paquete \texttt{boot} (ver \emph{Reverse dependencies} en la \href{https://cran.r-project.org/package=boot}{web de CRAN}).
Por ejemplo en la Sección \ref{boot-reg} se ilustrará el uso de la función \texttt{Boot()} del paquete \texttt{car} para hacer inferencia sobre modelos de regresión.

\hypertarget{boot-unif-multi}{%
\subsection{Ejemplo: Bootstrap uniforme multidimensional}\label{boot-unif-multi}}

Como ya se mostró en las Secciones \ref{intro-implementacion} y \ref{intro-paquetes} podemos implementar el bootstrap uniforme en el caso multidimensional (denominado también \emph{remuestreo de casos} o \emph{bootstrap de las observaciones}) de modo análogo al unidimensional.

Consideraremos como ejemplo el conjunto de datos \texttt{Prestige} del paquete \texttt{carData}, y supongamos que queremos realizar inferencias sobre el coeficiente de correlación entre \texttt{prestige} (puntuación de ocupaciones obtenidas a partir de una encuesta) e\texttt{income} (media de ingresos en la ocupación).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(Prestige, }\AttributeTok{package =} \StringTok{"carData"}\NormalTok{)}
\FunctionTok{str}\NormalTok{(Prestige)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    102 obs. of  6 variables:
##  $ education: num  13.1 12.3 12.8 11.4 14.6 ...
##  $ income   : int  12351 25879 9271 8865 8403 11030 8258 14163 11377 11023 ...
##  $ women    : num  11.16 4.02 15.7 9.11 11.68 ...
##  $ prestige : num  68.8 69.1 63.4 56.8 73.5 77.6 72.6 78.1 73.1 68.8 ...
##  $ census   : int  1113 1130 1171 1175 2111 2113 2133 2141 2143 2153 ...
##  $ type     : Factor w/ 3 levels "bc","prof","wc": 2 2 2 2 2 2 2 2 2 2 ...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# with(Prestige, cor(income, prestige))}
\FunctionTok{cor}\NormalTok{(Prestige}\SpecialCharTok{$}\NormalTok{income, Prestige}\SpecialCharTok{$}\NormalTok{prestige)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.7149057
\end{verbatim}

En el siguiente código se emplea el paquete \texttt{boot} para realizar bootstrap uniforme multidimensional sobre este estadístico:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(boot)}

\NormalTok{statistic }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, i)\{}
\NormalTok{  remuestra }\OtherTok{\textless{}{-}}\NormalTok{ data[i, ]}
  \FunctionTok{cor}\NormalTok{(remuestra}\SpecialCharTok{$}\NormalTok{income, remuestra}\SpecialCharTok{$}\NormalTok{prestige)}
\NormalTok{\}}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{B }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{res.boot }\OtherTok{\textless{}{-}} \FunctionTok{boot}\NormalTok{(Prestige, statistic, }\AttributeTok{R =}\NormalTok{ B)}
\NormalTok{res.boot}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## ORDINARY NONPARAMETRIC BOOTSTRAP
## 
## 
## Call:
## boot(data = Prestige, statistic = statistic, R = B)
## 
## 
## Bootstrap Statistics :
##      original      bias    std. error
## t1* 0.7149057 0.006306905  0.04406473
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(res.boot)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{01-intro_files/figure-latex/unnamed-chunk-22-1} \end{center}

En este caso podemos observar que la distribución bootstrap del estimador es asimétrica, por lo que asumir que su distribución es normal podría no ser adecuado (por ejemplo para la construcción de intervalos de confianza, que se tratarán en la Sección \ref{icboot-trans}).

Como comentario final, nótese que en principio el paquete boot está diseñado para obtener réplicas bootstrap de un estimador, por lo que si lo que nos interesa es emplear otro estadístico habría que construirlo a partir de ellas (como hacen otras funciones secundarias como \texttt{boot.ci()}).
Por ejemplo, si queremos emplear el estadístico \(R = \hat \theta - \theta\)
(bootstrap percentil básico o natural), podemos obtener la correspondiente distribución bootstrap (aproximada por Monte Carlo) con el siguiente código:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{estadistico\_boot }\OtherTok{\textless{}{-}}\NormalTok{ res.boot}\SpecialCharTok{$}\NormalTok{t }\SpecialCharTok{{-}}\NormalTok{ res.boot}\SpecialCharTok{$}\NormalTok{t0 }
\FunctionTok{hist}\NormalTok{(estadistico\_boot)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{01-intro_files/figure-latex/unnamed-chunk-23-1} \end{center}

A partir de la distribución empírica del estadístico bootstrap \(R^{\ast} = \hat \theta^{\ast} - \hat \theta\) aproximaríamos la característica de interés de la distribución en el muestreo de \(R = \hat \theta - \theta\).
Por ejemplo, para aproximar \(\psi \left( u \right) =P\left( R\leq u \right)\) emplearíamos la frecuencia relativa:
\[\hat{\psi}_{B}\left( u \right) =
\frac{1}{B}\sum_{i=1}^{B}\mathbf{1}\left\{ R^{\ast (i)}\leq u\right\}.\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{u }\OtherTok{\textless{}{-}} \DecValTok{0}
\FunctionTok{sum}\NormalTok{(estadistico\_boot }\SpecialCharTok{\textless{}=}\NormalTok{ u)}\SpecialCharTok{/}\NormalTok{B}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.427
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Equivalentemente:}
\FunctionTok{mean}\NormalTok{(estadistico\_boot }\SpecialCharTok{\textless{}=}\NormalTok{ u)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.427
\end{verbatim}

\hypertarget{prec-sesgo}{%
\chapter{Estimación de la precisión y el sesgo de un estimador}\label{prec-sesgo}}

Uno de los problemas más interesantes que pueden ser abordados desde la
perspectiva de los métodos de remuestreo es el de la estimación del
sesgo y la precisión de un estimador. En dicho contexto surgió el método
Jackknife (bastante antes que el bootstrap), que, en ese sentido, puede
considerarse el método de remuestreo más antiguo como tal.

\hypertarget{prec-sesgo-boot}{%
\section{Estimación bootstrap de la precisión y el sesgo de un estimador}\label{prec-sesgo-boot}}

Consideremos \(\mathbf{X}=\left( X_1,\ldots ,X_n \right)\) una
m.a.s. de una población con distribución \(F\) y supongamos que tenemos
interés en realizar inferencia sobre un parámetro de la población
\(\theta =\theta \left( F \right)\). Consideremos un estimador,
\(\hat{\theta}=T\left( \mathbf{X} \right)\), de dicho parámetro y
definamos el estadístico
\[R=R\left( \mathbf{X}, F \right) = T\left( \mathbf{X} \right) 
- \theta \left( F \right) = \hat{\theta} - \theta.\]
El sesgo del estimador no es más que la esperanza del
estadístico \(R\) y la varianza de \(\hat{\theta}\) es también la varianza
de \(R\) (pues \(\theta\) no es aleatorio). Además, el error cuadrático
medio del estimador también se puede escribir como el momento de orden 2
de \(R\):
\[\begin{aligned}
Sesgo\left( \hat{\theta} \right) &= E\left( \hat{\theta}-\theta \right)
=E\left( R \right), \\
Var\left( \hat{\theta} \right) &= Var\left( \hat{\theta}-\theta \right)
=Var\left( R \right), \\
MSE\left( \hat{\theta} \right) &= E\left[ \left( \hat{\theta}-\theta \right)
^2\right] =E\left( R^2 \right).
\end{aligned}\]

Dado que el principio bootstrap es útil para aproximar la distribución
en muestreo del estadístico \(R\), entonces también permitirá aproximar
sus momentos (su esperanza, su varianza, la esperanza de su cuadrado) y
así proceder según sigue:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Estimar la función de distribución de probabilidad mediante \(\hat{F}\)
\item
  Para cada \(i=1,\ldots ,n\) arrojar \(X_i^{\ast}\) a partir de
  \(\hat{F}\) y obtener
  \(\mathbf{X}^{\ast}=\left( X_1^{\ast}, \ldots ,X_n^{\ast} \right)\)
\item
  Calcular \(R^{\ast}=R\left( \mathbf{X}^{\ast},\hat{F} \right) =T\left( \mathbf{X}^{\ast} \right) -\theta \left( \hat{F} \right) = \hat{\theta}^{\ast}- \theta \left( \hat{F} \right)\)
\item
  Repetir \(B\) veces los pasos 2-3 para obtener las réplicas bootstrap
  \(R^{\ast (1)}, \ldots, R^{\ast (B)}\)
\item
  Calcular el estimador bootstrap del sesgo:
  \[Sesgo^{\ast}\left( \hat{\theta}^{\ast} \right) =\bar{R}^{\ast}=\frac{1
  }{B}\sum_{b=1}^{B}R^{\ast (b)}.\]
\end{enumerate}

El algoritmo anterior es útil para aproximar por bootstrap el sesgo. Si
se desea aproximar la varianza puede sustituirse al paso 5 por:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Calcular el estimador bootstrap de la varianza:
  \[Var^{\ast}\left( \hat{\theta}^{\ast} \right) =\frac{1}{B}
  \sum_{b=1}^{B}\left( R^{\ast (b)}-\bar{R}^{\ast} \right)^2\]
\end{enumerate}

Si se trata de aproximar por bootstrap el error cuadrático medio, el
paso 5 pasaría a ser:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Calcular el estimador bootstrap del error cuadrático medio:
  \(MSE^{\ast}\left( \hat{\theta}^{\ast} \right) =\frac{1}{B}\sum_{b=1}^{B}R^{\ast (b) 2}\)
\end{enumerate}

En el caso de la varianza podría ahorrarse algunos cálculos definiendo
directamente \(R=T\left( \mathbf{X} \right) =\hat{\theta}\) y,
consecuentemente,
\(R^{\ast}=T\left( \mathbf{X}^{\ast} \right) = \hat{\theta}^{\ast}\). Así otro algoritmo de cálculo algo menos
intensivo sería:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Estimar la función de distribución de probabilidad mediante \(\hat{ F}\)
\item
  Para cada \(i=1,\ldots ,n\) arrojar \(X_i^{\ast}\) a partir de
  \(\hat{F}\) y obtener
  \(\mathbf{X}^{\ast}=\left( X_1^{\ast}, \ldots, X_n^{\ast} \right)\)
\item
  Calcular
  \(T\left( \mathbf{X}^{\ast} \right) = \hat{\theta}^{\ast}\)
\item
  Repetir \(B\) veces los pasos 2-3 para obtener las réplicas bootstrap
  \(\hat{\theta}^{\ast (1)}, \ldots, \hat{\theta}^{\ast(B)}\)
\item
  Calcular
  \(\overline{\hat{\theta}^{\ast}}=\frac{1}{B}\sum_{b=1}^{B}\hat{ \theta}^{\ast (b)}\) y, con ello,
  \(Var^{\ast}\left( \hat{\theta}^{\ast} \right) =\frac{1}{B}\sum_{b=1}^{B}\left( \hat{\theta}^{\ast \left(b \right)}-\overline{\hat{\theta}^{\ast}} \right)^2\)
\end{enumerate}

Es interesante mencionar que esto permite aproximar por bootstrap
(mediante Monte Carlo) la varianza de un estimador sin conocer una
expresión explícita para dicha varianza teórica.
En general, el estimador, \(\hat{F}\), de \(F\) a utilizar en los pasos 1-2
de estos algoritmos se elije según proceda al caso. En el caso del
bootstrap uniforme sería \(\hat{F}=F_n\) y se puede proceder
como se mostró en la Sección \ref{intro-implementacion}.

\hypertarget{ejemplo-la-media-muestral}{%
\subsection{Ejemplo: la media muestral}\label{ejemplo-la-media-muestral}}

Consideremos como parámetro de interés la media de la población,
\(\theta =\theta \left( F \right) =\mu =\int xdF\left( x \right)\), y
tomemos como estimador la media muestral:
\(\hat{\theta}=\hat{\mu}=T\left( \mathbf{X} \right) =\bar{X}=\frac{1}{n}\sum_{j=1}^{n}X_j\).
Supongamos que deseamos estudiar la varianza de este estimador:
\(Var\left( \hat{\theta} \right) =Var\left( \bar{X} \right) =\frac{\sigma^2}{n}\).

A la hora de aproximar por bootstrap \(Var\left( \hat{\theta} \right)\),
si no disponemos de ninguna otra información adicional (como que la
distribución sea de cierta familia paramétrica o que sea continua),
parece razonable elegir como método de remuestreo el bootstrap uniforme.
En tal caso el algoritmo bootstrap de Monte Carlo procedería de esta
forma:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Para cada \(i=1,\ldots ,n\) arrojar \(U_i\sim \mathcal{U}\left( 0,1 \right)\) y
  hacer \(X_i^{\ast}=X_{\left\lfloor nU_i\right\rfloor +1}\)
\item
  Calcular
  \(T\left( \mathbf{X}^{\ast} \right) =\bar{X}^{\ast}= \frac{1}{n}\sum_{i=1}^{n}X_i^{\ast}\)
\item
  Repetir \(B\) veces los pasos 1-2 para obtener las réplicas bootstrap
  \(\bar{X}^{\ast (1)}, \ldots, \bar{X}^{\ast (B)}\)
\item
  Calcular \(\overline{\bar{X}^{\ast}}=\frac{1}{B}\sum_{b=1}^{B}\)
  \(\bar{X}^{\ast (b)}\) y
  \(Var^{\ast}\left( \bar{X}^{\ast} \right) =\frac{1}{B} \sum_{b=1}^{B}\left( \bar{X}^{\ast (b)}-\overline{\bar{X}^{\ast}} \right)^2\)
\end{enumerate}

De todas formas, en este caso puede verse fácilmente que no es necesario
realizar Monte Carlo. En efecto,
\[\begin{aligned}
Var^{\ast}\left( \bar{X}^{\ast} \right) & = \frac{1}{n^2}
\sum_{i=1}^{n}Var^{\ast}\left( X_i^{\ast} \right) =\frac{1}{n}Var^{\ast}\left( X_1^{\ast} \right) \\
& = \frac{1}{n}\left\{ E^{\ast}\left( X_1^{\ast 2} \right) -
\left[ E^{\ast}\left( X_1^{\ast} \right) \right]^2\right\} =\frac{1}{n}\left[ 
\frac{1}{n}\sum_{j=1}^{n}X_j^2-\left( \frac{1}{n}\sum_{j=1}^{n}X_j \right)^2 \right] \\
& = \frac{1}{n^2}\sum_{j=1}^{n}\left( X_j-\bar{X} \right)^2=\frac{S_n^2}{n},
\end{aligned}\]
que es precisamente el estimador
plug-in de la varianza de la media muestral.

\begin{example}[Aproximación bootstrap de la precisión de estimaciones del tiempo de vida medio de microorganismos]
\protect\hypertarget{exm:estimacion-boot-precision}{}{\label{exm:estimacion-boot-precision} \iffalse (Aproximación bootstrap de la precisión de estimaciones del tiempo de vida medio de microorganismos) \fi{} } \vspace{0.5cm}

Continuando con el ejemplo de los tiempos de vida de microorganismos,
supongamos que queremos estimar
la precisión de dos estimadores de su vida media: media muestral y
mediana muestral, a partir de los datos observados: 0.143, 0.182, 0.256, 0.260, 0.270,
0.437, 0.509, 0.611, 0.712, 1.04, 1.09, 1.15, 1.46, 1.88, 2.08.
\end{example}

La estimación media muestral resulta \(\bar{X}=0.8053333\). Por su
parte la estimación mediana muestral es \(x_{\left( 8 \right)}=0.611\).

La varianza del estimador media muestral, \(\bar{X}\) es \(Var\left( \bar{X} \right) =\frac{\sigma^2}{n}\), desconocida en este caso.
Su estimación bootstrap (idéntica a la plug-in) mediante un remuestreo
uniforme es calculable sin necesidad de realizar Monte Carlo y
resulta:
\[Var^{\ast}\left( \bar{X}^{\ast} \right) =\frac{1}{n^2}
\sum_{j=1}^{n}\left( x_j-\bar{X} \right)^2=0.024204877.\]
Con lo cual \(\sqrt{Var^{\ast}\left( \bar{X}^{\ast} \right)}=\sqrt{ 0.024204877}= 0.155\,58\)

Si consideramos ahora la mediana muestral (como estimador de la media),
también sabemos que su distribución bootstrap puede calcularse de forma
explícita, sin necesidad de realizar Monte Carlo. Su masa de
probabilidad viene dada por:

\[\begin{aligned}
P^{\ast}\left( X_{\left( 8 \right)}^{\ast}=x_{(1)} \right)
&= 1-\sum_{k=0}^{m-1}\binom{n}{k}\left( \frac{1}{n} \right)^{k}\left( \frac{
n-1}{n} \right)^{n-k} \\
P^{\ast}\left( X_{\left( 8 \right)}^{\ast}=x_{(j)} \right)
&= \sum_{k=0}^{m-1}\binom{n}{k}\left[ \left( \frac{j-1}{n} \right)^{k}\left( 
\frac{n-j+1}{n} \right)^{n-k} - \left( \frac{j}{n} \right)^{k}\left( \frac{n-j}{n} \right)^{n-k}
\right] \\
\text{para }j &= 2,\ldots ,n
\end{aligned}\]

Con los datos concretos del ejemplo resulta:
\[\begin{array}{ll}
P^{\ast}\left( X_{\left( 8 \right)}^{\ast} = 0.143 \right) = 1.639\times 10^{-6}\text{, } 
& P^{\ast}\left( X_{\left( 8 \right)}^{\ast} = 0.182 \right) = 2.655\times 10^{-4},\\
P^{\ast}\left( X_{\left( 8 \right)}^{\ast} = 0.256 \right) = 3.973\times 10^{-3}\text{, } 
& P^{\ast}\left( X_{\left( 8 \right)}^{\ast} = 0.260 \right) = 2.121\times 10^{-2}, \\
P^{\ast}\left( X_{\left( 8 \right)}^{\ast} = 0.270 \right) = 6.278\times 10^{-2}\text{, }
& P^{\ast}\left( X_{\left( 8 \right)}^{\ast} = 0.437 \right) = 0.1249, \\
P^{\ast}\left( X_{\left( 8 \right)}^{\ast} = 0.509 \right) = 0.1832\text{, }
& P^{\ast}\left( X_{\left( 8 \right)}^{\ast} = 0.611 \right) = 0.2073,\\
P^{\ast}\left( X_{\left( 8 \right)}^{\ast} = 0.712 \right) = 0.1832\text{, }
& P^{\ast}\left( X_{\left( 8 \right)}^{\ast} = 1.04 \right) = 0.1249,\\
P^{\ast}\left( X_{\left( 8 \right)}^{\ast} = 1.09 \right) = 6.278\times 10^{-2}\text{, }
& P^{\ast}\left( X_{\left( 8 \right)}^{\ast} = 1.15 \right) = 2.121\times 10^{-2}, \\
P^{\ast}\left( X_{\left( 8 \right)}^{\ast} = 1.46 \right) = 3.973\times 10^{-3}\text{, }
& P^{\ast}\left( X_{\left( 8 \right)}^{\ast} = 1.88 \right) = 2.655\times 10^{-4}, \\
P^{\ast}\left( X_{\left( 8 \right)}^{\ast} = 2.08 \right) = 1.639\times 10^{-6}. 
\end{array}\]

Como consecuencia,
\[\begin{aligned}
E^{\ast}\left( X_{\left( 8 \right)}^{\ast} \right)
&= \sum_{j=1}^{15}x_{(j)}P^{\ast}\left( X_{\left( 8 \right)
}^{\ast}=x_{(j)} \right) =0.65749924 \\
E^{\ast}\left( X_{\left( 8 \right)}^{\ast 2} \right)
&= \sum_{j=1}^{15}x_{(j)}^2P^{\ast}\left( X_{\left( 8 \right)
}^{\ast}=x_{(j)} \right) =0.49500381 \\
Var^{\ast}\left( X_{\left( 8 \right)}^{\ast} \right)
&= 0.49500381-0.65749924^2=6.\, 2699\times 10^{-2} \\
\sqrt{Var^{\ast}\left( X_{\left( 8 \right)}^{\ast} \right)} &= \sqrt{
6.\, 2699\times 10^{-2}}=0.250\,40
\end{aligned}\]

Las estimaciones bootstrap de los errores cuadráticos medios de ambos
estimadores (como estimadores de la media poblacional) son:
\[\begin{aligned}
MSE^{\ast}\left( \bar{X}^{\ast} \right) &= \left( E^{\ast}\left( 
\bar{X}^{\ast} \right) -\bar{X} \right)^2+Var^{\ast}\left( 
\bar{X}^{\ast} \right) \\
&= Var^{\ast}\left( \bar{X}^{\ast} \right) =0.024204877, \\
MSE^{\ast}\left( X_{\left( 8 \right)}^{\ast} \right) &= \left( E^{\ast
}\left( X_{\left( 8 \right)}^{\ast} \right) -\bar{X} \right)
^2+Var^{\ast}\left( X_{\left( 8 \right)}^{\ast} \right) = \\
&= \left( 0.65749924-0.8053333 \right)^2+6.2699 \times 10^{-2}\\
&=  0.084554.
\end{aligned}\]

Una aproximación de Monte Carlo de estas varianzas bootstrap se puede
llevar a cabo mediante el siguiente código:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Para la muestra de TIEMPOS DE VIDA, estima (plug{-}in) la precisión }
\CommentTok{\# de la media muestral (desvmedia) y también aproxima por Monte Carlo}
\CommentTok{\# la estimación bootstrap de dicha precisión (desvmediaboot) y}
\CommentTok{\# también la precisión de la mediana muestral, de la cual no se}
\CommentTok{\# conoce su expresión, (desvmedianaboot).}
\CommentTok{\# También estima el sesgo bootstrap de esos dos estimadores }
\CommentTok{\# (sesgomediaboot y sesgomedianaboot, respectivamente).}

\NormalTok{muestra }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{0.143}\NormalTok{, }\FloatTok{0.182}\NormalTok{, }\FloatTok{0.256}\NormalTok{, }\FloatTok{0.26}\NormalTok{, }\FloatTok{0.27}\NormalTok{, }\FloatTok{0.437}\NormalTok{, }\FloatTok{0.509}\NormalTok{,}
    \FloatTok{0.611}\NormalTok{, }\FloatTok{0.712}\NormalTok{, }\FloatTok{1.04}\NormalTok{, }\FloatTok{1.09}\NormalTok{, }\FloatTok{1.15}\NormalTok{, }\FloatTok{1.46}\NormalTok{, }\FloatTok{1.88}\NormalTok{, }\FloatTok{2.08}\NormalTok{)}
\NormalTok{n }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(muestra)}
\NormalTok{varmedia }\OtherTok{\textless{}{-}}\NormalTok{ (}\DecValTok{1}\SpecialCharTok{/}\NormalTok{(n}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)) }\SpecialCharTok{*} \FunctionTok{sum}\NormalTok{((muestra }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(muestra))}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\CommentTok{\# Alternativamente: varmedia \textless{}{-} var(muestra)/n}
\NormalTok{desvmedia }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(varmedia)}

\CommentTok{\# Remuestreo}
\NormalTok{B }\OtherTok{\textless{}{-}} \FloatTok{1e+04}
\NormalTok{media }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(B)}
\NormalTok{mediana }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(B)}
\ControlFlowTok{for}\NormalTok{ (k }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{B) \{}
\NormalTok{    remuestra }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(muestra, n, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{    media[k] }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(remuestra)}
    \CommentTok{\# remordenada \textless{}{-} sort(remuestra)}
    \CommentTok{\# mediana[k] \textless{}{-} remordenada[8]}
\NormalTok{    mediana[k] }\OtherTok{\textless{}{-}} \FunctionTok{median}\NormalTok{(remuestra)}
\NormalTok{\}}

\CommentTok{\# Aproximaciones precisión}
\NormalTok{varmediaboot }\OtherTok{\textless{}{-}}\NormalTok{ (}\DecValTok{1}\SpecialCharTok{/}\NormalTok{B) }\SpecialCharTok{*} \FunctionTok{sum}\NormalTok{((media }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(media))}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{desvmediaboot }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(varmediaboot)}
\NormalTok{varmedianaboot }\OtherTok{\textless{}{-}}\NormalTok{ (}\DecValTok{1}\SpecialCharTok{/}\NormalTok{B) }\SpecialCharTok{*} \FunctionTok{sum}\NormalTok{((mediana }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(mediana))}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{desvmedianaboot }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(varmedianaboot)}
\NormalTok{desvmedia}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1555792
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{desvmediaboot}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1572797
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{desvmedianaboot}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.2518982
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Aproximaciones sesgo}
\NormalTok{sesgomediaboot }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(media) }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(muestra)}
\NormalTok{sesgomedianaboot }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(mediana) }\SpecialCharTok{{-}}\NormalTok{ muestra[}\DecValTok{8}\NormalTok{]}
\NormalTok{sesgomediaboot}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -0.001110327
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sesgomedianaboot}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.0449271
\end{verbatim}

Empleando el paquete \texttt{boot} el código sería más simple:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(boot)}
\NormalTok{statistic }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, i)\{}
\NormalTok{  remuestra }\OtherTok{\textless{}{-}}\NormalTok{ data[i]}
  \FunctionTok{c}\NormalTok{(}\FunctionTok{mean}\NormalTok{(remuestra), }\FunctionTok{median}\NormalTok{(remuestra))}
\NormalTok{\}}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{res.boot }\OtherTok{\textless{}{-}} \FunctionTok{boot}\NormalTok{(muestra, statistic, }\AttributeTok{R =}\NormalTok{ B)}
\NormalTok{res.boot}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## ORDINARY NONPARAMETRIC BOOTSTRAP
## 
## 
## Call:
## boot(data = muestra, statistic = statistic, R = B)
## 
## 
## Bootstrap Statistics :
##      original       bias    std. error
## t1* 0.8053333 7.115333e-05   0.1572396
## t2* 0.6110000 4.529410e-02   0.2511022
\end{verbatim}

Lamentablemente la función \texttt{print.boot()} calcula las aproximaciones
bootstrap del sesgo y de la precisión pero no las almacena.
En el caso más simple podríamos obtenerlas con el siguiente código:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{op }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(res.boot, }\FunctionTok{cbind}\NormalTok{(}
\NormalTok{  t0, }\FunctionTok{apply}\NormalTok{(t, }\DecValTok{2}\NormalTok{, mean, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{{-}}\NormalTok{  t0,}
  \FunctionTok{apply}\NormalTok{(t, }\DecValTok{2}\NormalTok{, sd, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{  ))}
\FunctionTok{rownames}\NormalTok{(op) }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\StringTok{"t"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\FunctionTok{ncol}\NormalTok{(res.boot}\SpecialCharTok{$}\NormalTok{t), }\StringTok{"*"}\NormalTok{)}
\FunctionTok{colnames}\NormalTok{(op) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"original"}\NormalTok{, }\StringTok{"bias  "}\NormalTok{, }\StringTok{" std. error"}\NormalTok{)}
\NormalTok{op}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      original       bias    std. error
## t1* 0.8053333 7.115333e-05   0.1572396
## t2* 0.6110000 4.529410e-02   0.2511022
\end{verbatim}

Como comentario adicional, si se emplea \(\hat F\) como aproximación de la distribución poblacional, lo habitual es que \(\hat\theta = T\left( \mathbf{X} \right) = \theta ( \hat{F} )\) sea el estimador de \(\theta(F)\) (por ejemplo \(\hat\theta = \theta(F_n)\) en el caso del bootstrap uniforme).
De esta forma, en el universo bootstrap el parámetro teórico se sustituye por el estimador y, por ejemplo, \(R = \hat \theta - \theta\) se traduce en \(R^{\ast} = \hat{\theta}^{\ast}- \hat \theta\).
Este es el principal motivo por el que la mayoría de las herramientas utilizadas en la práctica siempre asumen que ocurre esto, como es el caso de los métodos implementados en el paquete \texttt{boot} (y derivados).

En el caso general \(\hat\theta = T\left( \mathbf{X} \right)\) puede ser distinto de \(\theta ( \hat{F} )\) (por ejemplo, en el caso del bootstrap uniforme puede haber estimadores mejores que \(\theta( F_n)\) y típicamente se suele considerar el mejor estimador disponible).
Como se mostró al principio de la Sección \ref{prec-sesgo-boot}, en ese caso se debería sustituir en el universo bootstrap el parámetro teórico por \(\theta ( \hat{F} )\) y, por ejemplo, \(R = \hat \theta - \theta\) se traduciría en \(R^{\ast} = \hat \theta^{\ast} - \theta ( \hat{F} )\) (esto puede ser especialmente importante en la aproximación del sesgo).
Suponiendo que podemos obtener \(\theta ( \hat{F} )\), podemos desarrollar un código que implemente este algoritmo sin mucha dificultad (se podría tomar como base la primera aproximación en el ejemplo anterior), pero no se puede hacer de forma directa con el paquete \texttt{boot} (que asume siempre que \(\hat\theta = \theta ( \hat{F} ))\).
Habría que rehacer los cálculos que implementan las herramientas de este paquete (empleando directamente \texttt{res.boot\$t} y, por ejemplo, cambiando \texttt{t0} en la aproximación del sesgo en el código anterior o procediendo de forma similar a como se hace en la Sección \ref{npden-r-boot}, donde la estimación del sesgo se podría considerar importante).

Finalmente, especialmente en el caso de métodos bootstrap más avanzados, puede resultar difícil obtener \(\theta ( \hat{F} )\) y la aproximación habitual es reemplazarlo directamente por \(\hat \theta\).
Esta es una justificación más de la implementación que se suele hacer en la práctica.

\hypertarget{jackknife}{%
\section{Motivación del método Jackknife}\label{jackknife}}

El jackknife es probablemente el método de remuestreo, propiamente
dicho, más antiguo. Fue propuesto por Quenouille (1949) para estimar el
sesgo de un estimador. Tukey (1958) bautiza el método y lo utiliza para
estimar la varianza de un estimador. En realidad el jackknife no suele
utilizarse para aproximar la distribución de \(R\left( \mathbf{X},F \right)\),
sino más bien para estimar características de dicha
variable aleatoria, como su esperanza o su varianza.

La diferencia entre el bootstrap y el jackknife es muy fácil de expresar
en términos de los vectores de remuestreo. Así, el bootstrap uniforme
utiliza vectores de remuestreo de la forma
\(\mathbf{P}^{\ast}=\left( \frac{m_1}{n},\ldots ,\frac{m_n}{n} \right)\), con
\(m_i\in \mathbb{Z}^{+}\), \(i=1,\ldots ,n\), mientras que el jackknife considera
vectores de remuestreo de la forma
\[\mathbf{P}_{(i)}^{\ast}=\left( \frac{1}{n-1},\ldots ,\underset{(i)}{0}
,\ldots ,\frac{1}{n-1} \right).\]
En otras palabras todas las remuestras
jackknife posibles son tantas como el tamaño muestral y cada una
consiste en eliminar una observación de la muestra, quedándose con una
remuestra de tamaño \(n-1\) en la que las demás observaciones aparecen
exactamente con frecuencia \(1\).

Evidentemente, el número de posibles remuestras jackknife, \(n\), es
muchísimo más pequeño que el número de remuestras bootstrap,
\(\binom{2n-1}{n}\), lo cual permite calcular con rapidez las realizaciones
del estadístico de interés en todas las posibles remuestras jackknife.

\hypertarget{estimaciuxf3n-jackknife-de-la-precisiuxf3n-y-el-sesgo-de-un-estimador}{%
\section{Estimación Jackknife de la precisión y el sesgo de un estimador}\label{estimaciuxf3n-jackknife-de-la-precisiuxf3n-y-el-sesgo-de-un-estimador}}

Cuando estamos interesados en el sesgo o la varianza de un estimador
\(\hat{\theta}=\theta \left( \mathbf{X} \right)\) de un parámetro
\(\theta =\theta \left( F \right)\), el estadístico de interés suele
definirse como
\(R=R\left( \mathbf{X},F \right) =\hat{\theta}-\theta\).
En este caso
\[\begin{aligned}
Sesgo\left( \hat{\theta} \right) &= E\left( \hat{\theta} \right) -\theta
=E\left( R \right), \\
Var\left( \hat{\theta} \right) &= Var\left( \hat{\theta}-\theta \right)
=Var\left( R \right).
\end{aligned}\]
Así pues trataremos de usar el
jackknife para aproximar la esperanza y varianza de \(R\), o,
equivalentemente, el sesgo y la varianza de \(\hat{\theta}\).

El conjunto de remuestras jackknife es
\[\mathcal{X}_{jackk}=\left\{ \mathbf{X}^{\ast}=
\mathbf{X}_{(i)}=\left( X_1,\ldots ,X_{i-1},X_{i+1},\ldots
,X_n \right) : i=1,\ldots ,n\right\}\]
y todas ellas se consideran con
equiprobabilidad en el universo jackknife. Como primera tentativa
estimaríamos el sesgo y la varianza jackknife mediante:
\[\begin{aligned}
E^{\ast}\left( R^{\ast} \right) &= E_{jackk}^{\ast}\left( \hat{\theta}
^{\ast} \right) -\theta \left( \mathbf{X} \right) =\frac{1}{n}
\sum_{i=1}^{n}\theta \left( \mathbf{X}_{(i)} \right) -
\hat{\theta}=\overline{\theta \left( \mathbf{X}_{(\cdot)} \right)}-\hat{\theta}, \\
Var^{\ast}\left( R^{\ast} \right) &= Var_{jackk}^{\ast}\left( \hat{\theta}
^{\ast} \right) =\frac{1}{n}\sum_{i=1}^{n}\left[ \theta \left( 
\mathbf{X}_{(i)} \right) -\overline{\theta \left( 
\mathbf{X}_{(\cdot)} \right)}\right]^2,
\end{aligned}\]
con \(\overline{\theta \left( \mathbf{X}_{(\cdot)} \right)} = \frac{1}{n}\sum_{j=1}^{n}\theta \left( \mathbf{X}_{(j)} \right)\).

Sin embargo es evidente que las réplicas jackknife son mucho más
parecidas a la muestra original de lo que lo son las remuestras
bootstrap, en general. De hecho se puede demostrar que el valor absoluto
de ese estimador jackknife del sesgo es siempre menor que el valor
absoluto del sesgo bootstrap y que la estimación jackknife de la
varianza que se acaba de proponer también es menor que la varianza
bootstrap. En resumen, el método jackknife necesita de un \textbf{factor de
elevación} para que las estimaciones que proporciona sean consistentes.
La idea es elegir dicho factor de elevación como aquel que provoca que,
al multiplicar los estadísticos anteriores por él, y considerando como
parámetro a estimar la media o la varianza poblacional, el estimador
jackknife finalmente resultante sea insesgado. Así, el factor de
elevación resulta ser \(n-1\) y las estimaciones jackknife finales son

\[\begin{aligned}
Sesgo_{jackk}^{\ast}\left( \hat{\theta}^{\ast} \right) &= \left( n-1 \right)
\left( \overline{\theta \left( \mathbf{X}_{(\cdot)}
 \right)}-\hat{\theta} \right) =\frac{n-1}{n}\sum_{i=1}^{n}\left( \theta
\left( \mathbf{X}_{(i)} \right) -\hat{\theta} \right), \\
Var_{jackk}^{\ast}\left( \hat{\theta}^{\ast} \right) &= \frac{n-1}{n}
\sum_{i=1}^{n}\left[ \theta \left( \mathbf{X}_{(i)}
 \right) -\overline{\theta \left( \mathbf{X}_{(\cdot)}
 \right)}\right]^2.
\end{aligned}\]

Tomando como parámetro de interés la media, \(\theta =\mu\), tenemos que
\[\begin{aligned}
\overline{\theta \left( \mathbf{X}_{(\cdot)} \right)}-
\hat{\theta} &= \frac{1}{n}\sum_{i=1}^{n}\theta \left( 
\mathbf{X}_{(i)} \right) -\hat{\theta}=\frac{1}{n}\sum_{i=1}^{n}
\overline{X_{(i)}}-\bar{X}= \\
&= \frac{1}{n}\sum_{i=1}^{n}\frac{1}{n-1}\sum_{j=1,j\neq i}^{n}X_j-
\bar{X}=\frac{1}{n\left( n-1 \right)}\sum_{i,j=1,i\neq j}^{n}X_j-
\bar{X} \\
&= \frac{1}{n\left( n-1 \right)}\sum_{j=1}^{n}\left( n-1 \right) X_j-
\bar{X}=\frac{1}{n}\sum_{j=1}^{n}X_j-\bar{X}=0,\end{aligned}\]

así que \(\overline{\theta \left( \mathbf{X}_{(\cdot)} \right)}-\hat{\theta}\) es un estimador insesgado del sesgo de
\(\bar{X}\) (que es cero). De esta forma, utilizando un factor de
elevación arbitrario, \(c\), se tiene igualmente que
\[c\left( \overline{\theta \left( \mathbf{X}_{(\cdot)} \right)}
- \hat{\theta} \right) = 0,\]
así que es también un estimador insesgado de
\(Sesgo\left( \bar{X} \right) =0.\)\$
Determinaremos el valor de \(c\)
imponiendo que el estimador jackknife de la varianza de dicho estimador
(\(\hat{\theta}=\bar{X}\)) es una estimador insesgado de la varianza de
dicho estimador. Por una parte, es bien conocido que la varianza de
\(\hat{\theta}\) es \(Var\left( \bar{X} \right) = \sigma^2 /n\).
Por otra parte, la estimación jackknife de la varianza de \(\hat{\theta}\),
con factor de elevación \(c\) es

\[\begin{aligned}
Var_{jackk}^{\ast}\left( \bar{X} \right) &= \frac{c}{n}\sum_{i=1}^{n}
\left[ \overline{X_{(i)}}-\overline{\overline{X_{\left( \cdot
 \right)}}}\right]^2 \\
&= \frac{c}{n}\sum_{i=1}^{n}\left[ \frac{1}{n-1}\sum_{j=1,j\neq i}^{n}X_j-
\frac{1}{n}\sum_{k=1}^{n}\frac{1}{n-1}\sum_{j=1,j\neq k}^{n}X_j\right]^2
\\
&= \frac{c}{n}\sum_{i=1}^{n}\left[ \frac{1}{n-1}\sum_{j=1,j\neq i}^{n}X_j-
\frac{1}{n\left( n-1 \right)}\sum_{k,j=1,j\neq k}^{n}X_j\right]^2 \\
&= \frac{c}{n}\sum_{i=1}^{n}\left[ \frac{1}{n\left( n-1 \right)}
\sum_{j=1,j\neq i}^{n}X_j-\frac{1}{n}X_i\right]^2.\end{aligned}\]

La esperanza de esta cantidad resulta:
\[\begin{aligned}
&E\left[ \frac{c}{n}\sum_{i=1}^{n}\left( \frac{1}{n\left( n-1 \right)}
\sum_{j=1,j\neq i}^{n}X_j-\frac{1}{n}X_i \right)^2\right] \\
&= \frac{c}{n}\sum_{i=1}^{n}E\left[ \left( \frac{1}{n\left( n-1 \right)}
\sum_{j=1,j\neq i}^{n}X_j-\frac{1}{n}X_i \right)^2\right] \\
&= \frac{c}{n}\sum_{i=1}^{n}E\left[ \left( \frac{1}{n\left( n-1 \right)}
\sum_{j=1,j\neq i}^{n}\left( X_j-\mu \right) 
-\frac{1}{n}\left( X_i-\mu \right) \right)^2\right] \\
&= \frac{c}{n}\sum_{i=1}^{n}Var\left[ \frac{1}{n\left( n-1 \right)}
\sum_{j=1,j\neq i}^{n}\left( X_j-\mu \right) 
-\frac{1}{n}\left( X_i-\mu \right) \right]\\
&= \frac{c}{n}\sum_{i=1}^{n}\left( \frac{1}{n^2\left( n-1 \right)^2}
\sum_{j=1,j\neq i}^{n}\sigma^2+\frac{1}{n^2}\sigma^2 \right) \\
&= \frac{c}{n}\sum_{i=1}^{n}\left( \frac{1}{n^2\left( n-1 \right)}\sigma^2+\frac{1}{n^2}\sigma^2 \right) 
=\frac{c\sigma^2}{n\left(n-1 \right)}.
\end{aligned}\]

Así pues, el sesgo del estimador jackknife de la varianza de la media
muestral es
\[E\left[ Var_{jackk}^{\ast}\left( \bar{X} \right) \right] -\frac{\sigma
^2}{n}=\frac{c\sigma^2}{n\left( n-1 \right)}-\frac{\sigma^2}{n}=
\frac{\sigma^2}{n}\left( \frac{c}{n-1}-1 \right),\]
que vale cero si y solamente si \(c=n-1\).
Dicho en otras palabras, tomando como factor de
elevación \(c=n-1\), entonces, tanto el estimador jackknife del sesgo de
\(\bar{X}\) como el estimador jackknife de la varianza de
\(\bar{X}\) son estimadores insesgados, respectivamente, del sesgo y
la varianza de \(\bar{X}\).

Dichos estimadores resultan\[\begin{aligned}
Sesgo_{jackk}^{\ast}\left( \bar{X} \right) &= \left( n-1 \right) \left( 
\overline{\overline{X_{(\cdot)}}}-\bar{X} \right), \\
Var_{jackk}^{\ast}\left( \bar{X} \right) &= \frac{n-1}{n}\sum_{i=1}^{n}
\left[ \overline{X_{(i)}}-\overline{\overline{X_{\left( \cdot
 \right)}}}\right]^2.\end{aligned}\]

Podemos realizar un razonamiento análogo cuando el parámetro de interés
es la varianza poblacional, \(\theta =\sigma^2\). En ese caso,
considerando el estimador varianza muestral: \(\hat{\theta}=S_n^2\),
se tiene que su esperanza viene dada por

\[\begin{aligned}
E\left( S_n^2 \right) &= E\left[ \frac{1}{n}\sum_{i=1}^{n}\left( X_i-
\bar{X} \right)^2\right] =\frac{1}{n}\sum_{i=1}^{n}E\left[ \left(
X_i-\bar{X} \right)^2\right] \\
&= E\left[ \left( X_1-\frac{1}{n}\sum_{j=1}^{n}X_j \right)^2\right] =E
\left[ \left( \left( X_1-\mu \right) -\frac{1}{n}\sum_{j=1}^{n}\left(
X_j-\mu \right) \right)^2\right] \\
&= Var\left[ \left( X_1-\mu \right) -\frac{1}{n}\sum_{j=1}^{n}\left(
X_j-\mu \right) \right] \\
&= Var\left[ \frac{n-1}{n}\left( X_1-\mu \right) -\frac{1}{n}
\sum_{j=1,j\neq 1}^{n}\left( X_j-\mu \right) \right] \\
&= \left( \frac{n-1}{n} \right)^2\sigma^2+\frac{1}{n^2}
\sum_{j=2}^{n}\sigma^2 \\
&= \frac{\left( n-1 \right)}{n^2}^2\sigma^2+\frac{n-1}{n^2}\sigma
^2=\frac{n\left( n-1 \right)}{n^2}\sigma^2=\frac{n-1}{n}\sigma^2,\end{aligned}\]

así que su sesgo es
\[Sesgo\left( S_n^2 \right) =E\left( S_n^2 \right) -\sigma^2=-\frac{1
}{n}\sigma^2.\]Para un factor de elevación, \(c\), el estimador
jackknife del sesgo de este estimador
es\[Sesgo_{jackk}^{\ast}\left( S_n^2 \right) =c\left( \overline{\theta
\left( \mathbf{X}_{(\cdot)} \right)}-\hat{\theta}
 \right) =c\left( \overline{S_{n,(\cdot)}^2}-S_n^2 \right)
.\]Con lo cual la esperanza de este estimador resulta
\[E\left( c\left( \overline{S_{n,(\cdot)}^2}-S_n^2 \right)
 \right) =c\left[ E\left( \overline{S_{n,(\cdot)}^2} \right)
-E\left( S_n^2 \right) \right]\]

Estudiemos por separado cada término:
\[\begin{aligned}
\overline{S_{n,(\cdot)}^2} &= \frac{1}{n}\sum_{i=1}^{n}S_{n,
(i)}^2=\frac{1}{n}\sum_{i=1}^{n}\frac{1}{n-1}\sum_{j=1,j\neq
i}^{n}\left( X_j-\overline{X_{(i)}} \right)^2 \\
&= \frac{1}{n}\sum_{i=1}^{n}\frac{1}{n-1}\sum_{j=1,j\neq i}^{n}\left( X_j-
\frac{1}{n-1}\sum_{k=1,k\neq i}^{n}X_{k} \right)^2 \\
&= \frac{1}{n\left( n-1 \right)}\sum_{i=1}^{n}\sum_{j=1,j\neq i}^{n}\left( 
\frac{n-2}{n-1}X_j-\frac{1}{n-1}\sum_{k=1,k\neq i,k\neq j}^{n}X_{k} \right)^2,
\end{aligned}\]
con lo cual

\[\begin{aligned}
E\left( \overline{S_{n,(\cdot)}^2} \right) 
&=\frac{1}{n\left(
n-1 \right)}\sum_{i=1}^{n}\sum_{j=1,j\neq i}^{n}E\left[ \left( \frac{n-2}{n-1
}X_j-\frac{1}{n-1}\sum_{k=1,k\neq i,k\neq j}^{n}X_{k} \right)^2\right] \\
&=\frac{1}{n\left( n-1 \right)}\sum_{i=1}^{n}\sum_{j=1,j\neq i}^{n}E\left[
\left( \frac{n-2}{n-1}\left( X_j-\mu \right) -\frac{1}{n-1}\sum_{k=1,k\neq
i,k\neq j}^{n}\left( X_{k}-\mu \right) \right)^2\right] \\
&=\frac{1}{n\left( n-1 \right)}\sum_{i=1}^{n}\sum_{j=1,j\neq i}^{n}Var\left[ 
\frac{n-2}{n-1}\left( X_j-\mu \right) -\frac{1}{n-1}\sum_{k=1,k\neq
i,k\neq j}^{n}\left( X_{k}-\mu \right) \right] \\
&=\frac{1}{n\left( n-1 \right)}\sum_{i=1}^{n}\sum_{j=1,j\neq i}^{n}\left[ 
\frac{\left( n-2 \right)^2}{\left( n-1 \right)^2}\sigma^2+\frac{1}{
\left( n-1 \right)^2}\left( n-2 \right) \sigma^2\right] \\
&=\frac{1}{n\left( n-1 \right)}\sum_{i=1}^{n}\sum_{j=1,j\neq i}^{n}\left[ 
\frac{\left( n-2 \right) \left( n-1 \right)}{\left( n-1 \right)^2}\sigma
^2\right] =\frac{n-2}{n-1}\sigma^2
\end{aligned}\]

Además, ya hemos visto anteriormente que \(E\left( S_n^2 \right) = (n-1)\sigma^2/n\), con o cual la esperanza del estimador
jackknife del sesgo es
\[\begin{aligned}
c\left( \frac{n-2}{n-1}\sigma^2-\frac{n-1}{n}\sigma^2 \right)
&= c\sigma^2\left( \frac{n-2}{n-1}-\frac{n-1}{n} \right) \\
&= c\sigma^2\frac{n\left( n-2 \right) -\left( n-1 \right)^2}{n\left( n-1 \right)} \\
&= c\sigma^2\frac{-1}{n\left( n-1 \right)}
=-\frac{c\sigma^2}{n\left(n-1 \right)}.
\end{aligned}\]

De esta forma, el sesgo del estimador jackknife del sesgo de \(S_n^2\)
resulta ser
\[\begin{aligned}
E\left[ Sesgo_{jackk}^{\ast}\left( S_n^2 \right) \right] 
-Sesgo\left(S_n^2 \right) &= -\frac{c\sigma^2}{n\left( n-1 \right)}
-\left( -\frac{1}{n}\sigma^2 \right) \\
&= -\frac{\sigma^2}{n}\left( \frac{c}{n-1}-1 \right),
\end{aligned}\]

con lo cual este sesgo será cero si y sólo si \(c=n-1\).

Esto da pie al estimador jackknife del sesgo de la varianza muestral:
\[Sesgo_{jackk}^{\ast}\left( S_n^2 \right) =\left( n-1 \right) \left( 
\overline{S_{n,(\cdot)}^2}-S_n^2 \right).\]

Así pues, queda justificado, en el caso de estimación de los parámetros
media y varianza, la razón de la elección del factor de elevación
\(c=n-1\).

\hypertarget{relaciuxf3n-bootstrapjackknife-en-dicha-estimaciuxf3n}{%
\section{Relación Bootstrap/Jackknife en dicha estimación}\label{relaciuxf3n-bootstrapjackknife-en-dicha-estimaciuxf3n}}

Consideremos un parámetro de interés \(\theta \left( F \right)\) y su
correspondiente estimador que supondremos funcional, \(\theta \left( F_n \right)\). En realidad, cuando calculamos cantidades como \(\theta \left( \mathbf{X}_{(i)} \right)\), lo que estamos
haciendo es evaluar el funcional \(\theta\) en otra función de
distribución\[F_{n,(i)}\left( x \right) =\frac{1}{n-1}\sum_{j=1,j\neq i}^{n}
\mathbf{1}\left( X_j\leq x \right).\]Dicho en terminología de vectores
de remuestreo, el estimador habitual consiste en evaluar \(\theta\) en el
vector \(\mathbf{p}=\left( \frac{1}{n},\ldots ,\frac{1}{n} \right)\),
\(\theta \left( \mathbf{p} \right)\), mientras que el estimador
construido con toda la muestra excepto el dato \(i\)-ésimo es la evaluación
\(\theta \left( \mathbf{p}_{(i)} \right)\), siendo
\[\mathbf{p}_{(i)}=\left( \frac{1}{n-1},\ldots ,\frac{1}{n-1},
\underset{(i)}{0},\frac{1}{n-1},\ldots ,\frac{1}{n-1} \right).\]

En lo que sigue, consideraremos funcionales \(\theta\) que definen
estimadores lineales o cuadráticos en los vectores de remuestreo:

\begin{itemize}
\item
  Estimadores lineales:
  \[\theta \left( \mathbf{p} \right) = a+\mathbf{b}^{T} \mathbf{p}\]
\item
  Estimadores cuadráticos:
  \[\theta \left( \mathbf{p} \right) = a+\mathbf{b}^{T}
  \mathbf{p}+\mathbf{p}^{T}C\mathbf{p}\]
\end{itemize}

Una forma alternativa de definir estos estimadores es

\begin{itemize}
\item
  Estimadores lineales:
  \[\theta \left( \mathbf{p} \right) = \mathbf{b}^{T}
  \left( \mathbf{p}-\mathbf{p}_{0} \right) \]
\item
  Estimadores cuadráticos:
  \[\theta \left( \mathbf{p} \right) = \left( \mathbf{p}-\mathbf{p}_{0} \right)^{T}
  C\left( \mathbf{p}-\mathbf{p}_{0} \right)\]
\end{itemize}

Por ejemplo, puede demostrarse fácilmente que la media, \(\bar{X}\),
es un estimador lineal en el vector de remuestreo y que la varianza
muestral, \(S_n^2\), es un estimador cuadrático en el vector de
remuestreo.

Existen dos resultados que relacionan el sesgo bootstrap y el sesgo
jackknife de cualquier estimador cuadrático y la varianza bootstrap y la
varianza jackknife de cualquier estimador lineal.

\begin{theorem}
\protect\hypertarget{thm:jack-boot-sesgo}{}{\label{thm:jack-boot-sesgo} } \vspace{0.5cm}

Si \(\hat{\theta}\) es un estimador cuadrático, entonces
\[Sesgo_{jackk}\left( \hat{\theta} \right) =\frac{n}{n-1}Sesgo_{boot}\left( 
\hat{\theta} \right)\]
\end{theorem}

\begin{theorem}
\protect\hypertarget{thm:jack-boot-precision}{}{\label{thm:jack-boot-precision} } \vspace{0.5cm}

Si \(\hat{\theta}\) es un estimador lineal, entonces
\[Var_{jackk}\left( \hat{\theta} \right) =\frac{n}{n-1}Var_{boot}\left( \hat{
\theta} \right)\]
\end{theorem}

Dicho en otras palabras, para cualquier estimador cuadrático, el sesgo
jackknife es mayor que el sesgo bootstrap. Si el estimador es lineal, la
varianza jackknife es mayor que la varianza bootstrap. En ambos casos,
el factor multiplicador es \(n/(n-1)\).

\begin{example}[Aproximación jackknife de la precisión de estimaciones del tiempo de vida medio de microorganismos]
\protect\hypertarget{exm:estimacion-jack-precision}{}{\label{exm:estimacion-jack-precision} \iffalse (Aproximación jackknife de la precisión de estimaciones del tiempo de vida medio de microorganismos) \fi{} } \vspace{0.5cm}

Consideremos la muestra de tiempos de vida de microorganismos ya
tratada. El siguiente código permite calcular los
estimadores jackknife del sesgo y de la precisión tanto de la media como
de la mediana muestral.
\end{example}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Para la muestra de TIEMPOS DE VIDA, estima (plug{-}in) la precisión }
\CommentTok{\# de la media muestral (desvmedia) y también estima mediante el jackknife }
\CommentTok{\# dicha precisión (desvmediajackk) y también la precisión de la mediana }
\CommentTok{\# muestral, de la cual no se conoce su expresión, (desvmedianajackk). }
\CommentTok{\# También estima el sesgo jackknife de esos dos estimadores}
\CommentTok{\# (sesgomediajackk y sesgomedianajackk, respectivamente).}

\NormalTok{muestra }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{0.143}\NormalTok{, }\FloatTok{0.182}\NormalTok{, }\FloatTok{0.256}\NormalTok{, }\FloatTok{0.26}\NormalTok{, }\FloatTok{0.27}\NormalTok{, }\FloatTok{0.437}\NormalTok{, }\FloatTok{0.509}\NormalTok{, }
    \FloatTok{0.611}\NormalTok{, }\FloatTok{0.712}\NormalTok{, }\FloatTok{1.04}\NormalTok{, }\FloatTok{1.09}\NormalTok{, }\FloatTok{1.15}\NormalTok{, }\FloatTok{1.46}\NormalTok{, }\FloatTok{1.88}\NormalTok{, }\FloatTok{2.08}\NormalTok{)}
\NormalTok{n }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(muestra)}
\NormalTok{varmedia }\OtherTok{\textless{}{-}}\NormalTok{ (}\DecValTok{1}\SpecialCharTok{/}\NormalTok{(n}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)) }\SpecialCharTok{*} \FunctionTok{sum}\NormalTok{((muestra }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(muestra))}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{desvmedia }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(varmedia)}

\CommentTok{\# Jackknife}
\NormalTok{media }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(n)}
\NormalTok{mediana }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(n)}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n) \{}
\NormalTok{    imuestra }\OtherTok{\textless{}{-}}\NormalTok{ muestra[}\SpecialCharTok{{-}}\NormalTok{i]}
\NormalTok{    media[i] }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(imuestra)}
    \CommentTok{\# remordenada \textless{}{-} sort(imuestra)}
    \CommentTok{\# mediana[i] \textless{}{-} (remordenada[7] + remordenada[8])/2}
\NormalTok{    mediana[i] }\OtherTok{\textless{}{-}} \FunctionTok{median}\NormalTok{(imuestra)}
\NormalTok{\}}

\CommentTok{\# Aproximaciones precisión}
\NormalTok{varmediajackk }\OtherTok{\textless{}{-}}\NormalTok{ ((n }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)}\SpecialCharTok{/}\NormalTok{n) }\SpecialCharTok{*} \FunctionTok{sum}\NormalTok{((media }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(media))}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{desvmediajackk }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(varmediajackk)}
\NormalTok{varmedianajackk }\OtherTok{\textless{}{-}}\NormalTok{ ((n }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)}\SpecialCharTok{/}\NormalTok{n) }\SpecialCharTok{*} \FunctionTok{sum}\NormalTok{((mediana }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(mediana))}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{desvmedianajackk }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(varmedianajackk)}
\NormalTok{desvmedia}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1555792
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{desvmediajackk}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1610397
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{desvmedianajackk}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1834505
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Aproximaciones sesgo}
\NormalTok{sesgomediajackk }\OtherTok{\textless{}{-}}\NormalTok{ (n }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{) }\SpecialCharTok{*}\NormalTok{ (}\FunctionTok{mean}\NormalTok{(media) }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(muestra))}
\CommentTok{\# sesgomedianajackk \textless{}{-} (n {-} 1) * (mean(mediana) {-} muestra[8])}
\NormalTok{sesgomedianajackk }\OtherTok{\textless{}{-}}\NormalTok{ (n }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{) }\SpecialCharTok{*}\NormalTok{ (}\FunctionTok{mean}\NormalTok{(mediana) }\SpecialCharTok{{-}} \FunctionTok{median}\NormalTok{(muestra))}
\NormalTok{sesgomediajackk}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sesgomedianajackk}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -0.003733333
\end{verbatim}

También podríamos emplear la la función \texttt{jackknife} del paquete \texttt{bootstrap}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(bootstrap)}
\NormalTok{resmedia }\OtherTok{\textless{}{-}} \FunctionTok{jackknife}\NormalTok{(muestra, mean)}
\NormalTok{resmedia}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $jack.se
## [1] 0.1610397
## 
## $jack.bias
## [1] 0
## 
## $jack.values
##  [1] 0.8526429 0.8498571 0.8445714 0.8442857 0.8435714 0.8316429 0.8265000
##  [8] 0.8192143 0.8120000 0.7885714 0.7850000 0.7807143 0.7585714 0.7285714
## [15] 0.7142857
## 
## $call
## jackknife(x = muestra, theta = mean)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{resmediana }\OtherTok{\textless{}{-}} \FunctionTok{jackknife}\NormalTok{(muestra, median)}
\NormalTok{resmediana}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $jack.se
## [1] 0.1834505
## 
## $jack.bias
## [1] -0.003733333
## 
## $jack.values
##  [1] 0.6615 0.6615 0.6615 0.6615 0.6615 0.6615 0.6615 0.6105 0.5600 0.5600
## [11] 0.5600 0.5600 0.5600 0.5600 0.5600
## 
## $call
## jackknife(x = muestra, theta = median)
\end{verbatim}

Estos resultados pueden compararse con los obtenidos en el Ejemplo \ref{exm:estimacion-boot-precision} empleando bootstrap.
En general las aproximaciones jackknife son adecuadas para el caso de estadísticos
``suaves'', como la media, pero pueden ser inconsistentes cuando no lo son,
como es el caso de la mediana.

\hypertarget{modunif}{%
\chapter{Modificaciones del Bootstrap uniforme}\label{modunif}}

El bootstrap uniforme (o naïve) es aquel en el que remuestreamos a
partir de la función de distribución empírica. Eso es muy razonable
cuando no tenemos ninguna información adicional sobre la función de
distribución poblacional, ya que la distribución empírica es el
estimador máximo verosímil no paramétrico de la función de distribución
poblacional. Sin embargo, cuando en el contexto en el que nos
encontremos conozcamos alguna propiedad adicional de dicha distribución
poblacional, entonces debemos incorporarla en el método de remuestreo,
dando lugar a otro método bootstrap que ya no debemos llamar uniforme o
naïve. Veremos algunos de ellos.

\hypertarget{modunif-boot-par}{%
\section{Bootstrap paramétrico}\label{modunif-boot-par}}

Supongamos que sabemos que la función de distribución poblacional
pertenece a cierta familia paramétrica. Es decir \(F=F_{\theta }\) para
algún vector \(d\)-dimensional \(\theta \in \Theta\). En ese caso parece
lógico estimar \(\theta\) a partir de la muestra (denotemos
\(\hat{\theta}\) un estimador de \(\theta\), por ejemplo el de máxima
verosimilitud) y obtener remuestras de \(F_{\hat{\theta}}\) no de \(F_n\).
Entonces, el bootstrap uniforme se modifica de la siguiente forma, dando
lugar al llamado bootstrap paramétrico:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Dada la muestra
  \(\mathbf{X}=\left( X_1,\ldots ,X_n \right)\), calcular
  \(\hat{\theta}\)
\item
  Para cada \(i=1,\ldots ,n\) arrojar \(X_i^{\ast}\) a partir de
  \(F_{\hat{\theta}}\)
\item
  Obtener \(\mathbf{X}^{\ast}=\left( X_1^{\ast},\ldots ,X_n^{\ast} \right)\)
\item
  Calcular \(R^{\ast}=R\left( \mathbf{X}^{\ast},F_{\hat{\theta}} \right)\)
\end{enumerate}

Así utilizaremos las distribución en el remuestreo de \(R^{\ast}\) para
aproximar la distribución en el muestreo de \(R\). Lógicamente, cuando no
sea posible obtener una expresión explícita para la distribución
bootstrap de \(R^{\ast}\) utilizaremos una aproximación de Monte Carlo de
la misma:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Dada la muestra
  \(\mathbf{X}=\left( X_1,\ldots ,X_n \right)\), calcular
  \(\hat{\theta}\)
\item
  Para cada \(i=1,\ldots ,n\) arrojar \(X_i^{\ast}\) a partir de
  \(F_{\hat{\theta}}\)
\item
  Obtener \(\mathbf{X}^{\ast}=\left( X_1^{\ast},\ldots ,X_n^{\ast} \right)\)
\item
  Calcular
  \(R^{\ast}=R\left( \mathbf{X}^{\ast},F_{\hat{\theta} } \right)\)
\item
  Repetir \(B\) veces los pasos 2-4 para obtener las réplicas bootstrap
  \(R^{\ast (1)}\), \(\ldots\), \(R^{\ast (B)}\)
\item
  Utilizar esas réplicas bootstrap para aproximar la distribución en el
  muestreo de \(R\)
\end{enumerate}

En general, para llevar a cabo el paso 2, debemos poder simular valores
de la distribución \(F_{\hat{\theta}}\) (en el caso del bootstrap uniforme
se trataba de simular valores de la distribución empírica, lo cual es
muy sencillo y rápido). Para ello podemos utilizar el método de
inversión, que consiste en simular un valor \(U\) procedente de una
distribución \(\mathcal{U}\left( 0,1 \right)\) (es decir, \(U\) es un número aleatorio
uniforme) y devolver \(X^{\ast}=F_{\hat{\theta}}^{-1}\left( U \right)\). Así, podríamos escribir el paso 2 de una forma más
detallada:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Para cada \(i=1,\ldots ,n\) arrojar \(U_i\sim \mathcal{U}\left( 0,1 \right)\) y
  hacer \(X_i^{\ast}=F_{\hat{\theta}}^{-1}\left( U_i \right)\)
\end{enumerate}

No en todos los modelos paramétricos es fácil de calcular la inversa
\(F_{\hat{\theta}}^{-1}\). En algunos modelos paramétricos (como el caso
de la distribución normal) ni siquiera tenemos una fórmula explícita
para \(F_{\theta }\left( x \right)\), con lo cual difícilmente podremos
calcular explícitamente su inversa. En casos como esos es frecuente
recurrir a otros métodos para simular la distribución en cuestión.
Normalmente existen rutinas incorporadas a la mayoría de los lenguajes
de programación y software estadístico (como \texttt{R}) que permiten simular
directamente la mayoría de las distribuciones paramétricas habituales.

\begin{example}[Inferencia sobre la media con varianza conocida, continuación]
\protect\hypertarget{exm:media-dt-conocida-par}{}{\label{exm:media-dt-conocida-par} \iffalse (Inferencia sobre la media con varianza conocida, continuación) \fi{} }
\end{example}
Continuando con el ejemplo de tiempo de vida de microorganismos,
podemos modificar fácilmente el código mostrado en el Ejemplo \ref{exm:media-dt-conocida}, de forma que se emplee bootstrap
paramétrico (normal), con desviación típica conocida, para
calcular un intervalo de confianza para la media poblacional.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{muestra }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{0.143}\NormalTok{, }\FloatTok{0.182}\NormalTok{, }\FloatTok{0.256}\NormalTok{, }\FloatTok{0.26}\NormalTok{, }\FloatTok{0.27}\NormalTok{, }\FloatTok{0.437}\NormalTok{, }\FloatTok{0.509}\NormalTok{, }
             \FloatTok{0.611}\NormalTok{, }\FloatTok{0.712}\NormalTok{, }\FloatTok{1.04}\NormalTok{, }\FloatTok{1.09}\NormalTok{, }\FloatTok{1.15}\NormalTok{, }\FloatTok{1.46}\NormalTok{, }\FloatTok{1.88}\NormalTok{, }\FloatTok{2.08}\NormalTok{)}
\NormalTok{n }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(muestra)}
\NormalTok{sigma }\OtherTok{\textless{}{-}} \FloatTok{0.6}

\NormalTok{alfa }\OtherTok{\textless{}{-}} \FloatTok{0.05}
\NormalTok{x\_barra }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(muestra)}

\CommentTok{\# Remuestreo}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{B }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{estadistico\_boot }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(B)}
\ControlFlowTok{for}\NormalTok{ (k }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{B) \{}
    \CommentTok{\# u \textless{}{-} rnorm(n)}
    \CommentTok{\# remuestra \textless{}{-} u * sigma + x\_barra}
\NormalTok{    remuestra }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n, x\_barra, sigma)}
\NormalTok{    x\_barra\_boot }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(remuestra)}
\NormalTok{    estadistico\_boot[k] }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(n) }\SpecialCharTok{*}\NormalTok{ (x\_barra\_boot }\SpecialCharTok{{-}}\NormalTok{ x\_barra)}\SpecialCharTok{/}\NormalTok{sigma}
\NormalTok{\}}

\CommentTok{\# Aproximación Monte Carlo de los ptos críticos}
\CommentTok{\# Empleando la distribución empírica del estadístico bootstrap: }
    \CommentTok{\# estadistico\_boot\_ordenado \textless{}{-} sort(estadistico\_boot)}
    \CommentTok{\# indice\_inf \textless{}{-} floor(B * alfa/2)}
    \CommentTok{\# indice\_sup \textless{}{-} floor(B * (1 {-} alfa/2))}
    \CommentTok{\# pto\_crit \textless{}{-} estadistico\_boot\_ordenado[c(indice\_inf, indice\_sup)]}
\CommentTok{\# Empleando la función \textasciigrave{}quantile\textasciigrave{}:}
\NormalTok{pto\_crit }\OtherTok{\textless{}{-}} \FunctionTok{quantile}\NormalTok{(estadistico\_boot, }\FunctionTok{c}\NormalTok{(alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{))}

\CommentTok{\# Construcción del IC}
\NormalTok{ic\_inf\_boot }\OtherTok{\textless{}{-}}\NormalTok{ x\_barra }\SpecialCharTok{{-}}\NormalTok{ pto\_crit[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ sigma}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(n)}
\NormalTok{ic\_sup\_boot }\OtherTok{\textless{}{-}}\NormalTok{ x\_barra }\SpecialCharTok{{-}}\NormalTok{ pto\_crit[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ sigma}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(n)}
\NormalTok{IC\_boot }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(ic\_inf\_boot, ic\_sup\_boot)}
\FunctionTok{names}\NormalTok{(IC\_boot) }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\DecValTok{100}\SpecialCharTok{*}\FunctionTok{c}\NormalTok{(alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{), }\StringTok{"\%"}\NormalTok{)}
\NormalTok{IC\_boot}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      2.5%     97.5% 
## 0.5236922 1.1217871
\end{verbatim}

En este caso concreto la distribución bootstrap del estadístico sería conocida (normal estándar) y realmente no sería necesario emplear la aproximación Monte Carlo:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(estadistico\_boot, }\AttributeTok{freq =} \ConstantTok{FALSE}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{v =}\NormalTok{ pto\_crit)}
\FunctionTok{curve}\NormalTok{(dnorm, }\AttributeTok{add=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{)}
\NormalTok{pto\_crit\_teor }\OtherTok{\textless{}{-}} \FunctionTok{qnorm}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{v =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{pto\_crit\_teor, pto\_crit\_teor), }\AttributeTok{lty =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{03-mod_boot_unif_files/figure-latex/unnamed-chunk-2-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ic\_inf\_boot\_teor }\OtherTok{\textless{}{-}}\NormalTok{ x\_barra }\SpecialCharTok{{-}}\NormalTok{ pto\_crit\_teor }\SpecialCharTok{*}\NormalTok{ sigma}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(n)}
\NormalTok{ic\_sup\_boot\_teor }\OtherTok{\textless{}{-}}\NormalTok{ x\_barra }\SpecialCharTok{+}\NormalTok{ pto\_crit\_teor }\SpecialCharTok{*}\NormalTok{ sigma}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(n)}
\NormalTok{IC\_boot\_teor }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(ic\_inf\_boot\_teor, ic\_sup\_boot\_teor)}
\FunctionTok{names}\NormalTok{(IC\_boot\_teor) }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\DecValTok{100}\SpecialCharTok{*}\FunctionTok{c}\NormalTok{(alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{), }\StringTok{"\%"}\NormalTok{)}
\NormalTok{IC\_boot\_teor}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     2.5%    97.5% 
## 0.501697 1.108970
\end{verbatim}

Para emplear el paquete \texttt{boot}, como se comentó en la Sección
\ref{intro-pkgboot}, habría que establecer en la llamada a la
función \texttt{boot()} los argumentos: \texttt{sim\ =\ "parametric"},
\texttt{mle} igual a los parámetros necesarios para la simulación y
\texttt{ran.gen\ =\ function(data,\ mle)}, una función de los datos originales
y de los parámetros que devuelve los datos generados.
En este caso además, la función \texttt{statistic} no necesita el vector
de índices como segundo parámetro.
Por ejemplo, para calcular el intervalo de confianza para la media del
tiempo de vida de los microorganismos, podríamos utilizar el siguiente código:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(boot)}
\NormalTok{ran.gen.norm }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, mle) \{}
    \CommentTok{\# Función para generar muestras aleatorias normales}
    \CommentTok{\# con desviación típica sigma = 0.6,}
    \CommentTok{\# mle contendrá la media de los datos originales}
\NormalTok{    out }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{length}\NormalTok{(data), mle, sigma)}
\NormalTok{    out}
\NormalTok{\}}

\NormalTok{statistic }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data)\{}
    \FunctionTok{c}\NormalTok{(}\FunctionTok{mean}\NormalTok{(data), sigma}\SpecialCharTok{\^{}}\DecValTok{2}\SpecialCharTok{/}\FunctionTok{length}\NormalTok{(data))}
\NormalTok{\}}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{res.boot }\OtherTok{\textless{}{-}} \FunctionTok{boot}\NormalTok{(muestra, statistic, }\AttributeTok{R =}\NormalTok{ B, }\AttributeTok{sim =} \StringTok{"parametric"}\NormalTok{,}
                 \AttributeTok{ran.gen =}\NormalTok{ ran.gen.norm, }\AttributeTok{mle =} \FunctionTok{mean}\NormalTok{(muestra))}

\FunctionTok{boot.ci}\NormalTok{(res.boot, }\AttributeTok{type =} \StringTok{"stud"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
## Based on 1000 bootstrap replicates
## 
## CALL : 
## boot.ci(boot.out = res.boot, type = "stud")
## 
## Intervals : 
## Level    Studentized     
## 95%   ( 0.5208,  1.1232 )  
## Calculations and Intervals on Original Scale
\end{verbatim}

Aunque los resultados dependerán en gran medida de que el modelo paramétrico
sea adecuado para describir la variabilidad de los datos
(en este caso no es muy razonable que el modelo admita tiempos de vida negativos).
Si, por ejemplo, consideramos que un modelo exponencial es más adecuado: {[}Figura \ref{fig:boot-par-aprox}{]}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Distribución bootstrap uniforme}
\FunctionTok{curve}\NormalTok{(}\FunctionTok{ecdf}\NormalTok{(muestra)(x), }\AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{.}\DecValTok{5}\NormalTok{, }\DecValTok{3}\NormalTok{), }\AttributeTok{ylab =} \StringTok{"F(x)"}\NormalTok{, }\AttributeTok{type =} \StringTok{"s"}\NormalTok{)}
\CommentTok{\# Distribución bootstrap paramétrico normal}
\FunctionTok{curve}\NormalTok{(}\FunctionTok{pnorm}\NormalTok{(x, }\FunctionTok{mean}\NormalTok{(muestra), }\FloatTok{0.6}\NormalTok{), }\AttributeTok{lty =} \DecValTok{2}\NormalTok{, }\AttributeTok{add =} \ConstantTok{TRUE}\NormalTok{)}
\CommentTok{\# Distribución bootstrap paramétrico exponencial}
\FunctionTok{curve}\NormalTok{(}\FunctionTok{pexp}\NormalTok{(x, }\DecValTok{1}\SpecialCharTok{/}\FunctionTok{mean}\NormalTok{(muestra)), }\AttributeTok{lty =} \DecValTok{3}\NormalTok{, }\AttributeTok{add =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}\StringTok{"bottomright"}\NormalTok{, }\AttributeTok{legend =} \FunctionTok{c}\NormalTok{(}\StringTok{"Empírica"}\NormalTok{, }\StringTok{"Aprox. normal"}\NormalTok{, }\StringTok{"Aprox. exponencial"}\NormalTok{), }\AttributeTok{lty =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[!htb]

{\centering \includegraphics[width=0.7\linewidth]{03-mod_boot_unif_files/figure-latex/boot-par-aprox-1} 

}

\caption{Distribución empírica de la muestra de tiempos de vida de microorganismos y aproximaciones paramétricas.}\label{fig:boot-par-aprox}
\end{figure}

Solo tendríamos que cambiar la función que genera los datos:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ran.gen.exp }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, mle) \{}
    \CommentTok{\# Función para generar muestras aleatorias exponenciales}
    \CommentTok{\# mle contendrá la media de los datos originales}
\NormalTok{    out }\OtherTok{\textless{}{-}} \FunctionTok{rexp}\NormalTok{(}\FunctionTok{length}\NormalTok{(data), }\DecValTok{1}\SpecialCharTok{/}\NormalTok{mle)}
\NormalTok{    out}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Una de las principales aplicaciones del bootstrap paramétrico
es el contraste de hipótesis que se tratará en la Sección \ref{contrastes-parametricos}.

\hypertarget{bootstrap-simetrizado}{%
\section{Bootstrap simetrizado}\label{bootstrap-simetrizado}}

Supongamos que conocemos que la función de distribución poblacional es
simétrica entorno a cierto valor. Eso significa que existe un valor \(c\)
tal que \(F\left( c-h \right) =1-F\left( c+h \right)\) para todo \(h>0\).
Equivalentemente, una variable aleatoria es simétrica entorno a \(c\) si
su función de distribución verifica
\[F\left( x \right) = 1 - F\left( 2c - x \right)\]
para todo \(x\in \mathbb{R}\). Puede demostrarse que dicho
centro de simetría, \(c\), ha de ser la media de la distribución,
\(\mu\), en caso de que exista. Esa información (la simetría) sobre la
distribución poblacional también se debe incorporarse en el bootstrap.
Así, para estimar la función de distribución poblacional, \(F\), supuesto
que es simétrica entorno a \(\mu\), es razonable utilizar una versión
simetrizada de la distribución empírica, \(F_n^{sim}\). Ese estimador
empírico simetrizado de la función de distribución es el que otorga
igual masa de probabilidad a una muestra artificialmente construida
simetrizando, alrededor de la media muestral, la muestra original:

\[Y_i=\left\{ 
\begin{array}{ll}
X_i & \text{si } i=1,\ldots ,n \\ 
2\bar{X}-X_{i-n} &\text{si } i=n+1,\ldots ,2n
\end{array}
\right.\]

con lo cual
\[F_n^{sim}\left( x \right) =\frac{1}{2n}\sum_{i=1}^{2n}\mathbf{1}\left( Y_i\leq x \right).\]
Puede demostrarse fácilmente que
\[F_n^{sim}\left( x \right) =\frac{1}{2}\left( F_n\left( x \right)
+1-F_n\left( 2\bar{X}-x \right) \right).\]

Al diseñar el plan de remuestreo debemos utilizar \(F_n^{sim}\)
(bootstrap simetrizado), en lugar de \(F_n\) (bootstrap uniforme).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Para cada \(i=1,\ldots ,n\) arrojar \(X_i^{\ast}\) a partir de
  \(F_n^{sim}\), es decir
  \(P^{\ast}\left( X_i^{\ast}=Y_j \right) =\frac{1 }{2n}\), \(j=1,\ldots ,2n\)
\item
  Obtener \(\mathbf{X}^{\ast}=\left( X_1^{\ast},\ldots ,X_n^{\ast} \right)\)
\item
  Calcular
  \(R^{\ast}=R\left( \mathbf{X}^{\ast},F_n^{sim} \right)\)
\end{enumerate}

Como veremos más adelante, a veces (muy poco frecuentemente) es posible
calcular exactamente la distribución bootstrap de \(R^{\ast}\). Cuando
eso no es posible, esa distribución es fácilmente aproximable por Monte
Carlo, arrojando una gran cantidad, \(B\), de réplicas de \(R^{\ast}\). En
ese caso, el algoritmo se convierte en:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Para cada \(i=1,\ldots ,n\) arrojar \(X_i^{\ast}\) a partir de
  \(F_n^{sim}\)
\item
  Obtener \(\mathbf{X}^{\ast}=\left( X_1^{\ast},\ldots ,X_n^{\ast} \right)\)
\item
  Calcular
  \(R^{\ast}=R\left( \mathbf{X}^{\ast},F_n^{sim} \right)\)
\item
  Repetir \(B\) veces los pasos 1-3 para obtener las réplicas bootstrap
  \(R^{\ast (1)}\), \(\ldots\), \(R^{\ast (B)}\)
\item
  Utilizar esas réplicas bootstrap para aproximar la distribución en el
  muestreo de \(R\)
\end{enumerate}

Para llevar a cabo el paso 1 podemos proceder de dos formas
equivalentes. La primera consiste en definir explícitamente la muestra
simetrizada en torno a la media, \(\mathbf{Y}\), y luego obtener
uno de los valores de dicha muestra con equiprobabilidad. El paso 1
quedaría de la siguiente forma:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Para cada \(i=1,\ldots ,n\) arrojar \(U_i\sim \mathcal{U}\left( 0,1 \right)\) y
  hacer \(X_i^{\ast}=Y_{\left\lfloor 2nU_i\right\rfloor +1}\)
\end{enumerate}

Alternativamente podemos proceder con el paso 1 utilizando el hecho de que la función de distribución \(F_n^{sim}\left( x \right)\) resultar ser la distribución de una variable aleatoria obtenida en dos etapas:
en la primera etapa se genera un valor según la empírica, \(F_n\left( x \right)\), y en la segunda se decide (con equiprobabilidad) si el valor obtenido no se altera o bien si se refleja alrededor de la media muestral, \(\bar{X}\)
(equivalentemente, la distribución simetrizada es una mixtura de la distribución empírica \(F_n\left( x \right)\) y de su versión ``reflejada'' \(1-F_n\left( 2\bar{X}-x \right)\) y se puede simular mediante el método de composición; ver p.e. Fernández-Casal y Cao, 2020, \href{https://rubenfcasal.github.io/simbook/m\%C3\%A9todo-de-composici\%C3\%B3n.html}{Sección 5.4}). Así el paso 1 resulta:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Para cada \(i=1,\ldots ,n\) arrojar
  \(U_i,V_i\sim \mathcal{U}\left( 0,1 \right)\). Si \(V_i\leq \frac{1}{2}\)
  entonces hacer \(X_i^{\ast}=X_{\left\lfloor nU_i\right\rfloor +1}\) y en caso contrario hacer
  \(X_i^{\ast}=2\overline{X }-X_{\left\lfloor nU_i\right\rfloor +1}\)
\end{enumerate}

La utilización de \(F_n^{sim}\left( x \right)\) en lugar de \(F_n\left( x \right)\) altera las propiedades conocidas de la distribución
(empírica) de la que se remuestrea en el bootstrap uniforme. Así, en
primer lugar, \(F_n^{sim}\left( x \right)\) es simétrica (como se desea)
con lo cual todos los momentos impares de esta distribución con respecto
a \(\bar{X}\) son cero.
En particular la media de \(F_n^{sim}\left(x \right)\) es
\[\begin{aligned}
\int x~dF_n^{sim}\left( x \right) &= \frac{1}{2n}\sum_{i=1}^{2n}Y_i=\frac{
1}{2n}\left[ \sum_{i=1}^{n}X_i+\sum_{i=1}^{n}\left( 2\bar{X}
-X_i \right) \right] \\
&= \frac{1}{2n}\left( n\bar{X}+2n\bar{X}-n\bar{X} \right) =
\bar{X}.\end{aligned}\]
También se conservan los momentos centrales de orden par:

\[\begin{aligned}
\int \left( x-\bar{X} \right)^{2k}~dF_n^{sim}\left( x \right) &= \frac{
1}{2n}\sum_{i=1}^{2n}\left( Y_i-\bar{X} \right)^{2k} \\
&= \frac{1}{2n}\left[ \sum_{i=1}^{n}\left( X_i-\bar{X} \right)
^{2k}+\sum_{i=1}^{n}\left[ \left( 2\bar{X}-X_i \right) -\bar{X}
\right]^{2k}\right] \\
&= \frac{1}{2n}\left[ \sum_{i=1}^{n}\left( X_i-\bar{X} \right)
^{2k}+\sum_{i=1}^{n}\left( \bar{X}-X_i \right)^{2k}\right] \\
&= \frac{1}{n}\sum_{i=1}^{n}\left( X_i-\bar{X} \right)^{2k}.
\end{aligned}\]

En particular, la varianza de \(F_n^{sim}\left( x \right)\) coincide con
la de \(F_n\left( x \right)\), que es \(S_n^2\).

En general, cuando la distribución de partida es simétrica, es más
adecuado utilizar el bootstrap simetrizado que el bootstrap uniforme.
Aún así, cuando se realiza inferencia sobre algún estadístico (como
\(\sqrt{n}(\bar{X}-\mu)/\sigma\)) cuya distribución
asintótica ya es simétrica (como la normal), la aproximación bootstrap
uniforme para distribuciones de partida simétricas, ya es especialmente
buena y, por tanto, la ganancia del bootstrap simétrizado aporta una
mejora difícil de detectar en la práctica. Ese no es el caso de otros
estadísticos (como los asociados a inferencia sobre la varianza) con
distribución más alejada de la simetría.

\begin{exercise}[Inferencia sobre la media con varianza conocida empleando bootstrap simetrizado]
\protect\hypertarget{exr:media-dt-conocida-sim}{}{\label{exr:media-dt-conocida-sim} \iffalse (Inferencia sobre la media con varianza conocida empleando bootstrap simetrizado) \fi{} }
Modificar adecuadamente el código del Ejemplo \ref{exm:media-dt-conocida}, para
implementar un método bootstrap simetrizado, con el objeto de calcular
un intervalo de confianza para la media con desviación típica conocida.
Qué diferencias se observan entre los intervalos obtenidos por el
bootstrap uniforme y por el simetrizado?
\end{exercise}

\hypertarget{modunif-boot-suav}{%
\section{Bootstrap suavizado}\label{modunif-boot-suav}}

Cuando la distribución poblacional, \(F\), es continua es lógico
incorporar dicha información al bootstrap. Eso significa que la función
de distribución tiene una función de densidad asociada, relacionadas
mediante la expresión: \(f\left( x \right) =F^{\prime}\left( x \right)\). Para ello, debemos utilizar un método bootstrap que
remuestree de un universo bootstrap continuo. En otras palabras debemos
utilizar un estimador de la función de densidad y remuestrear de él.

Pasamos a considerar brevemente el problema de estimar, no
paramétricamente, la función de densidad, \(f\), de una población, a
partir de una muestra, \(\left( X_1,X_2,\ldots ,X_n \right)\),
procedente de la misma. En ese contexto es bien conocido el método
histograma (basado en el cual sería posible idear un método bootstrap)
aunque es más recomendable utilizar el estimador tipo núcleo propuesto
por Parzen (1962) y Rosenblatt (1956), que viene dado por

\[\hat{f}_{h}\left( x \right) =\frac{1}{nh}\sum_{i=1}^{n}K\left( \frac{x-X_i}{
h} \right) =\frac{1}{n}\sum_{i=1}^{n}K_{h}\left( x-X_i \right),\]

donde
\[K_{h}\left( u \right) =\frac{1}{h}K\left( \frac{u}{h} \right),\]
\(K\) es una función núcleo (normalmente una densidad simétrica en torno
al cero) y \(h>0\) es una parámetro de suavizado, llamado ventana, que
regula el tamaño del entorno que se usa para llevar a cabo la
estimación. Este estimador generaliza el bien conocido histograma y, más
concretamente, su versión histograma móvil. Así, eligiendo como función
\(K\) la densidad de una \(\mathcal{U}\left( -1,1 \right)\), el estimador de
Parzen-Rosenblatt resulta:
\[\begin{aligned}
\frac{1}{nh}\sum_{i=1}^{n}\frac{1}{2}\mathbf{1}\left\{ \frac{x-X_i}{h}\in
\left( -1,1 \right) \right\} &= \frac{1}{2nh}\sum_{i=1}^{n}\mathbf{1}\left\{
X_i\in \left( x-h,x+h \right) \right\} \\
&= \frac{\#\left\{ X_i\in \left( x-h,x+h \right) \right\} }{2nh},
\end{aligned}\]
que no es más que la frecuencia relativa de datos \(X_i\) en el
intervalo \(\left( x-h,x+h \right)\) dividida entre la longitud del
intervalo en cuestión (\(2h\)).

Es habitual exigir que la función núcleo \(K\) sea no negativa y su
integral sea uno:
\[K\left( u \right) \geq 0,~\forall u,~\int_{-\infty }^{\infty }K\left(u \right) du=1.\]
Además también es frecuente exigir que \(K\) sea una
función simétrica (\(K\left( -u \right) =K\left( u \right)\)).

Aunque la elección de la función \(K\) no tiene gran impacto en las
propiedades del estimador (salvo sus condiciones de regularidad:
continuidad, diferenciabilidad, etc.) la elección del parámetro de
suavizado sí es muy importante para una correcta estimación. En otras
palabras, el tamaño del entorno usado para la estimación no paramétrica
debe ser adecuado (ni demasiado grande ni demasiado pequeño).

En \texttt{R} podemos emplear la función \texttt{density()} del paquete base para obtener
una estimación tipo núcleo de la densidad (con la ventana determinada
por el parámetro \texttt{bw}), aunque podríamos emplear implementaciones de otros
paquetes (en la Sección \ref{npden-r} se incluyen más detalles).
Por ejemplo, considerando el conjunto de datos \texttt{precip} (que contiene el promedio de precipitación, en pulgadas de lluvia, de 70 ciudades de Estados Unidos), podríamos utilizar el siguiente código {[}Figura \ref{fig:density}{]}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}}\NormalTok{ precip}
\NormalTok{npden }\OtherTok{\textless{}{-}} \FunctionTok{density}\NormalTok{(x)}
\CommentTok{\# npden \textless{}{-} density(x, bw = "SJ")}

\CommentTok{\# plot(npden)}
\NormalTok{bandwidth }\OtherTok{\textless{}{-}}\NormalTok{ npden}\SpecialCharTok{$}\NormalTok{bw}
\FunctionTok{hist}\NormalTok{(x, }\AttributeTok{freq =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{main =} \StringTok{"Kernel density estimation"}\NormalTok{, }
     \AttributeTok{xlab =} \FunctionTok{paste}\NormalTok{(}\StringTok{"Bandwidth ="}\NormalTok{, }\FunctionTok{formatC}\NormalTok{(bandwidth)), }\AttributeTok{lty =} \DecValTok{2}\NormalTok{, }
     \AttributeTok{border =} \StringTok{"darkgray"}\NormalTok{, }\AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{80}\NormalTok{), }\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.08}\NormalTok{))}
\FunctionTok{lines}\NormalTok{(npden, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\FunctionTok{rug}\NormalTok{(x, }\AttributeTok{col =} \StringTok{"darkgray"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[!htb]

{\centering \includegraphics[width=0.7\linewidth]{03-mod_boot_unif_files/figure-latex/density-1} 

}

\caption{Estimación tipo núcleo de la densidad de `precip`. }\label{fig:density}
\end{figure}

La sensibilidad del estimador tipo núcleo al parámetro de suavizado puede
observarse ejecutando el siguiente código (ver Figura \ref{fig:bandwidth-movie}, \href{./bandwidth-movie.gif}{bandwidth-movie.gif}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bws }\OtherTok{\textless{}{-}} \DecValTok{2}\SpecialCharTok{\^{}}\FunctionTok{seq}\NormalTok{(}\FunctionTok{log2}\NormalTok{(bandwidth }\SpecialCharTok{*} \FloatTok{0.01}\NormalTok{), }\FunctionTok{log2}\NormalTok{(bandwidth }\SpecialCharTok{*} \DecValTok{20}\NormalTok{), }\AttributeTok{len =} \DecValTok{50}\NormalTok{)}
\NormalTok{bws }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(bws, }\FunctionTok{rev}\NormalTok{(bws))}
\ControlFlowTok{for}\NormalTok{ (bw }\ControlFlowTok{in}\NormalTok{ bws)}
  \FunctionTok{plot}\NormalTok{(}\FunctionTok{density}\NormalTok{(x, }\AttributeTok{bw =}\NormalTok{ bw) , }\AttributeTok{main =} \StringTok{"Kernel density estimation"}\NormalTok{, }
         \AttributeTok{xlab =} \FunctionTok{paste}\NormalTok{(}\StringTok{"Bandwidth ="}\NormalTok{, }\FunctionTok{formatC}\NormalTok{(bw)), }
         \AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{80}\NormalTok{), }\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.08}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[!htb]

{\centering \includegraphics[width=0.7\linewidth]{03-mod_boot_unif_files/figure-latex/bandwidth-movie-1} 

}

\caption{Efecto de cambio en la ventana en la estimación tipo núcleo de la densidad.}\label{fig:bandwidth-movie}
\end{figure}

La función de distribución asociada al estimador tipo núcleo de la
función de densidad viene dada por
\[\begin{aligned}
\hat{F}_{h}\left( x \right) &= \int_{-\infty }^{x}\hat{f}_{h}\left( y \right) dy
=\int_{-\infty }^{x}\frac{1}{n}\sum_{i=1}^{n}\frac{1}{h}
K\left( \frac{y-X_i}{h} \right) dy \\
&= \frac{1}{nh}\sum_{i=1}^{n}\int_{-\infty }^{x}
K\left( \frac{y-X_i}{h} \right) dy \\
&= \frac{1}{n}\sum_{i=1}^{n}\int_{-\infty }^{\frac{x-X_i}{h}}K\left( u \right) du
=\frac{1}{n}\sum_{i=1}^{n}\mathbb{K}\left( \frac{x-X_i}{h} \right)
\end{aligned}\]
donde \(\mathbb{K}\) es la función de distribución
asociada al núcleo \(K\), es decir
\[\mathbb{K}\left( t \right) =\int_{-\infty }^{t}K\left(
u \right) du.\]

Por ejemplo, en el caso de del conjunto de datos de precipitaciones, el siguiente código compara la estimación tipo núcleo de la distribución con la empírica {[}Figura \ref{fig:pnp}{]}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Fn }\OtherTok{\textless{}{-}} \FunctionTok{ecdf}\NormalTok{(precip)}
\FunctionTok{curve}\NormalTok{(Fn, }\AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{75}\NormalTok{), }\AttributeTok{ylab =} \StringTok{"F(x)"}\NormalTok{, }\AttributeTok{type =} \StringTok{"s"}\NormalTok{)}
\NormalTok{Fnp }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{sapply}\NormalTok{(x, }\ControlFlowTok{function}\NormalTok{(y) }\FunctionTok{mean}\NormalTok{(}\FunctionTok{pnorm}\NormalTok{(y, precip, bandwidth)))}
\FunctionTok{curve}\NormalTok{(Fnp, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{, }\AttributeTok{add =} \ConstantTok{TRUE}\NormalTok{) }
\FunctionTok{legend}\NormalTok{(}\StringTok{"bottomright"}\NormalTok{, }\AttributeTok{legend =} \FunctionTok{c}\NormalTok{(}\StringTok{"Empírica"}\NormalTok{, }\StringTok{"Tipo núcleo"}\NormalTok{), }\AttributeTok{lty =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[!htb]

{\centering \includegraphics[width=0.7\linewidth]{03-mod_boot_unif_files/figure-latex/pnp-1} 

}

\caption{Estimación empírica y tipo núcleo de la función de distribución de `precip`. }\label{fig:pnp}
\end{figure}

El método bootstrap suavizado procede de la siguiente forma:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  A partir de la muestra \(\left( X_1,X_2,\ldots ,X_n \right)\) y
  utilizando un valor \(h>0\) como parámetro de suavizado, se calcula el
  estimador de Parzen-Rosenblatt \(\hat{f}_{h}\)
\item
  Se arrojan remuestras bootstrap \(\mathbf{X}^{\ast}=\left( X_1^{\ast},X_2^{\ast},\ldots ,X_n^{\ast} \right)\) a partir de
  la densidad \(\hat{f}_{h}\)
\item
  Calcular
  \(R^{\ast}=R\left( \mathbf{X}^{\ast},\hat{F}_{h} \right)\)
\item
  Repetir \(B\) veces los pasos 2-3 para obtener las réplicas bootstrap
  \(R^{\ast (1)}\), \(\ldots\), \(R^{\ast (B)}\)
\end{enumerate}

Para llevar a cabo un bootstrap que remuestree a partir del estimador
\(\hat{f}_{h}\left( x \right)\) es útil pensar en dicho estimador como una
combinación lineal convexa de funciones de densidad, \(K_{h}\left( x-X_i \right)\), cada una con coeficiente \(\frac{1}{n}\) en dicha
combinación lineal. Gracias a esa representación podemos simular
valores,
\(X^{\ast}\), procedentes de \(\hat{f}_{h}\left( x \right)\) en dos pasos (empleando el denominado método de composición; ver p.e. Fernández-Casal y Cao, 2020, \href{https://rubenfcasal.github.io/simbook/m\%C3\%A9todo-de-composici\%C3\%B3n.html}{Sección 5.4}).

En un primer paso elegiremos (aleatoriamente y con equiprobabilidad)
cuál de los índices \(i\in \left\{ 1,\ldots ,n\right\}\) vamos a
considerar y en un segundo paso simularemos \(X^{\ast}\) a partir de la
densidad \(K_{h}\left( \cdot -X_i \right)\). Esta última fase puede relacionarse fácilmente
con la simulación de un valor, \(V\), con densidad \(K\), sin más que hacer
\(X_i+hV\). Así, el paso 2 del algoritmo previo puede llevarse a cabo
mediante el siguiente procedimiento:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Para cada \(i=1,\ldots ,n\) arrojar \(U_i\sim \mathcal{U}\left( 0,1 \right)\) y
  \(V_i\) con densidad \(K\) y hacer \(X_i^{\ast}=X_{\left\lfloor nU_i\right\rfloor +1}+hV_i\)
\end{enumerate}

La equivalencia de ambas presentaciones del paso 2 viene dada por el
siguiente razonamiento. Denotando \(U\sim \mathcal{U}\left( 0,1 \right)\),
\(I=\left\lfloor nU\right\rfloor +1\) y \(V\sim K\), independiente de \(U\), se
tiene:

\[\begin{aligned}
P^{\ast}\left( X^{\ast}\leq x \right) &= \sum_{i=1}^{n}P^{\ast}\left(
\left. X^{\ast}\leq x\right\vert _{I=i} \right) P^{\ast}\left( I=i \right) \\
&= \sum_{i=1}^{n}P^{\ast}\left( \left. X_i+hV\leq x\right\vert
_{I=i} \right) P^{\ast}\left( I=i \right) \\
&= \sum_{i=1}^{n}P^{\ast}\left( \left. V\leq \frac{x-X_i}{h}
\right\vert _{X_i} \right) \frac{1}{n}=\frac{1}{n}\sum_{i=1}^{n}\mathbb{K}
\left( \frac{x-X_i}{h} \right),
\end{aligned}\]

cuya función de densidad es, como ya sabemos, \(\hat{f}_{h}\left( x \right)\). Esto justifica la presentación alternativa del paso 2, de
forma que el bootstrap suavizado puede pensarse, a partir del bootstrap
uniforme (\(X_i^{\ast}=X_{\left\lfloor nU_i\right\rfloor +1}\))
añadiendo al mismo una perturbación (\(hV_i\)) cuya magnitud viene dada
por el parámetro de suavizado (\(h\)) y cuya forma imita a la de una
variable aleatoria (\(V_i\)) con densidad \(K\).

Por ejemplo, la función \texttt{density()} emplea por defecto un núcleo
gaussiano, y como se muestra en la ayuda de esta función,
podemos emplear un código como el siguiente para obtener
\texttt{nsim} simulaciones (ver Figura \ref{fig:density-sim}):

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# simulation from a density() fit:}
\CommentTok{\# a kernel density fit is an equally{-}weighted mixture.}
\NormalTok{nsim }\OtherTok{\textless{}{-}} \FloatTok{1e6}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\CommentTok{\# x\_boot \textless{}{-} sample(x, nsim, replace = TRUE)}
\CommentTok{\# x\_boot \textless{}{-} x\_boot + bandwidth * rnorm(nsim)}
\NormalTok{x\_boot }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(nsim, }\FunctionTok{sample}\NormalTok{(x, nsim, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{), bandwidth)}

\FunctionTok{plot}\NormalTok{(npden, }\AttributeTok{main =} \StringTok{""}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(}\FunctionTok{density}\NormalTok{(x\_boot), }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[!htb]

{\centering \includegraphics[width=0.7\linewidth]{03-mod_boot_unif_files/figure-latex/density-sim-1} 

}

\caption{Estimaciónes tipo núcleo de las densidades de `precip` y de una simulación.}\label{fig:density-sim}
\end{figure}

Es fácil percatarse de que los posibles valores que puede tomar una
observación \(X_i^{\ast}\) de cada remuestra bootstrap son infinitos,
pues la variable \(V\) puede tomar infinitos posibles valores, según la
densidad de probabilidad \(K\). Esto significa que la distribución en el
remuestreo de la remuestra bootstrap, \(\mathbf{X}^{\ast}\), es
mucho más complicada que para el bootstrap uniforme. En particular no es
discreta y por tanto no puede caracterizarse a partir de vectores de
remuestreo sobre la muestra original. Un problema importante es la
elección del parámetro de suavizado, \(h\), en este procedimiento de
remuestreo. En la práctica es razonable elegir \(h\) como un valor
bastante pequeño, en relación con la desviación típica de la muestra. Es
fácil observar que en el caso extremo \(h=0\) este método de remuestreo se
reduce al bootstrap uniforme.

\begin{example}[Inferencia sobre la media con varianza conocida, continuación]
\protect\hypertarget{exm:media-dt-conocida-suav}{}{\label{exm:media-dt-conocida-suav} \iffalse (Inferencia sobre la media con varianza conocida, continuación) \fi{} }
Continuando con el ejemplo de tiempo de vida de microorganismos,
podemos modificar fácilmente el código mostrado en el Ejemplo \ref{exm:media-dt-conocida}, para implementar bootstrap suavizado
con función núcleo gaussiana, para calcular un intervalo de confianza
para la media poblacional con desviación típica conocida:
\end{example}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{muestra }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{0.143}\NormalTok{, }\FloatTok{0.182}\NormalTok{, }\FloatTok{0.256}\NormalTok{, }\FloatTok{0.26}\NormalTok{, }\FloatTok{0.27}\NormalTok{, }\FloatTok{0.437}\NormalTok{, }\FloatTok{0.509}\NormalTok{, }
             \FloatTok{0.611}\NormalTok{, }\FloatTok{0.712}\NormalTok{, }\FloatTok{1.04}\NormalTok{, }\FloatTok{1.09}\NormalTok{, }\FloatTok{1.15}\NormalTok{, }\FloatTok{1.46}\NormalTok{, }\FloatTok{1.88}\NormalTok{, }\FloatTok{2.08}\NormalTok{)}
\NormalTok{n }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(muestra)}
\NormalTok{sigma }\OtherTok{\textless{}{-}} \FloatTok{0.6}

\NormalTok{alfa }\OtherTok{\textless{}{-}} \FloatTok{0.05}
\NormalTok{x\_barra }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(muestra)}

\CommentTok{\# Remuestreo}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{B }\OtherTok{\textless{}{-}} \DecValTok{1000}
\CommentTok{\# h \textless{}{-} 1e{-}08}
\NormalTok{h }\OtherTok{\textless{}{-}} \FunctionTok{bw.SJ}\NormalTok{(muestra)}\SpecialCharTok{/}\DecValTok{2}
\NormalTok{estadistico\_boot }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(B)}
\ControlFlowTok{for}\NormalTok{ (k }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{B) \{}
    \CommentTok{\# remuestra \textless{}{-} sample(muestra, n, replace = TRUE)}
    \CommentTok{\# remuestrasu \textless{}{-} remuestra + h * rnorm(n, 0, 1)}
\NormalTok{    remuestrasu }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n, }\FunctionTok{sample}\NormalTok{(muestra, n, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{), h)}
\NormalTok{    x\_barra\_boot }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(remuestrasu)}
\NormalTok{    estadistico\_boot[k] }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(n) }\SpecialCharTok{*}\NormalTok{ (x\_barra\_boot }\SpecialCharTok{{-}}\NormalTok{ x\_barra)}\SpecialCharTok{/}\NormalTok{sigma}
\NormalTok{\}}

\CommentTok{\# Aproximación bootstrap de los ptos críticos}
\NormalTok{pto\_crit }\OtherTok{\textless{}{-}} \FunctionTok{quantile}\NormalTok{(estadistico\_boot, }\FunctionTok{c}\NormalTok{(alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{))}

\CommentTok{\# Construcción del IC}
\NormalTok{ic\_inf\_boot }\OtherTok{\textless{}{-}}\NormalTok{ x\_barra }\SpecialCharTok{{-}}\NormalTok{ pto\_crit[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ sigma}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(n)}
\NormalTok{ic\_sup\_boot }\OtherTok{\textless{}{-}}\NormalTok{ x\_barra }\SpecialCharTok{{-}}\NormalTok{ pto\_crit[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ sigma}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(n)}
\NormalTok{IC\_boot }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(ic\_inf\_boot, ic\_sup\_boot)}
\FunctionTok{names}\NormalTok{(IC\_boot) }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\DecValTok{100}\SpecialCharTok{*}\FunctionTok{c}\NormalTok{(alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{), }\StringTok{"\%"}\NormalTok{)}
\NormalTok{IC\_boot}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      2.5%     97.5% 
## 0.4668897 1.0798549
\end{verbatim}

Con el paquete \texttt{boot}, la recomendación es implementarlo como
un bootstrap paramétrico:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(boot)}
\NormalTok{ran.gen.smooth }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, mle) \{}
    \CommentTok{\# Función para generar muestras aleatorias mediante }
    \CommentTok{\# bootstrap suavizado con función núcleo gaussiana,}
    \CommentTok{\# mle contendrá la ventana.}
\NormalTok{    n }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(data)}
\NormalTok{    h }\OtherTok{\textless{}{-}}\NormalTok{ mle}
\NormalTok{    out }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n, }\FunctionTok{sample}\NormalTok{(data, n, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{), h)}
\NormalTok{    out}
\NormalTok{\}}

\NormalTok{statistic }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data)\{}
    \FunctionTok{c}\NormalTok{(}\FunctionTok{mean}\NormalTok{(data), sigma}\SpecialCharTok{\^{}}\DecValTok{2}\SpecialCharTok{/}\FunctionTok{length}\NormalTok{(data))}
\NormalTok{\}}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{res.boot }\OtherTok{\textless{}{-}} \FunctionTok{boot}\NormalTok{(muestra, statistic, }\AttributeTok{R =}\NormalTok{ B, }\AttributeTok{sim =} \StringTok{"parametric"}\NormalTok{,}
                 \AttributeTok{ran.gen =}\NormalTok{ ran.gen.smooth, }\AttributeTok{mle =}\NormalTok{ h)}

\FunctionTok{boot.ci}\NormalTok{(res.boot, }\AttributeTok{type =} \StringTok{"stud"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
## Based on 1000 bootstrap replicates
## 
## CALL : 
## boot.ci(boot.out = res.boot, type = "stud")
## 
## Intervals : 
## Level    Studentized     
## 95%   ( 0.4664,  1.0830 )  
## Calculations and Intervals on Original Scale
\end{verbatim}

\hypertarget{bootstrap-ponderado-y-bootstrap-sesgado}{%
\section{Bootstrap ponderado y bootstrap sesgado}\label{bootstrap-ponderado-y-bootstrap-sesgado}}

Mediante el nombre bootstrap ponderado se incluyen todos aquellos
métodos de remuestreo bootstrap en los que la distribución de la que se
remuestrea es discreta y asigna probabilidades sólo a los datos de la
muestra:
\[\hat{F}\left( X_i \right) -\hat{F}\left( X_i^{-} \right) = p_i
\text{, para }i=1,\ldots, n\]
siendo \(p_i\geq 0\) y \(\sum_{i=1}^{n}p_i=1\). En el caso particular \(p_i= \frac{1}{n}\) para todo \(i=1,\ldots ,n\), se tiene el bootstrap uniforme.
Veremos más adelante casos particulares de métodos bootstrap ponderados
en el contexto de datos censurados y también para datos dependientes.

El bootstrap ponderado da lugar al bootstrap sesgado cuando los pesos,
\(p_i\), se eligen de forma que el vector \(\mathbf{p}\) minimice
la distancia al vector de pesos del bootstrap uniforme
\(\left( \frac{1}{n},\ldots ,\frac{1}{n} \right)\),
sujeto a una serie de restricciones inherentes al problema en estudio.
Este método fue propuesto por Hall (1998).

\hypertarget{deficien-unif}{%
\section{Deficiencias del bootstrap uniforme}\label{deficien-unif}}

Supongamos un contexto paramétrico en el que la distribución
poblacional, \(F\), es la \(\mathcal{U}\left( 0,\theta \right)\). Nuestro interés
será hacer inferencia acerca del parámetro \(\theta\), para lo cual, dada
una muestra observada, \(\mathbf{X}=\left( X_1,X_2,\ldots ,X_n \right)\), consideraremos el estimador máximo verosímil en este
contexto: \(\hat{\theta}=X_{(n)}\). Para realizar dicha
inferencia estaremos interesados en aproximar la distribución de
\(R\left( \mathbf{X},F \right) =\hat{\theta}-\theta\)

La función de distribución en el muestreo, \(G\left( x \right)\), de
\(\hat{\theta}\) puede calcularse de forma sencilla:\[\begin{aligned}
G\left( x \right) &= P\left( \hat{\theta}\leq x \right) =P\left( X_{\left(
n \right)}\leq x \right) =P\left( X_i\leq x\,,\forall i\in \left\{ 1,\ldots
n\right\} \right) \\
&= \prod_{i=1}^{n}P\left( X_i\leq x \right) =F\left( x \right)^{n}=\left( 
\frac{x}{\theta } \right)^{n},\text{ si }x\in \left[ 0,\theta \right]\end{aligned}\]

con lo cual su función de densidad viene dada por
\[g\left( x \right) =\frac{n}{\theta }\left( \frac{x}{\theta } \right)^{n-1},
\text{ si }x\in \left[ 0,\theta \right] .\]
Tomando, por ejemplo, \(\theta =1\) y \(n=50\), esta función de densidad resulta
{[}Figura \ref{fig:den-max}{]}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{theta }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{50}
\FunctionTok{curve}\NormalTok{(n}\SpecialCharTok{/}\NormalTok{theta }\SpecialCharTok{*}\NormalTok{ (x}\SpecialCharTok{/}\NormalTok{theta)}\SpecialCharTok{\^{}}\NormalTok{(n }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{), }\DecValTok{0}\NormalTok{, theta, }\AttributeTok{ylab =} \StringTok{"Density"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[!htb]

{\centering \includegraphics[width=0.7\linewidth]{03-mod_boot_unif_files/figure-latex/den-max-1} 

}

\caption{Función de densidad del máximo de una muestra procedente de una uniforme.}\label{fig:den-max}
\end{figure}

Como consecuencia podemos hallar fácilmente el sesgo del estimador
\(\hat{\theta}\), ya que
\[E\left( \hat{\theta} \right) =\int_{0}^{\theta }x\frac{n}{\theta }\left( 
\frac{x}{\theta } \right)^{n-1}dx=\left[ \frac{n}{n+1}\frac{x^{n+1}}{\theta
^{n}}\right] _{x=0}^{x=\theta }=\frac{n}{n+1}\theta ,\]
con lo cual
\[Sesgo\left( \hat{\theta} \right) =E\left( \hat{\theta} \right)
-\theta = -\frac{\theta }{n+1}.\]
Se ve claramente que \(\hat{\theta}\)
es un estimador sesgado de \(\theta\), puesto que se tiene que
\(\hat{\theta}\leq \theta\) con probabilidad 1.

Si deseamos aproximar mediante bootstrap la distribución en el muestreo
de \(\hat{\theta}\) (o la de \(R\)) y utilizamos un bootstrap uniforme
(naïve), la versión bootstrap del estimador resulta ser
\(\hat{\theta}^{\ast }=X_{(n)}^{\ast}\), siendo
\(\mathbf{X}^{\ast}=\left( X_1^{\ast}\text{, }X_2^{\ast}\text{, }\ldots \text{, }X_n^{\ast } \right)\) una remuestra bootstrap obtenida a partir de la distribución
empírica \(F_n\). La distribución en el remuestreo de \(\hat{\theta} ^{\ast}\,\) resulta un poco más complicada pues es discreta y sólo puede
tomar cualquiera de los valores de la muestra.

Suponiendo que no hay empates en las observaciones de la muestra, es
fácil darse cuenta de que
\[P^{\ast}\left( \hat{\theta}^{\ast}\leq X_{(j)} \right)
=P^{\ast}\left( X_{(n)}^{\ast}\leq X_{(j)
} \right) =P^{\ast}\left( X_i^{\ast}\leq X_{(j)}\,,
 1 \leq i \leq n \right) =\left( \frac{j}{n} \right)^{n}\]
y, por tanto, su masa de probabilidad viene dada por
\[P^{\ast}\left( \hat{\theta}^{\ast}=X_{(j)} \right) =\left( 
\frac{j}{n} \right)^{n}-\left( \frac{j-1}{n} \right)^{n}\text{, }j=1,\ldots,n.\]

En particular,
\[P^{\ast}\left( \hat{\theta}^{\ast}=X_{(n)} \right) =1-\left( 1-
\frac{1}{n} \right)^{n}\rightarrow 1-\frac{1}{e}\simeq 0.6321,\]
con lo cual la distribución en remuestreo de \(R^{\ast}=R\left( \mathbf{X}^{\ast},F_n \right) =\hat{\theta}^{\ast}-X_{\left( n \right)}\) tiene un átomo de probabilidad en el valor \(0\) cuya
probabilidad tiende a \(1-\frac{1}{e}\) cuando el tamaño muestral tiende a
infinito, es decir

\[\lim_{n\rightarrow \infty }P^{\ast}\left( R^{\ast}=0 \right) =1-\frac{1}{e},\]

cosa que no ocurre con la distribución en el muestreo de \(R\), que es continua
con densidad:
\[g_R\left( x \right) =\frac{n}{\theta }\left( \frac{x + \theta}{\theta } \right)^{n-1},
\text{ si }x\in \left[ -\theta, 0\right].\]
De esta forma vemos que el bootstrap uniforme (no paramétrico) es inconsistente.

\begin{example}[Inferencia sobre el máximo de una distribución uniforme]
\protect\hypertarget{exm:boot-maximo-uniforme}{}{\label{exm:boot-maximo-uniforme} \iffalse (Inferencia sobre el máximo de una distribución uniforme) \fi{} }
\end{example}

El siguiente código implementa el método
bootstrap uniforme (también llamado naïve) para aproximar la
distribución del estadístico \(R=\hat{\theta}-\theta\), para una muestra
de tamaño \(n=50\), proveniente de una población con distribución
\(\mathcal{U}\left( 0,1\right)\) {[}Figura \ref{fig:boot-uniforme-maximo}{]}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{theta }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{50}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{muestra }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\DecValTok{50}\NormalTok{) }\SpecialCharTok{*}\NormalTok{ theta}
\NormalTok{theta\_est }\OtherTok{\textless{}{-}} \FunctionTok{max}\NormalTok{(muestra)}
\CommentTok{\# Remuestreo}
\NormalTok{B }\OtherTok{\textless{}{-}} \DecValTok{2000}
\NormalTok{maximo }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(B)}
\NormalTok{estadistico }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(B)}
\ControlFlowTok{for}\NormalTok{ (k }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{B) \{}
\NormalTok{    remuestra }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(muestra, n, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{    maximo[k] }\OtherTok{\textless{}{-}} \FunctionTok{max}\NormalTok{(remuestra)}
\NormalTok{    estadistico[k] }\OtherTok{\textless{}{-}}\NormalTok{ maximo[k] }\SpecialCharTok{{-}}\NormalTok{ theta\_est}
\NormalTok{\}}
\CommentTok{\# Distribución estadístico}
\NormalTok{xlim }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{theta}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{) }\CommentTok{\# c({-}theta, 0)}
\FunctionTok{hist}\NormalTok{(estadistico, }\AttributeTok{freq =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{main =} \StringTok{""}\NormalTok{, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{, }
     \AttributeTok{border =} \StringTok{"darkgray"}\NormalTok{, }\AttributeTok{xlim =}\NormalTok{ xlim)}
\FunctionTok{lines}\NormalTok{(}\FunctionTok{density}\NormalTok{(estadistico))}
\FunctionTok{rug}\NormalTok{(estadistico, }\AttributeTok{col =} \StringTok{"darkgray"}\NormalTok{)}
\FunctionTok{curve}\NormalTok{(n}\SpecialCharTok{/}\NormalTok{theta }\SpecialCharTok{*}\NormalTok{ ((x }\SpecialCharTok{+}\NormalTok{ theta)}\SpecialCharTok{/}\NormalTok{theta)}\SpecialCharTok{\^{}}\NormalTok{(n }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{), }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{add =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[!htb]

{\centering \includegraphics[width=0.7\linewidth]{03-mod_boot_unif_files/figure-latex/boot-uniforme-maximo-1} 

}

\caption{Distribución de las réplicas bootstrap (uniforme) del estadístico y distribución poblacional.}\label{fig:boot-uniforme-maximo}
\end{figure}

\hypertarget{ejemplo-muxe9todo-alternativo}{%
\subsection{Ejemplo (método alternativo)}\label{ejemplo-muxe9todo-alternativo}}

En este contexto, al conocer la familia paramétrica (\(\mathcal{U}\left( 0,\theta \right)\)) a la cual pertenece la distribución de la población de
partida, lo natural sería utilizar un bootstrap paramétrico, consistente
en obtener las remuestras bootstrap a partir de una distribución
uniforme con parámetro estimado:
\[\mathbf{X}^{\ast}=\left( X_1^{\ast}\text{, }X_2^{\ast}\text{, 
}\ldots \text{, }X_n^{\ast} \right), \text{ con } X_i^{\ast} \sim \mathcal{U}\left( 0,\hat{\theta}\right).\]
En estas circunstancias es muy sencillo obtener la distribución en el
remuestreo de \(\hat{\theta}^{\ast}\), ya que su deducción es totalmente
paralela a la de la distribución en el muestreo de \(\hat{\theta}\). Así,
la función de densidad de \(\hat{\theta}^{\ast}\) es
\[\hat{g}\left( x \right) =\frac{n}{\hat{\theta}}\left( \frac{x}{\hat{\theta}}
 \right)^{n-1},\text{ si }x\in \left[ 0,\hat{\theta}\right] .\]

Con lo cual, al utilizar un bootstrap paramétrico, la distribución en el
remuestreo de \(R^{\ast}=R\left( \mathbf{X}^{\ast},F_{\hat{ \theta}} \right) =\hat{\theta}^{\ast}-\hat{\theta}\) imita a la
distribución en muestreo de
\(R=R\left( \mathbf{X},F \right) =\hat{\theta}-\theta\).

\begin{example}[Inferencia sobre el máximo de una distribución uniforme, continuación]
\protect\hypertarget{exm:boot-maximo-parametrico}{}{\label{exm:boot-maximo-parametrico} \iffalse (Inferencia sobre el máximo de una distribución uniforme, continuación) \fi{} }
\end{example}

Para emplear el bootstrap paramétrico (que remuestrea de una distribución
uniforme con parámetro estimado) podríamos emplear un código muy similar al
del Ejemplo \ref{exm:boot-maximo-uniforme} {[}Figura \ref{fig:boot-parametrico-maximo}{]}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Remuestreo}
\NormalTok{B }\OtherTok{\textless{}{-}} \DecValTok{2000}
\NormalTok{maximo }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(B)}
\NormalTok{estadistico }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(B)}
\ControlFlowTok{for}\NormalTok{ (k }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{B) \{}
\NormalTok{    remuestra }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(n) }\SpecialCharTok{*}\NormalTok{ theta\_est}
\NormalTok{    maximo[k] }\OtherTok{\textless{}{-}} \FunctionTok{max}\NormalTok{(remuestra)}
\NormalTok{    estadistico[k] }\OtherTok{\textless{}{-}}\NormalTok{ maximo[k] }\SpecialCharTok{{-}}\NormalTok{ theta\_est}
\NormalTok{\}}
\CommentTok{\# Distribución estadístico}
\NormalTok{xlim }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{theta}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{) }\CommentTok{\# c({-}theta, 0)}
\FunctionTok{hist}\NormalTok{(estadistico, }\AttributeTok{freq =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{main =} \StringTok{""}\NormalTok{, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{, }
     \AttributeTok{border =} \StringTok{"darkgray"}\NormalTok{, }\AttributeTok{xlim =}\NormalTok{ xlim)}
\FunctionTok{lines}\NormalTok{(}\FunctionTok{density}\NormalTok{(estadistico))}
\FunctionTok{rug}\NormalTok{(estadistico, }\AttributeTok{col =} \StringTok{"darkgray"}\NormalTok{)}
\FunctionTok{curve}\NormalTok{(n}\SpecialCharTok{/}\NormalTok{theta }\SpecialCharTok{*}\NormalTok{ ((x }\SpecialCharTok{+}\NormalTok{ theta)}\SpecialCharTok{/}\NormalTok{theta)}\SpecialCharTok{\^{}}\NormalTok{(n }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{), }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{add =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[!htb]

{\centering \includegraphics[width=0.7\linewidth]{03-mod_boot_unif_files/figure-latex/boot-parametrico-maximo-1} 

}

\caption{Distribución bootstrap paramétrica y distribución poblacional.}\label{fig:boot-parametrico-maximo}
\end{figure}

\hypertarget{validez-de-la-aproximaciuxf3n-bootstrap}{%
\section{Validez de la aproximación Bootstrap}\label{validez-de-la-aproximaciuxf3n-bootstrap}}

Trataremos ahora de dar una justificación teórica del buen
funcionamiento del bootstrap uniforme. Para ello, por simplicidad, nos
centraremos en el problema de aproximar la distribución en el muestreo
del estadístico
\[R=R\left( \mathbf{X},F \right) =\sqrt{n}\frac{\bar{X}-\mu }{\sigma },\]
donde \(\mathbf{X}=\left( X_1,X_2,\ldots ,X_n \right)\) es una
m.a.s. procedente de una distribución \(F\), con media \(\mu\) y desviación
típica \(\sigma\). Sabemos que, bajo ciertas condiciones, el teorema
central del límite permite obtener la distribución asintótica de \(R\),
que es una \(\mathcal{N}\left( 0,1 \right)\), es decir
\[\lim_{n\rightarrow \infty }P\left( R\leq u \right) =\Phi \left( u \right),
\quad\forall u\in \mathbb{R},\]
siendo \(\Phi\) la función de distribución de una normal estándar, cuya
función de densidad denotaremos por \(\phi\).

Para ver cómo de buena es la aproximación por normal del estadístico
\(R\), debemos razonar cómo de rápida es la convergencia en el límite
anteriormente expuesto. La respuesta a esa pregunta viene dada por el
Teorema de Cramer que usa los llamados desarrollos de Edgeworth de un
estadístico para aproximarlo por una suma de términos, el primero es la
función de distribución normal estándar y los siguientes irán tendiendo
a cero sucesivamente más rápido cuando el tamaño muestral tiende a
infinito. Enunciemos ese resultado.

\begin{theorem}[Cramer]
\protect\hypertarget{thm:aprox-cramer}{}{\label{thm:aprox-cramer} \iffalse (Cramer) \fi{} } \vspace{0.5cm}

Consideremos variables aleatorias
\(X_1,X_2,\ldots ,X_n,\ldots\) independientes e idénticamente
distribuidas procedentes de una distribución \(F\), con media \(\mu\) y
desviación típica \(\sigma\). Supongamos que existe cierto \(j\), natural,
para el cual \(E\left( \left\vert X\right\vert^{j+2} \right) <\infty \,\), y que \(\lim_{\left\vert t\right\vert \rightarrow \infty }\left\vert \alpha \left( t \right) \right\vert <1\), siendo \(\alpha \left( t \right) =E\left( e^{itX} \right)\) la función característica de la población.
Entonces:
\[\begin{aligned}
P\left( R\leq u \right) &=P\left( \sqrt{n}\frac{\bar{X}-\mu }{\sigma }
\leq u \right) \\
&= \Phi \left( u \right) +n^{-\frac{1}{2}}p_1\left( u \right) \phi \left(
u \right) +\cdots +n^{-\frac{j-1}{2}}p_{j-1}\left( u \right) \phi \left(
u \right) +O\left( n^{-\frac{j}{2}} \right),
\end{aligned}\]
siendo los \(p_i\left( u \right)\) polinomios de grado \(3i-1\) cuyos coeficientes
dependen de los momentos de \(X\) de orden menor o igual que \(i+2\).
En particular
\[\begin{aligned}
p_1\left( u \right) &= -\frac{1}{6}\frac{k_3}{\sigma^{3}}\left(
u^2-1 \right), \\
p_2\left( u \right) &= -u\left[ \frac{1}{24}\frac{k_4}{\sigma^{4}}\left(
u^2-3 \right) +\frac{1}{72}\left( \frac{k_3}{\sigma^{3}} \right)
^2\left( u^{4}-10u^2+15 \right) \right] ,
\end{aligned}\]

siendo \(k_j\) el \(j\)-ésimo cumulante, es decir el términos que acompaña
a \(\frac{\left( it \right)^{j}}{j!}\) en el desarrollo en serie del
logaritmo de la función característica:
\[\log \alpha \left( t \right) =\sum_{j=1}^{\infty }k_j\frac{\left( it \right)
^{j}}{j!}.\]

Además dichos polinomios tienen paridad alternada, es
decir, \(p_1\) es simétrico, \(p_2\) es antisimétrico, \(p_3\) es
simétrico, y así sucesivamente:
\[p_1\left( -u \right) = p_1\left( u \right),\quad p_2\left( -u \right)
= -p_2\left( u \right),\quad p_3\left( -u \right) = p_3\left( u \right)
,\cdots\]
\end{theorem}

Existen ecuaciones que relacionan todos los cumulantes hasta cierto
orden con todos los momentos poblacionales hasta ese mismo orden. Dichas
ecuaciones permiten expresar los cumulantes en función de los momentos y
viceversa.

Como consecuencia de este resultado teórico, el grado de aproximación
entre la distribución de \(R\) y la normal estándar límite es
\(O (n^{-\frac{1}{2}})\). Sin embargo, puede razonarse
fácilmente que este orden de aproximación mejorará cuando utilizamos el
bootstrap uniforme, en lugar de la normal estándar, para aproximar la
distribución de \(R\). Un desarrollo de Edgeworth para la distribución en
el remuestreo de \(R^{\ast}\) permite obtener la siguiente expresión:
\[\begin{aligned}
P^{\ast}\left( R^{\ast}\leq u \right) &= \Phi \left( u \right) +n^{-\frac{1}{2}
}\hat{p}_1\left( u \right) \phi \left( u \right) +\cdots +n^{-\frac{j-1}{2}}
\hat{p}_{j-1}\left( u \right) \phi \left( u \right) \\
&+ O_{P}\left( n^{-\frac{j}{2}} \right),
\end{aligned}\]

donde los polinomios \(\hat{p}_i\left( u \right)\) tienen la misma
estructura que los \(p_i\left( u \right)\) pero reemplazando los
cumulantes teóricos por los empíricos y la desviación típica teórica por
la empírica. Así pues el grado de aproximación entre cada polinomio
\(\hat{p}_i( u )\) y su análogo teórico \(p_i( u )\) es
\(\hat{p}_i( u ) -p_i( u ) = O_{P}( n^{-\frac{1}{2}} )\).
Como consecuencia, puede obtenerse el orden de aproximación entre la distribución
en el muestreo de \(R\) y la distribución en el remuestreo de \(R^{\ast}\):
\[\begin{aligned}
P\left( R\leq u \right) -P^{\ast}\left( R^{\ast}\leq u \right) &=  n^{-\frac{1}{
2}}\left[ p_1\left( u \right) -\hat{p}_1\left( u \right) \right] \phi
\left( u \right) +O_{P}\left( n^{-1} \right) \\
&=  O_{P}\left( n^{-1} \right),\end{aligned}\]que es mejor que el orden
de aproximación de la normal estándar límite. Dichos órdenes pueden
resumirse en la siguiente tabla.

\begin{longtable}[]{@{}ll@{}}
\toprule
Aproximación & Orden \\
\midrule
\endhead
Normal límite & \(O\left( n^{-\frac{1}{2}} \right)\) \\
Boot. uniforme & \(O_{P}\left( n^{-1} \right)\) \\
\bottomrule
\end{longtable}

Usando razonamiento similares pueden encontrarse los órdenes de
aproximación, tanto de la normal límite, como del bootstrap uniforme y
del bootstrap simetrizado, cuando la distribucional de partida es
simétrica. En ese caso, \(p_1\left( u \right) =0\), ya que \(k_3\) es
cero debido a la simetría de la distribución poblacional. Sin embargo
\(\hat{p}_1\left( u \right)\) no es cero cuando se usa el bootstrap
uniforme, aunque sí lo es en el caso del bootstrap simetrizado. La
siguiente tabla recoge los órdenes de las distintas aproximaciones.

\begin{longtable}[]{@{}ll@{}}
\toprule
Aproximación & Orden \\
\midrule
\endhead
Normal límite & \(O\left( n^{-1} \right)\) \\
Boot. uniforme & \(O_{P}\left( n^{-1} \right)\) \\
Boot. simetrizado & \(O_{P}\left( n^{-\frac{3}{2}} \right)\) \\
\bottomrule
\end{longtable}

El siguiente resultado permite generalizar los desarrollos de Edgeworth
(Teorema \ref{thm:aprox-cramer})
a otros estadísticos (estandarizados o studentizados) obtenidos para
otros estimadores arbitrarios, \(\hat{\theta}\), no necesariamente iguales
a la media muestral.

\begin{theorem}[Bhattacharya-Ghosh]
\protect\hypertarget{thm:aprox-bhat-gho}{}{\label{thm:aprox-bhat-gho} \iffalse (Bhattacharya-Ghosh) \fi{} } \vspace{0.5cm}

Consideremos variables aleatorias
\(X_1,X_2,\ldots ,X_n,\ldots\) independientes e idénticamente
distribuidas procedentes de una distribución \(F\). Sea \(\theta =\theta \left( F \right)\) un parámetro de dicha distribución y \(\hat{\theta}\) un
estimador de dicho parámetro. Supongamos además
que\[\sqrt{n}\left( \hat{\theta}-\theta \right) \rightarrow \mathcal{N}\left( 0,\sigma
_{\theta }^2 \right),\]
en distribución. Entonces, bajo ciertas condiciones de regularidad
(pueden verse en Bhattacharya y Ghosh, 1978) se tiene:
\[\begin{aligned}
P\left( \sqrt{n}\frac{\hat{\theta}-\theta }{\sigma _{\theta }}\leq u \right)
= &\ \Phi \left( u \right) +n^{-\frac{1}{2}}p_1\left( u \right) \phi \left(
u \right) +\cdots \\
& +n^{-\frac{j-1}{2}}p_{j-1}\left( u \right) \phi \left( u \right) +O\left(
n^{-\frac{j}{2}} \right), \\
P\left( \sqrt{n}\frac{\hat{\theta}-\theta }{\hat{\sigma}_{\theta }}\leq
u \right) = &\ \Phi \left( u \right) +n^{-\frac{1}{2}}q_1\left( u \right) \phi
\left( u \right) +\cdots \\
& +n^{-\frac{j-1}{2}}q_{j-1}\left( u \right) \phi \left( u \right) +O\left(
n^{-\frac{j}{2}} \right),\end{aligned}\]

siendo los \(p_i\left( u \right)\) y \(q_i\left( u \right)\) polinomios
de grado \(3i-1\) con paridad alternada, es decir, \(p_1\) y \(q_1\) son
simétricos, \(p_2\) y \(q_2\) son antisimétricos, \(p_3\) y \(q_3\) son
simétricos y así sucesivamente.
\end{theorem}

\hypertarget{boot-reg}{%
\section{Bootstrap semiparamétrico y bootstrap residual}\label{boot-reg}}

En ocasiones nos pueden interesar modelos semiparamétricos, en los que se asume una componente paramétrica pero no se especifica por completo la distribución de los datos.
Una de las situaciones más habituales es en regresión, donde se puede considerar un modelo para la tendencia pero sin asumir una forma concreta para la distribución del error.

Nos centraremos en el caso de regresión y consideraremos como base el siguiente modelo general:
\begin{equation} 
  Y = m(\mathbf{X}) + \varepsilon,
  \label{eq:modelogeneral}
\end{equation}
donde \(Y\) es la respuesta, \(\mathbf{X}=(X_1, X_2, \ldots, X_p)\) es el vector de variables explicativas, \(m(\mathbf{x}) = E\left( \left. Y\right\vert_{\mathbf{X}=\mathbf{x}} \right)\) es la media condicional, denominada función de regresión (o tendencia), y \(\varepsilon\) es un error aleatorio de media cero y varianza \(\sigma^2\), independiente de \(\mathbf{X}\) (errores homocedásticos independientes).

Supondremos que el objetivo es, a partir de una muestra:
\[\left\{ \left( X_{1i}, \ldots, X_{pi}, Y_{i} \right)  : i = 1, \ldots, n \right\},\]
realizar inferencias sobre la distribución condicional
\(\left.Y \right\vert_{\mathbf{X}=\mathbf{x}}\).

El modelo \eqref{eq:modelogeneral} se corresponde con el denominado \emph{diseño aleatorio}, mas general.
Alternativamente se podría asumir que los valores de las variables explicativas no son aleatorios (por ejemplo han sido fijados por el experimentador), hablaríamos entonces de \emph{diseño fijo}.
Para realizar inferencias sobre modelos de regresión con errores homocedásticos se podrían emplear dos algoritmos bootstrap (e.g.~\href{http://cran.fhcrc.org/doc/Rnews/Rnews_2002-3.pdf}{Canty, 2002}, y subsecciones siguientes).
El primero consistiría en utilizar directamente bootstrap uniforme, remuestreando las observaciones, y sería adecuado para el caso de diseño aleatorio.
La otra alternativa, que podría ser más adecuada para el caso de diseño fijo, sería lo que se conoce como \emph{remuestreo residual}, \emph{remuestreo basado en modelos} o \emph{bootstrap semiparamétrico}.
En esta aproximación se mantienen fijos los valores de las variables explicativas y se remuestrean los residuos.
Una de las aplicaciones del bootstrap semiparamétrico es el contraste de hipótesis en regresión, que se tratará en la Sección \ref{contrastes-semiparametricos}.

Se puede generalizar el modelo \eqref{eq:modelogeneral} de diversas formas, por ejemplo asumiendo que la distribución del error depende de \(X\) únicamente a través de la varianza (error heterocedástico independiente).
En este caso se suele reescribir como:
\[Y = m(\mathbf{X}) + \sigma(\mathbf{X}) \varepsilon,\]
siendo \(\sigma^2(\mathbf{x}) = Var\left( \left. Y\right\vert_{\mathbf{X}=\mathbf{x}} \right)\) la varianza condicional y suponiendo adicionalmente que \(\varepsilon\) tiene varianza uno.
Se podría modificar el bootstrap residual para este caso pero habría que modelizar y estimar la varianza condicional.
Alternativamente se podría emplear el denominado \emph{Wild Bootstrap} que se describirá en la Sección \ref{wild-bootstrap} para el caso de modelos de regresión no paramétricos.

En esta sección nos centraremos en el caso de regresión lineal:
\[m_{\boldsymbol{\beta}}(\mathbf{x}) =  \beta_{0} + \beta_{1}X_{1} + \beta_{2}X_{2} + \cdots + \beta_{p}X_{p},\]
siendo \(\boldsymbol{\beta} = \left( \beta_{0}, \beta_{1}, \ldots, \beta_{p} \right)^{T}\) el vector de parámetros (desconocidos).
Su estimador mínimo cuadrático es:
\[\boldsymbol{\hat{\beta}} = \left( X^{T}X\right)^{-1}X^{T}\mathbf{Y},\]
siendo \(\mathbf{Y} = \left( Y_{1}, \ldots, Y_{n} \right)^{T}\) el vector de observaciones de la variable \(Y\) y \(X\) la denominada \emph{matriz del diseño} de las variables regresoras, cuyas filas son los valores observados de las variables explicativas.

En regresión lineal múltiple, bajo las hipótesis estructurales del modelo de normalidad y homocedásticidad, se dispone de resultados teóricos que permiten realizar inferencias sobre características de la distribución condicional. Si alguna de estas hipótesis no es cierta se podrían emplear aproximaciones basadas en resultados asintóticos, pero podrían ser poco adecuadas para tamaños muestrales no muy grandes. Alternativamente se podría emplear bootstrap.
Con otros métodos de regresión, como los modelos no paramétricos descritos en el Capítulo \ref{npreg}, es habitual emplear bootstrap para realizar inferencias sobre la distribución condicional.

En esta sección se empleará el conjunto de datos \texttt{Prestige} del paquete \texttt{carData}, considerando como variable respuesta \texttt{prestige} (puntuación de ocupaciones obtenidas a partir de una encuesta) y como variables explicativas: \texttt{income} (media de ingresos en la ocupación) y \texttt{education} (media de los años de educación).
Para ajustar el correspondiente modelo de regresión lineal podemos emplear el siguiente código:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(Prestige, }\AttributeTok{package =} \StringTok{"carData"}\NormalTok{)}
\CommentTok{\# ?Prestige}
\NormalTok{modelo }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(prestige }\SpecialCharTok{\textasciitilde{}}\NormalTok{ income }\SpecialCharTok{+}\NormalTok{ education, }\AttributeTok{data =}\NormalTok{ Prestige)}
\FunctionTok{summary}\NormalTok{(modelo)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = prestige ~ income + education, data = Prestige)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -19.4040  -5.3308   0.0154   4.9803  17.6889 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(>|t|)    
## (Intercept) -6.8477787  3.2189771  -2.127   0.0359 *  
## income       0.0013612  0.0002242   6.071 2.36e-08 ***
## education    4.1374444  0.3489120  11.858  < 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 7.81 on 99 degrees of freedom
## Multiple R-squared:  0.798,  Adjusted R-squared:  0.7939 
## F-statistic: 195.6 on 2 and 99 DF,  p-value: < 2.2e-16
\end{verbatim}

Como ejemplo, consideraremos que el objetivo es realizar inferencias sobre el coeficiente de determinación ajustado:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res }\OtherTok{\textless{}{-}} \FunctionTok{summary}\NormalTok{(modelo)}
\FunctionTok{names}\NormalTok{(res)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "call"          "terms"         "residuals"     "coefficients" 
##  [5] "aliased"       "sigma"         "df"            "r.squared"    
##  [9] "adj.r.squared" "fstatistic"    "cov.unscaled"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res}\SpecialCharTok{$}\NormalTok{adj.r.squared}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.7939201
\end{verbatim}

\hypertarget{boot-unif-reg}{%
\subsection{Remuestreo de las observaciones}\label{boot-unif-reg}}

Como ya se comentó, en regresión podríamos emplear bootstrap uniforme multidimensional para el caso de diseño aleatorio, aunque hay que tener en cuenta que con este método la distribución en el remuestreo de \(\left. Y^{\ast}\right\vert _{X^{\ast}=X_i}\) es degenerada.

En este caso, podríamos realizar inferencias sobre el coeficiente de determinación ajustado empleando el siguiente código:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(boot)}

\NormalTok{case.stat }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, i) \{}
\NormalTok{  fit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(prestige }\SpecialCharTok{\textasciitilde{}}\NormalTok{ income }\SpecialCharTok{+}\NormalTok{ education, }\AttributeTok{data =}\NormalTok{ data[i, ])}
  \FunctionTok{summary}\NormalTok{(fit)}\SpecialCharTok{$}\NormalTok{adj.r.squared}
\NormalTok{\}}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{boot.case }\OtherTok{\textless{}{-}} \FunctionTok{boot}\NormalTok{(Prestige, case.stat, }\AttributeTok{R =} \DecValTok{1000}\NormalTok{)}
\NormalTok{boot.case}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## ORDINARY NONPARAMETRIC BOOTSTRAP
## 
## 
## Call:
## boot(data = Prestige, statistic = case.stat, R = 1000)
## 
## 
## Bootstrap Statistics :
##      original      bias    std. error
## t1* 0.7939201 0.002495631   0.0315275
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# plot(boot.case)}
\FunctionTok{boot.ci}\NormalTok{(boot.case, }\AttributeTok{type =} \FunctionTok{c}\NormalTok{(}\StringTok{"basic"}\NormalTok{, }\StringTok{"perc"}\NormalTok{, }\StringTok{"bca"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
## Based on 1000 bootstrap replicates
## 
## CALL : 
## boot.ci(boot.out = boot.case, type = c("basic", "perc", "bca"))
## 
## Intervals : 
## Level      Basic              Percentile            BCa          
## 95%   ( 0.7331,  0.8570 )   ( 0.7308,  0.8547 )   ( 0.7203,  0.8497 )  
## Calculations and Intervals on Original Scale
\end{verbatim}

\hypertarget{boot-residual}{%
\subsection{Bootstrap residual}\label{boot-residual}}

Como ya se comentó, en el caso de diseño fijo podemos realizar un remuestreo de los residuos:
\[\mathbf{r} = \mathbf{Y} - X\hat{\mathbf{\beta}} = \mathbf{Y} - \hat{\mathbf{Y}}\]
obteniéndose las réplicas bootstrap:
\[\mathbf{Y}^{\ast} = \hat{\mathbf{Y}} + \mathbf{r}^{\ast}.\]
Por ejemplo, adaptando el código en Canty (2002) para este conjunto de datos, podríamos emplear:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pres.dat }\OtherTok{\textless{}{-}}\NormalTok{ Prestige}
\NormalTok{pres.dat}\SpecialCharTok{$}\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{fitted}\NormalTok{(modelo)}
\NormalTok{pres.dat}\SpecialCharTok{$}\NormalTok{res }\OtherTok{\textless{}{-}} \FunctionTok{residuals}\NormalTok{(modelo)}

\NormalTok{mod.stat }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, i) \{}
\NormalTok{    data}\SpecialCharTok{$}\NormalTok{prestige }\OtherTok{\textless{}{-}}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{fit }\SpecialCharTok{+}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{res[i]}
\NormalTok{    fit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(prestige }\SpecialCharTok{\textasciitilde{}}\NormalTok{ income }\SpecialCharTok{+}\NormalTok{ education, }\AttributeTok{data =}\NormalTok{ data)}
    \FunctionTok{summary}\NormalTok{(fit)}\SpecialCharTok{$}\NormalTok{adj.r.squared}
\NormalTok{\}}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{boot.mod }\OtherTok{\textless{}{-}} \FunctionTok{boot}\NormalTok{(pres.dat, mod.stat, }\AttributeTok{R =} \DecValTok{1000}\NormalTok{)}
\NormalTok{boot.mod}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## ORDINARY NONPARAMETRIC BOOTSTRAP
## 
## 
## Call:
## boot(data = pres.dat, statistic = mod.stat, R = 1000)
## 
## 
## Bootstrap Statistics :
##      original      bias    std. error
## t1* 0.7939201 0.004401997  0.02671996
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# plot(boot.mod)}
\FunctionTok{boot.ci}\NormalTok{(boot.mod, }\AttributeTok{type =} \FunctionTok{c}\NormalTok{(}\StringTok{"basic"}\NormalTok{, }\StringTok{"perc"}\NormalTok{, }\StringTok{"bca"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
## Based on 1000 bootstrap replicates
## 
## CALL : 
## boot.ci(boot.out = boot.mod, type = c("basic", "perc", "bca"))
## 
## Intervals : 
## Level      Basic              Percentile            BCa          
## 95%   ( 0.7407,  0.8464 )   ( 0.7415,  0.8471 )   ( 0.7244,  0.8331 )  
## Calculations and Intervals on Original Scale
## Some BCa intervals may be unstable
\end{verbatim}

Sin embargo, la variabilidad de los residuos no reproduce la de los verdaderos errores, por lo que podría ser preferible (especialmente si el tamaño muestral es pequeño) emplear la modificación descrita en Davison y Hinkley (1997, Alg. 6.3, p.~271).
Teniendo en cuenta que:
\[\mathbf{r} = \left( I - H \right)\mathbf{Y},\]
siendo \(H = X\left( X^{T}X\right)^{-1}X^{T}\) la matriz de proyección.
La idea es remuestrear los residuos reescalados (de forma que su varianza sea constante) y centrados \(e_i - \bar{e}\), siendo:
\[e_i = \frac{r_i}{\sqrt{1 - h_{ii}}},\]
donde \(h_{ii}\) es el valor de influencia o leverage, el elemento \(i\)-ésimo de la diagonal de \(H\).

En \texttt{R} podríamos obtener estos residuos mediante los comandos:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pres.dat}\SpecialCharTok{$}\NormalTok{sres }\OtherTok{\textless{}{-}} \FunctionTok{residuals}\NormalTok{(modelo)}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{hatvalues}\NormalTok{(modelo))}
\NormalTok{pres.dat}\SpecialCharTok{$}\NormalTok{sres }\OtherTok{\textless{}{-}}\NormalTok{ pres.dat}\SpecialCharTok{$}\NormalTok{sres }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(pres.dat}\SpecialCharTok{$}\NormalTok{sres)}
\end{Highlighting}
\end{Shaded}

Sin embargo puede ser más cómodo emplear la función \texttt{Boot()} del paquete \texttt{car} (que internamente llama a la función \texttt{boot()}),
como se describe en el apéndice ``Bootstrapping Regression Models in R'' del libro ``An R Companion to Applied Regression'' de Fox y Weisberg (2018), disponible \href{https://socialsciences.mcmaster.ca/jfox/Books/Companion/appendices/Appendix-Bootstrapping.pdf}{aquí}.

Esta función es de la forma:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{Boot}\NormalTok{(object, }\AttributeTok{f =}\NormalTok{ coef, }\AttributeTok{labels =} \FunctionTok{names}\NormalTok{(}\FunctionTok{f}\NormalTok{(object)), }\AttributeTok{R =} \DecValTok{999}\NormalTok{, }
     \AttributeTok{method =} \FunctionTok{c}\NormalTok{(}\StringTok{"case"}\NormalTok{, }\StringTok{"residual"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

donde:

\begin{itemize}
\item
  \texttt{object}: es un objeto que contiene el ajuste de un modelo de regresión.
\item
  \texttt{f}: es la función de estadísticos (utilizando el ajuste como argumento).
\item
  \texttt{method}: especifíca el tipo de remuestreo: remuestreo de observaciones (\texttt{"case"})
  o de residuos (\texttt{"residual"}), empleando la modificación descrita anteriormente.
\end{itemize}

\begin{exercise}
\protect\hypertarget{exr:boot-car}{}{\label{exr:boot-car} }
\end{exercise}
Emplear la función \texttt{Boot()} del paquete \texttt{car} para hacer inferencia sobre
el coeficiente de determinación ajustado del modelo de regresión lineal
que explica \texttt{prestige} a partir de \texttt{income} y \texttt{education}
(obtener una estimación del sesgo y de la predicción,
y una estimación por intervalo de confianza de este estadístico).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(car)}

\CommentTok{\# set.seed(DNI)}
\CommentTok{\# ...}
\end{Highlighting}
\end{Shaded}

\hypertarget{icboot}{%
\chapter{Intervalos de confianza bootstrap}\label{icboot}}

Consideremos el problema de construcción, mediante bootstrap, de un
intervalo de confianza bilateral, con nivel de confianza \(1-\alpha\),
para un parámetro \(\theta\) de la distribución \(F\). Una vez elegido el
método bootstrap adecuado a la información disponible en el contexto del
que se trate, un aspecto importante es el de la posible corrección de
los intervalos de confianza bootstrap, aproximados por el método de
Monte Carlo, al objeto de que la probabilidad de cobertura sea lo más
parecida posible al nivel nominal \(1-\alpha\). Comenzaremos analizando
el error de cobertura de los intervalos de confianza clásicos, los
basados en la distribución normal asintótica.

\hypertarget{icboot-norm}{%
\section{Intervalos basados en la distribución normal asintótica}\label{icboot-norm}}

Consideremos primeramente el caso más sencillo (y poco realista) de
construcción de un intervalo de confianza para la media, \(\mu \,\), con
desviación típica, \(\sigma\), conocida. El estadístico usado para
construir el intervalo de confianza es
\[R=\sqrt{n}\frac{\bar{X}-\mu }{\sigma }\]que cuando
\(n\rightarrow \infty\) tiende en distribución a una \(N\left( 0,1 \right)\). El intervalo de confianza basado en dicha aproximación
normal es
\(\hat{I}=\left( \bar{X}-\frac{\sigma }{\sqrt{n}}z_{\alpha /2}, \bar{X}+\frac{\sigma }{\sqrt{n}}z_{\alpha /2} \right)\). Mediante el
desarrollo de Edgeworth dado por el Teorema de Cramer es fácil obtener
una cota para el error de cobertura de dicho intervalo:

\[\begin{aligned}
P\left( \mu \in \hat{I} \right) -\left( 1-\alpha \right) =&\  P\left(
R<z_{\alpha /2} \right) -P\left( R\leq -z_{\alpha /2} \right) \\
&-\left( \Phi \left( z_{\alpha /2} \right) -\Phi \left( -z_{\alpha
/2} \right) \right) \\
=&\ \ n^{-\frac{1}{2}}p_1\left( z_{\alpha /2} \right) \phi \left( z_{\alpha
/2} \right) +O\left( n^{-1} \right) \\
&-\left( n^{-\frac{1}{2}}p_1\left( -z_{\alpha /2} \right) \phi \left(
-z_{\alpha /2} \right) +O\left( n^{-1} \right) \right) \\
=&\  O\left( n^{-1} \right),\end{aligned}\]

ya que por la simetría de las funciones \(p_1\left( u \right)\) y \(\phi \left( u \right)\) se tiene
\[p_1\left( -z_{\alpha /2} \right) \phi \left( -z_{\alpha /2} \right)
=p_1\left( z_{\alpha /2} \right) \phi \left( z_{\alpha /2} \right).\]
De esta forma, el orden del error de cobertura del intervalo de confianza
bilateral con desviación típica poblacional conocida es \(O\left( n^{-1} \right)\). Puede obtenerse fácilmente el orden del error de
cobertura de los intervalos unilaterales que resulta ser
\(O\left( n^{-\frac{1}{2}} \right)\).

En el caso más realista en que la desviación típica, \(\sigma\), sea
desconocida, el intervalo de confianza resulta
\[\hat{I}_{0}=\left( 
\bar{X}-\frac{S_n}{\sqrt{n}}z_{\alpha /2},\bar{X}+\frac{S_n}{
\sqrt{n}}z_{\alpha /2} \right).\]
Ahora, el estadístico en el que se basa la inferencia resulta:
\[R_1=\sqrt{n}\frac{\bar{X}-\mu }{S_n}.\]
Un desarrollo de Edgeworth del tipo del obtenido en el Teorema de Bhattacharya-Ghosh
permite acotar el error de cobertura de este intervalo:
\[\begin{aligned}
P\left( \mu \in \hat{I}_{0} \right) -\left( 1-\alpha \right) =&\ P\left(
R_1<z_{\alpha /2} \right) -P\left( R_1\leq -z_{\alpha /2} \right) \\
&-\left( \Phi \left( z_{\alpha /2} \right) -\Phi \left( -z_{\alpha
/2} \right) \right) \\
=&\ n^{-\frac{1}{2}}q_1\left( z_{\alpha /2} \right) \phi \left( z_{\alpha
/2} \right) +O\left( n^{-1} \right) \\
&-\left( n^{-\frac{1}{2}}q_1\left( -z_{\alpha /2} \right) \phi \left(
-z_{\alpha /2} \right) +O\left( n^{-1} \right) \right) \\
=&\ O\left( n^{-1} \right).
\end{aligned}\]

De esta forma, el orden del error de cobertura del intervalo de
confianza bilateral con desviación típica desconocida es \(O\left( n^{-1} \right)\). El orden del error de cobertura para el intervalo de
confianza unilateral resulta \(O\left( n^{-\frac{1}{2}} \right)\).

Si el parámetro de interés fuese otro arbitrario: \(\theta =\theta \left( F \right)\), no necesariamente la media, puede obtenerse,
análogamente un intervalo de confianza basado en la normal asintótica:
\[\hat{I}_{0}=\left( \hat{\theta}-\frac{\hat{\sigma}_{\theta }}{\sqrt{n}}
z_{\alpha /2},\hat{\theta}+\frac{\hat{\sigma}_{\theta }}{\sqrt{n}}
z_{\alpha/2} \right),\]
que está basado en el estadístico
\[R_1=\sqrt{n}\frac{\hat{\theta}-\theta }{\hat{\sigma}_{\theta }}.\]
De forma análoga a lo ya razonado para la media muestral, puede
deducirse que el error de cobertura del intervalo de confianza bilateral
tiene un orden de \(O\left( n^{-1} \right)\), mientras que para intervalos
unilaterales el orden es \(O\left( n^{-\frac{1}{2}} \right)\).

\hypertarget{icboot-basic}{%
\section{Método percentil (básico)}\label{icboot-basic}}

Este método se basa en la construcción del intervalo de confianza,
mediante bootstrap, a partir del estadístico no estandarizado
\[R_2=\sqrt{n}\left( \hat{\theta}-\theta \right).\]
Una vez realizado el correspondiente remuestreo (uniforme, suavizado,
simetrizado, \ldots), a partir de cierto estimador, \(\hat{F}\,\), de la
distribución poblacional, \(F\), la distribución en el muestreo de
\(R_2\) se aproxima mediante la distribución bootstrap de
\[R_2^{\ast}=\sqrt{n}\left( \hat{\theta}^{\ast}-\theta \left( \hat{F}
 \right) \right).\]
Así se obtienen valores \(x_{\alpha /2}\) y
\(x_{1-\alpha /2}\), siendo \(x_{\beta }\), tal que
\(P^{\ast}\left( R_2^{\ast }\leq x_{\beta } \right) =\beta\),
y a partir de ellos sabemos que
\[\begin{aligned}
1-\alpha &= 1-\frac{\alpha }{2}-\frac{\alpha }{2}=P^{\ast}\left(
R_2^{\ast}<x_{1-\alpha /2} \right) -P^{\ast}\left( R_2^{\ast}\leq
x_{\alpha /2} \right) \\
&= P^{\ast}\left( x_{\alpha /2}<R_2^{\ast}<x_{1-\alpha /2} \right),
\end{aligned}\]
con lo cual decimos que también ha de ser aproximadamente igual a
\(1-\alpha\) la siguiente probabilidad\[\begin{aligned}
P\left( x_{\alpha /2}<R_2<x_{1-\alpha /2} \right) &= P\left( x_{\alpha /2}<
\sqrt{n}\left( \hat{\theta}-\theta \right) <x_{1-\alpha /2} \right) \\
&= P\left( \hat{\theta}-\frac{x_{1-\alpha /2}}{\sqrt{n}}<\theta <\hat{\theta}
-\frac{x_{\alpha /2}}{\sqrt{n}} \right).\end{aligned}\]
Ello da pie a definir el intervalo de confianza bootstrap calculado
por el método percentil como
\[\hat{I}_1=\left( \hat{\theta}-\frac{x_{1-\alpha /2}}{\sqrt{n}},\hat{\theta}
-\frac{x_{\alpha /2}}{\sqrt{n}} \right).\]

Para estudiar el error de cobertura de este intervalo de confianza
conviene ver antes qué grado de aproximación existe entre la
distribución en el muestreo de \(R_2\) y la distribución bootstrap de
\(R_2^{\ast}\). A partir del Teorema de Bhattacharya-Ghosh se
tiene\[\begin{aligned}
P\left( R_2\leq v \right) &= P\left( \sqrt{n}\left( \hat{\theta}-\theta
 \right) \leq v \right) =P\left( \sqrt{n}\frac{\hat{\theta}-\theta }{\sigma
_{\theta }}\leq \frac{v}{\sigma _{\theta }} \right) \\
&= \Phi \left( \frac{v}{\sigma _{\theta }} \right) +O\left( n^{-\frac{1}{2}
} \right) \\
P^{\ast}\left( R_2^{\ast}\leq v \right) &= P^{\ast}\left( \sqrt{n}\left( 
\hat{\theta}^{\ast}-\theta \left( \hat{F} \right) \right) \leq v \right)
\\
&=  P^{\ast}\left( \sqrt{n}\frac{\hat{\theta}^{\ast}-\theta \left( \hat{F}
 \right)}{\sigma _{\hat{\theta}}}\leq \frac{v}{\sigma _{\hat{\theta}}} \right)
=\Phi \left( \frac{v}{\sigma _{\hat{\theta}}} \right) +O_{P}\left( n^{-
\frac{1}{2}} \right).
\end{aligned}\]

Como consecuencia
\[P^{\ast}\left( R_2^{\ast}\leq v \right) -P\left( R_2\leq v \right) =\Phi
\left( \frac{v}{\sigma _{\hat{\theta}}} \right) -\Phi \left( \frac{v}{\sigma
_{\theta }} \right) +O_{P}\left( n^{-\frac{1}{2}} \right) =O_{P}\left( n^{-
\frac{1}{2}} \right),\]
ya que, típicamente, \(\sigma _{\hat{\theta}}-\sigma _{\theta} = O_{P}\left( n^{-\frac{1}{2}} \right)\). En resumen, la distribución en
el muestreo de \(R_2\) y la distribución bootstrap de \(R_2^{\ast}\) se
aproximan, una a la otra, a la velocidad \(O_{P}\left( n^{-\frac{1}{2} } \right)\), cuando \(n\rightarrow \infty\).

El error de cobertura del intervalo de confianza bilateral calculado
mediante bootstrap por el método percentil es
\[\begin{aligned}
P\left( \theta \in \hat{I}_1 \right) -\left( 1-\alpha \right) =&\ P\left(
R_2<x_{1-\alpha /2} \right) -P\left( R_2\leq x_{\alpha /2} \right) \\
&-\left[ P^{\ast}\left( R_2^{\ast}<x_{1-\alpha /2} \right) -P^{\ast
}\left( R_2^{\ast}\leq x_{\alpha /2} \right) \right] \\
=&\ P\left( R_2<x_{1-\alpha /2} \right) -P^{\ast}\left( R_2^{\ast
}<x_{1-\alpha /2} \right) \\
&-\left[ P\left( R_2\leq x_{\alpha /2} \right) -P^{\ast}\left(
R_2^{\ast}\leq x_{\alpha /2} \right) \right] \\
=&\ O_{P}\left( n^{-\frac{1}{2}} \right)
\end{aligned}\]

De esta forma el error de cobertura para los intervalos de confianza
bilaterales bootstrap obtenidos mediante el método percentil es \(O\left( n^{-\frac{1}{2}} \right)\). Puede deducirse que ese es también el orden
para los intervalos unilaterales obtenidos por este método. Así pues el
orden del error de cobertura para el método percentil cuando se
construyen intervalos de confianza unilaterales coincide con el de los
construidos usando la normal asintótica pero el orden del error de
cobertura de los intervalos bilaterales bootstrap constuidos por el
método percentil es peor que el de los basados en la normal asintótica,
que es del orden \(O\left( n^{-1} \right)\).

\begin{example}[Inferencia sobre la media con varianza desconocida, continuación]
\protect\hypertarget{exm:media-dt-desconocida-perc}{}{\label{exm:media-dt-desconocida-perc} \iffalse (Inferencia sobre la media con varianza desconocida, continuación) \fi{} } \vspace{0.5cm}

Continuando con el ejemplo de los tiempos de vida de microorganismos
(sin asumir varianza conocida),
supongamos que queremos obtener una estimación por intervalo de confianza
de su vida media empleando este método.
El código necesario sería muy similar al del Ejemplo \ref{exm:media-dt-desconocida}:
\end{example}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{muestra }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{0.143}\NormalTok{, }\FloatTok{0.182}\NormalTok{, }\FloatTok{0.256}\NormalTok{, }\FloatTok{0.26}\NormalTok{, }\FloatTok{0.27}\NormalTok{, }\FloatTok{0.437}\NormalTok{, }\FloatTok{0.509}\NormalTok{, }
             \FloatTok{0.611}\NormalTok{, }\FloatTok{0.712}\NormalTok{, }\FloatTok{1.04}\NormalTok{, }\FloatTok{1.09}\NormalTok{, }\FloatTok{1.15}\NormalTok{, }\FloatTok{1.46}\NormalTok{, }\FloatTok{1.88}\NormalTok{, }\FloatTok{2.08}\NormalTok{)}
\NormalTok{n }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(muestra)}
\NormalTok{alfa }\OtherTok{\textless{}{-}} \FloatTok{0.05}
\NormalTok{x\_barra }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(muestra)}

\CommentTok{\# Remuestreo}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{B }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{remuestra }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(n)}
\NormalTok{x\_barra\_boot }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(B) }
\NormalTok{estadistico\_boot }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(B)}
\ControlFlowTok{for}\NormalTok{ (k }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{B) \{}
\NormalTok{  remuestra }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(muestra, n, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{  x\_barra\_boot[k] }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(remuestra)}
\NormalTok{  estadistico\_boot[k] }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(n) }\SpecialCharTok{*}\NormalTok{ (x\_barra\_boot[k] }\SpecialCharTok{{-}}\NormalTok{ x\_barra)}
\NormalTok{\}}

\CommentTok{\# Aproximación bootstrap de los ptos críticos}
\NormalTok{pto\_crit }\OtherTok{\textless{}{-}} \FunctionTok{quantile}\NormalTok{(estadistico\_boot, }\FunctionTok{c}\NormalTok{(alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{))}

\CommentTok{\# Construcción del IC}
\NormalTok{ic\_inf\_boot }\OtherTok{\textless{}{-}}\NormalTok{ x\_barra }\SpecialCharTok{{-}}\NormalTok{ pto\_crit[}\DecValTok{2}\NormalTok{]}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(n)}
\NormalTok{ic\_sup\_boot }\OtherTok{\textless{}{-}}\NormalTok{ x\_barra }\SpecialCharTok{{-}}\NormalTok{ pto\_crit[}\DecValTok{1}\NormalTok{]}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(n)}
\NormalTok{IC\_boot }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(ic\_inf\_boot, ic\_sup\_boot)}
\FunctionTok{names}\NormalTok{(IC\_boot) }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\DecValTok{100}\SpecialCharTok{*}\FunctionTok{c}\NormalTok{(alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{), }\StringTok{"\%"}\NormalTok{)}
\NormalTok{IC\_boot}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      2.5%     97.5% 
## 0.4837233 1.1025650
\end{verbatim}

Aunque en este caso también podemos obtener el intervalo a
partir de las réplicas bootstrap del estimador:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pto\_crit }\OtherTok{\textless{}{-}} \FunctionTok{quantile}\NormalTok{(x\_barra\_boot, }\FunctionTok{c}\NormalTok{(alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{))}
\NormalTok{ic\_inf\_boot }\OtherTok{\textless{}{-}} \DecValTok{2}\SpecialCharTok{*}\NormalTok{x\_barra }\SpecialCharTok{{-}}\NormalTok{ pto\_crit[}\DecValTok{2}\NormalTok{]}
\NormalTok{ic\_sup\_boot }\OtherTok{\textless{}{-}} \DecValTok{2}\SpecialCharTok{*}\NormalTok{x\_barra }\SpecialCharTok{{-}}\NormalTok{ pto\_crit[}\DecValTok{1}\NormalTok{]}
\NormalTok{IC\_boot }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(ic\_inf\_boot, ic\_sup\_boot)}
\FunctionTok{names}\NormalTok{(IC\_boot) }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\DecValTok{100}\SpecialCharTok{*}\FunctionTok{c}\NormalTok{(alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{), }\StringTok{"\%"}\NormalTok{)}
\NormalTok{IC\_boot}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      2.5%     97.5% 
## 0.4837233 1.1025650
\end{verbatim}

Esta forma de proceder es la que emplea el paquete \texttt{boot} para obtener
el que denomina intervalo de confianza \emph{bootstrap básico}
(estableciendo \texttt{type="basic"} en la llamada a la función \texttt{boot.ci()}):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(boot)}
\NormalTok{statistic }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, i) }\FunctionTok{mean}\NormalTok{(data[i])}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{res.boot }\OtherTok{\textless{}{-}} \FunctionTok{boot}\NormalTok{(muestra, statistic, }\AttributeTok{R =} \DecValTok{1000}\NormalTok{)}
\NormalTok{res }\OtherTok{\textless{}{-}} \FunctionTok{boot.ci}\NormalTok{(res.boot, }\AttributeTok{type =} \StringTok{"basic"}\NormalTok{)}
\NormalTok{res}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
## Based on 1000 bootstrap replicates
## 
## CALL : 
## boot.ci(boot.out = res.boot, type = "basic")
## 
## Intervals : 
## Level      Basic         
## 95%   ( 0.4825,  1.0980 )  
## Calculations and Intervals on Original Scale
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{IC\_boot }\OtherTok{\textless{}{-}}\NormalTok{ res}\SpecialCharTok{$}\NormalTok{basic[}\DecValTok{4}\SpecialCharTok{:}\DecValTok{5}\NormalTok{]}
\NormalTok{IC\_boot}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.4824717 1.0980120
\end{verbatim}

Además del paquete \texttt{boot}, otros autores también denominan a este método
\emph{bootstrap básico} (\emph{bootstrap percentil básico} o incluso \emph{bootstrap natural}),
y utilizan la terminología \emph{bootstrap percentil} cuando se emplea
directamente el estimador como estadístico (\(R = \hat \theta\)) para
realizar inferencia. Con el paquete \texttt{boot} habrá que establecer \texttt{type="perc"}
en la llamada a la función \texttt{boot.ci()} para obtener el intervalo
correspondiente:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{boot.ci}\NormalTok{(res.boot, }\AttributeTok{type =} \StringTok{"perc"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
## Based on 1000 bootstrap replicates
## 
## CALL : 
## boot.ci(boot.out = res.boot, type = "perc")
## 
## Intervals : 
## Level     Percentile     
## 95%   ( 0.5127,  1.1282 )  
## Calculations and Intervals on Original Scale
\end{verbatim}

En este método se emplean directamente los cuantiles de las
réplicas bootstrap del estadístico:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# IC\_boot \textless{}{-}  quantile(res.boot$t, c(alfa/2, 1 {-} alfa/2)) \# type = 7}
\NormalTok{IC\_boot }\OtherTok{\textless{}{-}}  \FunctionTok{quantile}\NormalTok{(res.boot}\SpecialCharTok{$}\NormalTok{t, }\FunctionTok{c}\NormalTok{(alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{), }\AttributeTok{type =} \DecValTok{6}\NormalTok{)}
\NormalTok{IC\_boot }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      2.5%     97.5% 
## 0.5126517 1.1281950
\end{verbatim}

Asintóticamente ambos métodos son equivalentes, aunque en general
es preferible (evita sesgos) el bootstrap percentil básico.

\hypertarget{icboot-perc-t}{%
\section{\texorpdfstring{Método percentil-\emph{t}}{Método percentil-t}}\label{icboot-perc-t}}

Este método bootstrap, construye un intervalo de confianza bootstrap a
partir del estadístico studentizado:
\[R_1=\sqrt{n}\frac{\hat{\theta}-\theta }{\hat{\sigma}_{\theta }}.\]
Su distribución en el muestreo se aproxima mediante la distribución
bootstrap de
\[R_1^{\ast}=\sqrt{n}\frac{\hat{\theta}^{\ast}-\theta \left( \hat{F}
 \right)}{\hat{\sigma}_{\theta }^{\ast}}.\]
En este caso, los valores \(x_{\alpha /2}\) y
\(x_{1-\alpha /2}\), se obtienen a partir de esta última distribución
bootstrap, es decir, \(x_{\beta }\) se define a partir de \(P^{\ast}\left( R_1^{\ast}\leq x_{\beta } \right) =\beta\).
Como
\[1-\alpha =P^{\ast}\left( x_{\alpha /2}<R_1^{\ast}<x_{1-\alpha /2} \right),\]
razonamos que también ha de ser aproximadamente igual a \(1-\alpha\) la
siguiente probabilidad
\[\begin{aligned}
P\left( x_{\alpha /2}<R_1<x_{1-\alpha /2} \right) &= P\left( x_{\alpha /2}<
\sqrt{n}\frac{\hat{\theta}-\theta }{\hat{\sigma}_{\theta }}<x_{1-\alpha
/2} \right) \\
&= P\left( \hat{\theta}-\frac{\hat{\sigma}_{\theta }}{\sqrt{n}}x_{1-\alpha
/2}<\theta <\hat{\theta}-\frac{\hat{\sigma}_{\theta }}{\sqrt{n}}x_{\alpha
/2} \right).
\end{aligned}\]
Con lo cual, el intervalo de confianza bootstrap calculado
por el método percentil-\(t\) se define como
\[\hat{I}_2=\left( \hat{\theta}-\frac{\hat{\sigma}_{\theta }}{\sqrt{n}}
x_{1-\alpha /2},\hat{\theta}-\frac{\hat{\sigma}_{\theta }}{\sqrt{n}}
x_{\alpha /2} \right).\]

Utilizando el Teorema de Bhattacharya-Ghosh puede acotarse el error de
aproximación entre la distribución en el muestreo de \(R_1\) y la
distribución bootstrap de \(R_1^{\ast}\):
\[\begin{aligned}
P^{\ast}\left( R_1^{\ast}\leq u \right) -P\left( R_1\leq u \right)
=&\ P^{\ast}\left( \sqrt{n}\frac{\hat{\theta}^{\ast}-\theta \left( \hat{F}
 \right)}{\hat{\sigma}_{\theta }^{\ast}}\leq u \right)-P\left( \sqrt{n}
\frac{\hat{\theta}-\theta }{\hat{\sigma}_{\theta }}\leq u \right)  \\
=&\  \Phi \left( u \right) +n^{-\frac{1}{2}}\hat{q}_1\left( u \right) \phi
\left( u \right) +O_{P}\left( n^{-1} \right) \\
&-\left[ \Phi \left( u \right) +n^{-\frac{1}{2}}q_1\left( u \right) \phi
\left( u \right) +O\left( n^{-1} \right) \right] \\
=&\  n^{-\frac{1}{2}}\left[ \hat{q}_1\left( u \right) -q_1\left( u \right) 
\right] \phi \left( u \right) +O_{P}\left( n^{-1} \right) \\
=&\ O_{P}\left( n^{-1} \right),
\end{aligned}\]
ya que los coeficientes que aparecen en el polinomio \(\hat{q}_1\left( u \right)\) son estimadores \(\sqrt{n}\)-consistentes de los coeficientes
del polinomio \(q_1\left( u \right)\). Los de éste último dependen de
los momentos poblacionales y los del primero son sus correspondientes
versiones empíricas.

Así pues, el error de cobertura del intervalo de confianza bootstrap
bilateral calculado por el método percentil-\(t\) es
\[\begin{aligned}
P\left( \theta \in \hat{I}_2 \right) -\left( 1-\alpha \right) =&\  P\left(
R_1<x_{1-\alpha /2} \right) -P\left( R_1\leq x_{\alpha /2} \right) \\
& -\left[ P^{\ast}\left( R_1^{\ast}<x_{1-\alpha /2} \right) -P^{\ast
}\left( R_1^{\ast}\leq x_{\alpha /2} \right) \right] \\
=&\  P\left( R_1<x_{1-\alpha /2} \right) -P^{\ast}\left( R_1^{\ast
}<x_{1-\alpha /2} \right) \\
&-\left[ P\left( R_1\leq x_{\alpha /2} \right) -P^{\ast}\left(
R_1^{\ast}\leq x_{\alpha /2} \right) \right] \\
=&\  O_{P}\left( n^{-1} \right)
\end{aligned}\]

Se tiene entonces que el error de cobertura para los intervalos de
confianza bilaterales bootstrap obtenidos mediante el método
percentil-\(t\) es
\(O\left( n^{-1} \right)\). Puede deducirse que ese es también el orden
para los intervalos unilaterales obtenidos por este método. Así pues el
orden del error de cobertura para el método percentil-\(t\) cuando se
construyen intervalos de confianza unilaterales mejora al de los
intervalos unilaterales basados en la normal asintótica, con un error de
cobertura de orden \(O\left( n^{-\frac{1}{2}} \right)\). En el caso de los
intervalos de confianza bilaterales, el orden del error de cobertura
usando la normal asintótica o bien el bootstrap por el método
percentil-\(t\) es el mismo, \(O\left( n^{-1} \right)\) en ambos casos.

En el Ejemplo \ref{exm:media-dt-desconocida}, se implementó
este método para obtener una estimación por intervalo de confianza
del tiempo de vida medio de microorganismos.
En el Ejemplo \ref{exm:media-dt-desconocida-boot} se mostró como
calcular este intervalo empleando el paquete \texttt{boot} (haciendo que
la función \texttt{statistic} devuelva también la varianza del estadístico
y estableciendo \texttt{type="stud"} en la llamada a la función \texttt{boot.ci()}).

\hypertarget{icboot-perc-t-sim}{%
\section{\texorpdfstring{Método percentil-\emph{t} simetrizado}{Método percentil-t simetrizado}}\label{icboot-perc-t-sim}}

Es un método análogo al percentil-\(t\). Sólo difiere de él en la forma de
seleccionar los cuantiles de la distribución bootstrap. En lugar de
tomar cuantiles que dejen colas iguales (\(\frac{\alpha }{2}\) a la
izquierda y a la derecha, respectivamente), se eligen los cuantiles de
forma que sean simétricos. Así, dado el estadístico \(R_1\) y su versión
bootstrap \(R_1^{\ast}\), se considera el valor \(y_{1-\alpha }\) que
cumple \(P^{\ast}\left( \left\vert R_1^{\ast}\right\vert \leq y_{1-\alpha } \right) =1-\alpha\).
Así se tiene que
\[1-\alpha =P^{\ast}\left( -y_{1-\alpha }\leq R_1^{\ast}
\leq y_{1-\alpha} \right).\]
De esa forma se razona que también ha de ser aproximadamente
igual a \(1-\alpha\) la siguiente probabilidad
\[\begin{aligned}
P\left( -y_{1-\alpha }<R_1<y_{1-\alpha } \right) &= P\left( -y_{1-\alpha }
< \sqrt{n}\frac{\hat{\theta}-\theta }{\hat{\sigma}_{\theta }}
< y_{1-\alpha} \right) \\
&= P\left( \hat{\theta}-\frac{\hat{\sigma}_{\theta }}{\sqrt{n}}y_{1-\alpha
}<\theta <\hat{\theta}+\frac{\hat{\sigma}_{\theta }}{\sqrt{n}}y_{1-\alpha
} \right).
\end{aligned}\]

Con lo cual, el intervalo de confianza bootstrap calculado por el método
percentil-\(t\) simetrizado se define como
\[\hat{I}_3=\left( \hat{\theta}-\frac{\hat{\sigma}_{\theta }}{\sqrt{n}}
y_{1-\alpha },\hat{\theta}+\frac{\hat{\sigma}_{\theta }}{\sqrt{n}}
y_{1-\alpha } \right).\]

Utilizando el Teorema de Bhattacharya-Ghosh con desarrollos hasta el
orden \(n^{-1}\) se tiene:
\[\begin{aligned}
P^{\ast}\left( R_1^{\ast}\leq u \right) -P\left( R_1\leq u \right) 
=&\ P^{\ast}\left( \sqrt{n}\frac{\hat{\theta}^{\ast}-\theta \left( \hat{F}
 \right)}{\hat{\sigma}_{\theta }^{\ast}}\leq u \right) -P\left( \sqrt{n}
\frac{\hat{\theta}-\theta }{\hat{\sigma}_{\theta }}\leq u \right) \\
=&\ \Phi \left( u \right) +n^{-\frac{1}{2}}\hat{q}_1\left( u \right) \phi
\left( u \right) +n^{-1}\hat{q}_2\left( u \right) \phi \left( u \right)
+O_{P}\left( n^{-\frac{3}{2}} \right) \\
& -\left[ \Phi \left( u \right) +n^{-\frac{1}{2}}q_1\left( u \right) \phi
\left( u \right) +n^{-1}q_2\left( u \right) \phi \left( u \right) +O\left(
n^{-\frac{3}{2}} \right) \right] \\
=&\ n^{-\frac{1}{2}}\left[ \hat{q}_1\left( u \right) -q_1\left( u \right) 
\right] \phi \left( u \right) 
+n^{-1}\left[ \hat{q}_2\left( u \right) -q_2\left( u \right) \right] \phi
\left( u \right) +O_{P}\left( n^{-\frac{3}{2}} \right).
\end{aligned}\]

Como consecuencia, el error de cobertura del intervalo de confianza
bootstrap bilateral calculado por el método percentil-\(t\) simetrizado
es
\[\begin{aligned}
P\left( \theta \in \hat{I}_3 \right) -\left( 1-\alpha \right) 
=&\ P\left(-y_{1-\alpha }<R_1<y_{1-\alpha } \right) -P^{\ast}\left( -y_{1-\alpha
}<R_1^{\ast}<y_{1-\alpha } \right) \\
=&\ P\left( R_1<y_{1-\alpha } \right) -P^{\ast}\left( R_1^{\ast
}<y_{1-\alpha } \right) - \\
&-\left[ P\left( R_1\leq -y_{1-\alpha } \right) -P^{\ast}\left( R_1^{\ast
}\leq -y_{1-\alpha } \right) \right] \\
=&\ n^{-\frac{1}{2}}\left[ q_1\left( y_{1-\alpha } \right) -\hat{q}_1\left(
y_{1-\alpha } \right) \right] \phi \left( y_{1-\alpha } \right) \\
&+n^{-1}\left[ q_2\left( y_{1-\alpha } \right) -\hat{q}_2\left(
y_{1-\alpha } \right) \right] \phi \left( y_{1-\alpha } \right) \\
&-n^{-\frac{1}{2}}\left[ q_1\left( -y_{1-\alpha } \right) -\hat{q}_1\left(
-y_{1-\alpha } \right) \right] \phi \left( -y_{1-\alpha } \right) \\
&-n^{-1}\left[ q_2\left( -y_{1-\alpha } \right) -\hat{q}_2\left(
-y_{1-\alpha } \right) \right] \phi \left( -y_{1-\alpha } \right) +O_{P}\left(
n^{-\frac{3}{2}} \right) \\
=&\ 2n^{-1}\left[ q_2\left( y_{1-\alpha } \right) -\hat{q}_2\left(
y_{1-\alpha } \right) \right] \phi \left( y_{1-\alpha } \right) +O_{P}\left(
n^{-\frac{3}{2}} \right) \\
=&\ O_{P}\left( n^{-\frac{3}{2}} \right)
\end{aligned}\]

ya que los polinomios \(q_1\left( u \right)\) y
\(\hat{q}_1\left( u \right)\) son simétricos, \(q_2\left( u \right)\) y
\(\hat{q}_2\left( u \right)\) son antisimétricos, la función
\(\phi \left( u \right)\) es simétrica y los coeficientes que aparecen en
el polinomio \(\hat{q}_2\left( u \right)\) son estimadores \(\sqrt{n}\)-consistentes de los coeficientes
del polinomio \(q_2\left( u \right)\). Como consecuencia, el error de
cobertura para los intervalos de confianza bilaterales bootstrap
obtenidos mediante el método percentil-\(t\) simetrizado es
\(O\left( n^{-\frac{3}{2}} \right)\). Puede deducirse que el orden para
los intervalos unilaterales obtenidos por este método es
\(O\left( n^{-1} \right)\). Así pues el orden del error de cobertura para
el método percentil-\(t\) simetrizado cuando se construyen intervalos de
confianza unilaterales mejora al de los intervalos unilaterales basados
en la normal asintótica, con un error de cobertura de orden
\(O\left( n^{-\frac{1}{2}} \right)\), e iguala al orden del error de
cobertura de los obtenidos mediante el percentil-\(t\). En el caso de los
intervalos de confianza bilaterales, el orden del error de cobertura
usando el método percentil-\(t\) simetrizado es
\(O\left( n^{-\frac{3}{2}} \right)\), el cual mejora el orden
\(O\left( n^{-1} \right)\), que es el que presentan los intervalos basados
en la normal asintótica o bien en el método percentil-\(t\).

\hypertarget{cap-err-cober}{%
\section{Tabla resumen de los errores de cobertura}\label{cap-err-cober}}

\begin{longtable}[]{@{}lll@{}}
\toprule
Tipo de I.C. & Unilateral & Bilateral \\
\midrule
\endhead
Percentil & \(O_{P}\left( n^{-\frac{1}{2}}\right)\) & \(O_{P}\left(n^{-\frac{1}{2}} \right)\) \\
Percentil-\(t\) & \(O_{P}\left( n^{-1} \right)\) & \(O_{P}\left( n^{-1} \right)\) \\
Percentil-\(t\) simetrizado & \(O_{P}\left( n^{-1} \right)\) & \(O_{P}\left( n^{-\frac{3}{2}} \right)\) \\
\bottomrule
\end{longtable}

\hypertarget{icboot-ejem}{%
\section{Ejemplos}\label{icboot-ejem}}

\hypertarget{media-dt-desconocida-persim}{%
\subsection{\texorpdfstring{IC bootstrap para la media mediante el método percentil-\emph{t} simetrizado}{IC bootstrap para la media mediante el método percentil-t simetrizado}}\label{media-dt-desconocida-persim}}

Continuando con el ejemplo de los tiempos de vida de microorganismos
(sin asumir varianza conocida),
para obtener una estimación por intervalo de confianza
de su vida media empleando el método bootstrap percentil-\emph{t} simetrizado
se podría utilizar (ver Ejemplo \ref{exm:media-dt-desconocida}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{muestra }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{0.143}\NormalTok{, }\FloatTok{0.182}\NormalTok{, }\FloatTok{0.256}\NormalTok{, }\FloatTok{0.26}\NormalTok{, }\FloatTok{0.27}\NormalTok{, }\FloatTok{0.437}\NormalTok{, }\FloatTok{0.509}\NormalTok{, }
             \FloatTok{0.611}\NormalTok{, }\FloatTok{0.712}\NormalTok{, }\FloatTok{1.04}\NormalTok{, }\FloatTok{1.09}\NormalTok{, }\FloatTok{1.15}\NormalTok{, }\FloatTok{1.46}\NormalTok{, }\FloatTok{1.88}\NormalTok{, }\FloatTok{2.08}\NormalTok{)}
\NormalTok{n }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(muestra)}
\NormalTok{alfa }\OtherTok{\textless{}{-}} \FloatTok{0.05}
\NormalTok{x\_barra }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(muestra)}
\NormalTok{cuasi\_dt }\OtherTok{\textless{}{-}} \FunctionTok{sd}\NormalTok{(muestra)}

\CommentTok{\# Remuestreo}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{B }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{remuestra }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(n)}
\NormalTok{estadistico\_boot }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(B)}
\ControlFlowTok{for}\NormalTok{ (k }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{B) \{}
\NormalTok{  remuestra }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(muestra, n, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{  x\_barra\_boot }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(remuestra)}
\NormalTok{  cuasi\_dt\_boot }\OtherTok{\textless{}{-}} \FunctionTok{sd}\NormalTok{(remuestra)}
\NormalTok{  estadistico\_boot[k] }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(n) }\SpecialCharTok{*} \FunctionTok{abs}\NormalTok{(x\_barra\_boot }\SpecialCharTok{{-}}\NormalTok{ x\_barra)}\SpecialCharTok{/}\NormalTok{cuasi\_dt\_boot}
\NormalTok{\}}

\CommentTok{\# Aproximación bootstrap del pto crítico}
\NormalTok{pto\_crit }\OtherTok{\textless{}{-}} \FunctionTok{quantile}\NormalTok{(estadistico\_boot, }\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alfa)}

\CommentTok{\# Construcción del IC}
\NormalTok{ic\_inf\_boot }\OtherTok{\textless{}{-}}\NormalTok{ x\_barra }\SpecialCharTok{{-}}\NormalTok{ pto\_crit }\SpecialCharTok{*}\NormalTok{ cuasi\_dt}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(n)}
\NormalTok{ic\_sup\_boot }\OtherTok{\textless{}{-}}\NormalTok{ x\_barra }\SpecialCharTok{+}\NormalTok{ pto\_crit }\SpecialCharTok{*}\NormalTok{ cuasi\_dt}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(n)}
\NormalTok{IC\_boot }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(ic\_inf\_boot, ic\_sup\_boot)}
\FunctionTok{names}\NormalTok{(IC\_boot) }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\DecValTok{100}\SpecialCharTok{*}\FunctionTok{c}\NormalTok{(alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{), }\StringTok{"\%"}\NormalTok{)}
\NormalTok{IC\_boot}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      2.5%     97.5% 
## 0.4334742 1.1771924
\end{verbatim}

\hypertarget{estudio-sim-exp}{%
\subsection{Estudio de simulación}\label{estudio-sim-exp}}

El siguiente código permite realizar estudios de
simulación comparando las probabilidades de cobertura y las longitudes
de los intervalos de confianza clásicos (basados en normalidad),
bootstrap percentil, bootstrap percentil-\(t\) y bootstrap percentil-\(t\)
simetrizado para la media, en el caso de muestras procedentes de una
distribución \(\exp \left( \lambda \right)\).
En este caso se obtienen las estimaciones Monte Carlo a partir de 500
simulaciones con \(\lambda = 0.01\), tamaño muestral \(n=100\) y \(B=1000\) réplicas
bootstrap para un nivel de confianza nominal del 90\% (\(\alpha =0.10\)).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{t.ini }\OtherTok{\textless{}{-}} \FunctionTok{proc.time}\NormalTok{()}
\NormalTok{rate }\OtherTok{\textless{}{-}} \FloatTok{0.01}
\NormalTok{mu }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{/}\NormalTok{rate}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{100}

\NormalTok{alfa }\OtherTok{\textless{}{-}} \FloatTok{0.1}
\NormalTok{namesI }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\DecValTok{100}\SpecialCharTok{*}\FunctionTok{c}\NormalTok{(alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{), }\StringTok{"\%"}\NormalTok{)}

\NormalTok{B }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{percentil }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(B)}
\NormalTok{percentilt }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(B)}
\NormalTok{percentilts }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(B)}

\NormalTok{nsim }\OtherTok{\textless{}{-}} \DecValTok{500}
\NormalTok{resultados }\OtherTok{\textless{}{-}} \FunctionTok{array}\NormalTok{(}\AttributeTok{dim =} \FunctionTok{c}\NormalTok{(nsim, }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{))}
\FunctionTok{dimnames}\NormalTok{(resultados) }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\ConstantTok{NULL}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\StringTok{"Cobertura"}\NormalTok{, }\StringTok{"Longitud"}\NormalTok{),}
        \FunctionTok{c}\NormalTok{(}\StringTok{"Normal"}\NormalTok{, }\StringTok{"Percentil"}\NormalTok{, }\StringTok{"Percentil{-}t"}\NormalTok{, }\StringTok{"Percentil{-}t simetrizado"}\NormalTok{))}
\CommentTok{\# Bucle simulación}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ (isim }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{nsim) \{}
    \CommentTok{\# Aproximación clásica}
\NormalTok{    muestra }\OtherTok{\textless{}{-}} \FunctionTok{rexp}\NormalTok{(n, }\AttributeTok{rate =} \FloatTok{0.01}\NormalTok{)}
\NormalTok{    media }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(muestra)}
\NormalTok{    desv }\OtherTok{\textless{}{-}} \FunctionTok{sd}\NormalTok{(muestra)}
\NormalTok{    z }\OtherTok{\textless{}{-}} \FunctionTok{qnorm}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{)}
\NormalTok{    ic\_inf }\OtherTok{\textless{}{-}}\NormalTok{ media }\SpecialCharTok{{-}}\NormalTok{ z}\SpecialCharTok{*}\NormalTok{desv}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(n)}
\NormalTok{    ic\_sup }\OtherTok{\textless{}{-}}\NormalTok{ media }\SpecialCharTok{+}\NormalTok{ z}\SpecialCharTok{*}\NormalTok{desv}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(n)}
\NormalTok{    I0 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(ic\_inf, ic\_sup)}
    \CommentTok{\# names(I0) \textless{}{-} namesI}
\NormalTok{    resultados[isim, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ (I0[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{\textless{}}\NormalTok{ mu) }\SpecialCharTok{\&\&}\NormalTok{ (mu }\SpecialCharTok{\textless{}}\NormalTok{ I0[}\DecValTok{2}\NormalTok{])}
\NormalTok{    resultados[isim, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ I0[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{{-}}\NormalTok{ I0[}\DecValTok{1}\NormalTok{]}
    
    \CommentTok{\# Remuestreo bootstrap}
    \ControlFlowTok{for}\NormalTok{ (k }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{B) \{}
\NormalTok{        remuestra }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(muestra, n, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{        percentil[k] }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(n) }\SpecialCharTok{*}\NormalTok{ (}\FunctionTok{mean}\NormalTok{(remuestra) }\SpecialCharTok{{-}}\NormalTok{ media)}
\NormalTok{        percentilt[k] }\OtherTok{\textless{}{-}}\NormalTok{ percentil[k]}\SpecialCharTok{/}\FunctionTok{sd}\NormalTok{(remuestra)}
\NormalTok{        percentilts[k] }\OtherTok{\textless{}{-}} \FunctionTok{abs}\NormalTok{(percentilt[k])}
\NormalTok{    \}}
    
    \CommentTok{\# Aproximación bootstrap percentil}
\NormalTok{    pto\_crit }\OtherTok{\textless{}{-}} \FunctionTok{quantile}\NormalTok{(percentil, }\FunctionTok{c}\NormalTok{(alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{))}
    \CommentTok{\# Construcción del IC}
\NormalTok{    ic\_inf\_boot }\OtherTok{\textless{}{-}}\NormalTok{ media }\SpecialCharTok{{-}}\NormalTok{ pto\_crit[}\DecValTok{2}\NormalTok{]}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(n)}
\NormalTok{    ic\_sup\_boot }\OtherTok{\textless{}{-}}\NormalTok{ media }\SpecialCharTok{{-}}\NormalTok{ pto\_crit[}\DecValTok{1}\NormalTok{]}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(n)}
\NormalTok{    I1 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(ic\_inf\_boot, ic\_sup\_boot)}
    \CommentTok{\# names(I1) \textless{}{-} namesI}
\NormalTok{    resultados[isim, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ (I1[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{\textless{}}\NormalTok{ mu) }\SpecialCharTok{\&\&}\NormalTok{ (mu }\SpecialCharTok{\textless{}}\NormalTok{ I1[}\DecValTok{2}\NormalTok{])}
\NormalTok{    resultados[isim, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ I1[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{{-}}\NormalTok{ I1[}\DecValTok{1}\NormalTok{]}
    
    \CommentTok{\# Aproximación bootstrap percentil{-}t}
\NormalTok{    pto\_crit }\OtherTok{\textless{}{-}} \FunctionTok{quantile}\NormalTok{(percentilt, }\FunctionTok{c}\NormalTok{(alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{))}
    \CommentTok{\# Construcción del IC}
\NormalTok{    ic\_inf\_boot }\OtherTok{\textless{}{-}}\NormalTok{ media }\SpecialCharTok{{-}}\NormalTok{ pto\_crit[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ desv}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(n)}
\NormalTok{    ic\_sup\_boot }\OtherTok{\textless{}{-}}\NormalTok{ media }\SpecialCharTok{{-}}\NormalTok{ pto\_crit[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ desv}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(n)}
\NormalTok{    I2 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(ic\_inf\_boot, ic\_sup\_boot)}
    \CommentTok{\# names(I2) \textless{}{-} namesI}
\NormalTok{    resultados[isim, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ (I2[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{\textless{}}\NormalTok{ mu) }\SpecialCharTok{\&\&}\NormalTok{ (mu }\SpecialCharTok{\textless{}}\NormalTok{ I2[}\DecValTok{2}\NormalTok{])}
\NormalTok{    resultados[isim, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ I2[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{{-}}\NormalTok{ I2[}\DecValTok{1}\NormalTok{]}
    
    \CommentTok{\# Aproximación bootstrap percentil{-}t simetrizado}
\NormalTok{    pto\_crit }\OtherTok{\textless{}{-}} \FunctionTok{quantile}\NormalTok{(percentilts, }\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alfa)}
    \CommentTok{\# Construcción del IC}
\NormalTok{    ic\_inf\_boot }\OtherTok{\textless{}{-}}\NormalTok{ media }\SpecialCharTok{{-}}\NormalTok{ pto\_crit }\SpecialCharTok{*}\NormalTok{ desv}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(n)}
\NormalTok{    ic\_sup\_boot }\OtherTok{\textless{}{-}}\NormalTok{ media }\SpecialCharTok{+}\NormalTok{ pto\_crit }\SpecialCharTok{*}\NormalTok{ desv}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(n)}
\NormalTok{    I3 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(ic\_inf\_boot, ic\_sup\_boot)}
    \CommentTok{\# names(I3) \textless{}{-} namesI}
\NormalTok{    resultados[isim, }\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ (I3[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{\textless{}}\NormalTok{ mu) }\SpecialCharTok{\&\&}\NormalTok{ (mu }\SpecialCharTok{\textless{}}\NormalTok{ I3[}\DecValTok{2}\NormalTok{])}
\NormalTok{    resultados[isim, }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ I3[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{{-}}\NormalTok{ I3[}\DecValTok{1}\NormalTok{]}
\NormalTok{\}}

\NormalTok{t.fin }\OtherTok{\textless{}{-}} \FunctionTok{proc.time}\NormalTok{() }\SpecialCharTok{{-}}\NormalTok{ t.ini}
\NormalTok{t.fin}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    user  system elapsed 
##   15.87    0.02   16.27
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{apply}\NormalTok{(resultados, }\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{), mean)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##            Normal Percentil Percentil-t Percentil-t simetrizado
## Cobertura  0.8800   0.86600      0.8880                 0.88800
## Longitud  32.5022  32.13928     33.5653                33.49959
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# knitr::kable(t(apply(resultados, c(2, 3), mean)), digits = 3)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lrr@{}}
\toprule
Aproximación & Cobertura & Longitud \\
\midrule
\endhead
Normal & 0.892 & 32.243 \\
Percentil & 0.886 & 32.051 \\
Percentil-t & 0.912 & 33.395 \\
Percentil-t simetrizado & 0.904 & 33.342 \\
\bottomrule
\end{longtable}

La siguiente tabla recoge las probabilidades de cobertura, estimadas por
Monte Carlo, en una ejecución con tamaño muestral \(n=100\), \(N=10000\)
trials y \(B=1000\) réplicas bootstrap para un nivel de confianza nominal
del 90\% (\(\alpha =0.10\)).

\begin{longtable}[]{@{}ll@{}}
\toprule
Aproximación & Cobertura IC \\
\midrule
\endhead
Normal & 88.60\% \\
Boot. percentil & 88.60\% \\
Boot. percentil-\(t\) & 89.76\% \\
Boot. percentil-\(t\) simetrizado & 89.46\% \\
\bottomrule
\end{longtable}

En la Sección \ref{estudio-sim-boot} del Apéndice \ref{intro-hpc} se incluye un estudio similar, empleando computación en paralelo para comparar las probabilidades de cobertura y las longitudes de los intervalos de confianza implementados en la función \texttt{boot.ci()}.

\hypertarget{icboot-trans}{%
\subsection{IC bootstrap para el coeficiente de correlación}\label{icboot-trans}}

Supongamos que queremos estudiar la correlación entre dos variables \(X\) e \(Y\) a partir del coeficiente de correlación lineal de Pearson:
\[\rho =\frac{ Cov \left( X, Y \right) }
{ \sigma \left( X \right) \sigma \left( Y \right) },\]
cuyo estimador natural es el coeficiente de correlación muestral:
\[r=\frac{\sum_{i=1}^{n}(x_i-\overline{x})(y_i-\overline{y})}
{\sqrt{ \sum_{i=1}^{n}(x_i-\overline{x})^{2}} 
\sqrt{\sum_{i=1}^{n}(y_i-\overline{y})^{2}}},\]
que podemos calcular en \texttt{R} empleando la función \texttt{cor()}.

Para realizar inferencias sobre el coeficiente de correlación, como aproximación más simple, se puede considerar que la distribución muestral de \(r\) es aproximadamente normal y emplear el estadístico:

\begin{equation} 
\frac{r -\rho}{\sqrt{\frac{1 - r^2}{n - 2}}} \underset{aprox}{\sim } t_{n-2}
\label{eq:cor-t}
\end{equation}

Pero esta aproximación solo sería válida en el caso de muestras grandes (o si la distribución bivariante de \((X, Y)\) es aproximadamente normal) cuando la correlación entre las variables es débil o moderada.
En caso contrario la distribución muestral de \(r\) puede ser muy asimétrica y los resultados obtenidos con el estadístico anterior no ser muy adecuados (esto concuerda con lo observado en la Sección \ref{boot-unif-multi}, al emplear bootstrap uniforme multidimensional para hacer inferencia sobre \(R = r -\rho\)).
Para evitar este problema se suelen obtener intervalos de confianza para \(\rho\) empleando la transformación \(Z\) de Fisher (1915):
\[Z = \frac{1}{2}\ln \left( \frac{1+r}{1-r} \right) = \operatorname{arctanh}(r),\]
que es una transformación (aprox.) normalizadora y estabilizadora de la varianza.
Suponiendo que \((X, Y)\) es normal bivariante y que hay independencia entre las observaciones:
\[Z \sim \mathcal{N}\left( \frac{1}{2}\ln \left( \frac{1+\rho}{1-\rho} \right), \frac{1}{n-3} \right).\]
El intervalo de confianza asintótico se obtiene empleando la aproximación normal tradicional en la escala \(Z\) y aplicando posteriormente la transformación inversa:
\[r = \frac{\exp(2Z)-1}{\exp(2Z)+1} = \operatorname{tanh}(Z).\]

Esta aproximación está implementada en la función \texttt{cor.test()} del paquete base \texttt{stat} de R\footnote{Se puede obtener el código tecleando en la consola \texttt{stats:::cor.test.default}.}, además de que también realiza el contraste \(H_0: \rho = 0\) empleando el estadístico \eqref{eq:cor-t}.

Continuando con el ejemplo de la Sección \ref{boot-unif-multi}, para obtener un intervalo de confianza para el coeficiente de correlación lineal entre las variables \texttt{income} y \texttt{prestige} del conjunto de datos \texttt{Prestige}, podríamos emplear el siguiente código:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(Prestige, }\AttributeTok{package=}\StringTok{"carData"}\NormalTok{)}
\CommentTok{\# with(Prestige, cor.test(income, prestige))}
\FunctionTok{cor.test}\NormalTok{(Prestige}\SpecialCharTok{$}\NormalTok{income, Prestige}\SpecialCharTok{$}\NormalTok{prestige)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Pearson's product-moment correlation
## 
## data:  Prestige$income and Prestige$prestige
## t = 10.224, df = 100, p-value < 2.2e-16
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.6044711 0.7983807
## sample estimates:
##       cor 
## 0.7149057
\end{verbatim}

La función \texttt{boot.ci()} del paquete \texttt{boot} también permite obtener intervalos de confianza calculados en una escala transfomada del estadístico,
mediante los parámetros:

\begin{itemize}
\item
  \texttt{h}: función vectorial que define la transformación.
  Los intervalos se calculan en la escala de \(h(t)\) y se aplica la función inversa (si se especifica) para transformarlos a la escala original.
\item
  \texttt{hinv}: (opcional) función inversa de la transformación
  (si no se especifica solo se calculan los intervalos en la escala transformada).
\item
  \texttt{hdot}: (opcional) función derivada de la transformación
  (empleada por algunos métodos para aproximar la varianza en la escala transformada mediante el método delta).
\end{itemize}

Por ejemplo, para considerar la transformación \(Z\) de Fisher en este caso, se podría emplear el siguiente código:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(boot)}

\NormalTok{statistic }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, i)\{}
\NormalTok{  remuestra }\OtherTok{\textless{}{-}}\NormalTok{ data[i, ]}
  \FunctionTok{cor}\NormalTok{(remuestra}\SpecialCharTok{$}\NormalTok{income, remuestra}\SpecialCharTok{$}\NormalTok{prestige)}
\NormalTok{\}}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{res.boot }\OtherTok{\textless{}{-}} \FunctionTok{boot}\NormalTok{(Prestige, statistic, }\AttributeTok{R =} \DecValTok{1000}\NormalTok{)}

\NormalTok{h }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(t) }\FunctionTok{atanh}\NormalTok{(t)}
\NormalTok{hdot }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(t) }\DecValTok{1}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ t}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{hinv }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(t) }\FunctionTok{tanh}\NormalTok{(t)}

\CommentTok{\# boot.ci(res.boot, type = "norm", h = h)}
\CommentTok{\# boot.ci(res.boot, type = "norm", h = h, hinv = hinv)}
\FunctionTok{boot.ci}\NormalTok{(res.boot, }\AttributeTok{type =} \StringTok{"norm"}\NormalTok{, }\AttributeTok{h =}\NormalTok{ h, }\AttributeTok{hdot =}\NormalTok{ hdot, }\AttributeTok{hinv =}\NormalTok{ hinv)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
## Based on 1000 bootstrap replicates
## 
## CALL : 
## boot.ci(boot.out = res.boot, type = "norm", h = h, hdot = hdot, 
##     hinv = hinv)
## 
## Intervals : 
## Level      Normal        
## 95%   ( 0.6016,  0.7858 )  
## Calculations on Transformed Scale;  Intervals on Original Scale
\end{verbatim}

Esto sería en principo preferible a trabajar en la escala original, ya que la distribución bootstrap en la escala transformada se aproximaría más a la normalidad:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ht }\OtherTok{\textless{}{-}} \FunctionTok{h}\NormalTok{(res.boot}\SpecialCharTok{$}\NormalTok{t)}
\FunctionTok{hist}\NormalTok{(ht, }\AttributeTok{freq =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{breaks =} \StringTok{"FD"}\NormalTok{,}
     \AttributeTok{main =} \StringTok{"Distribución bootstrap en la escala transformada"}\NormalTok{)}
\FunctionTok{curve}\NormalTok{(}\FunctionTok{dnorm}\NormalTok{(x, }\AttributeTok{mean=}\FunctionTok{mean}\NormalTok{(ht), }\AttributeTok{sd=}\FunctionTok{sd}\NormalTok{(ht)), }\AttributeTok{lty =} \DecValTok{2}\NormalTok{, }\AttributeTok{add =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{04-ic_boot_files/figure-latex/unnamed-chunk-10-1} \end{center}

\hypertarget{contrastes}{%
\chapter{Aplicaciones del Bootstrap en contrastes de hipótesis}\label{contrastes}}

El objetivo de los contrastes de hipótesis es, a partir de la información
que proporciona una muestra, decidir (tratanto de controlar el riesgo de
equivocarse al no disponer de toda la información)
entre dos hipótesis sobre alguna característica de interés de la población:
hipótesis nula (\(H_{0}\)) e hipótesis alternativa (\(H_{1}\)).

Entre los distintos tipos de contrastes de hipótesis (e.g.~paramétricos,
no paramétricos, \ldots), nos centraremos principalmente en los contrastes
de bondad de ajuste. En este caso interesará distinguir principalmente
entre hipótesis nulas simples (especifican un único modelo) y compuestas
(especifican un conjunto/familia de modelos).

Para realizar el contraste se emplea un estadístico \(D\left( X_1,\ldots ,X_n;H_0\right)\),
que mide la discrepancia entre la muestra observada y la hipótesis nula,
con distribución conocida (o que se puede aproximar) bajo \(H_0\).
Por ejemplo, en el caso de una hipótesis nula paramétrica
es habitual emplear un estadístico estudentizado de la forma:
\[D\left( X_1,\ldots ,X_n;H_0\right) =
\frac{\hat{\theta}-\theta _0}{\hat\sigma_{\hat\theta}}\]
(o algún tipo de razón de verosimilitudes).

La regla de decisión depende de la hipótesis altervativa y
del riesgo asumible al rechazar \(H_0\) siendo cierta:
\[P\left( \text{rechazar }H_0\mid H_0\text{ cierta}\right) =\alpha,\]
denominado nivel de significación.
Se determina una región de rechazo (RR) a partir de los valores que tiende
a tomar el estadístico cuando \(H_1\) es cierta,
de forma que\footnote{Aunque cuando la hipotesis nula es compuesta:
  \(P\left( D\in RR \mid H_0\text{ cierta}\right) \leq \alpha\).}:
\[P\left( D\in RR \mid H_0\text{ cierta}\right) =\alpha.\]
Se rechaza la hipótesis nula cuando el valor observado del
estadístico \(\hat{d}=D\left( x_1,\ldots ,x_n;H_0\right)\) pertenece
a la región de rechazo.

Para medir el nivel de evidencia en contra de \(H_0\) se emplea el
\(p\)-valor del contraste (también denominado valor crítico o
tamaño del contraste), el menor valor del nivel
de significación para el que se rechaza \(H_0\)
(que se puede interpretar también como la
probabilidad de obtener una discrepancia mayor o igual que
\(\hat{d}\) cuando \(H_0\) es cierta).

El cálculo del \(p\)-valor dependerá por tanto de la hipótesis altervativa.
Por ejemplo, si el estadístico del contraste tiende a tomar valores
grandes cuando \(H_0\) es falsa (contraste unilateral derecho):
\[p = P\left( D \geq \hat{d} \middle| H_0\right).\]
En otros casos (contrastes bilaterales) hay evidencias en contra de
\(H_0\) si el estadístico toma valores significativamente grandes o pequeños.
En estos casos la distribución del estadístico del contraste bajo \(H_0\)
suele ser simétrica en torno al cero, por lo que:
\[p = 2P\left( D \geq \vert \hat{d} \vert \middle| H_0 \right).\]
Pero si esta distribución es asimétrica:
\[p = 2 \min \left\{ P\left( D \leq \hat{d} \middle| H_0 \right),
P\left( D \geq \hat{d} \middle| H_0\right) \right\}.\]

La regla de decisión a partir del \(p\)-valor es siempre la misma.
Rechazamos \(H_0\), al nivel de significación \(\alpha\), si \(p \leq \alpha\),
en cuyo caso se dice que el contraste es estadísticamente significativo
(rechazamos \(H_0\) con mayor seguridad cuanto más pequeño es el \(p\)-valor).
Por tanto, la correspondiente variable aleatoria \(\mathcal{P}\) debería verificar:
\[P\left( \mathcal{P} \leq \alpha \middle| H_0\right)= \alpha.\]
Es decir, la distribución del \(p\)-valor bajo \(H_0\) debería ser \(\mathcal{U}(0,1)\)
(si la distribución del estadístico del constrate es continua).

\hypertarget{aproximaciuxf3n-del-p-valor-mediante-remuestreo}{%
\section{Aproximación del p-valor mediante remuestreo}\label{aproximaciuxf3n-del-p-valor-mediante-remuestreo}}

En los métodos tradicionales de contrastes de hipótesis se conoce o se
puede aproximar la distribución del estadístico del contraste bajo \(H_0\).
Muchas de estas aproximaciones están basadas en resultados asintóticos
y pueden no ser adecuadas para tamaños muestrales pequeños.
En ese caso, o si no se dispone de estas herramientas,
se puede recurrir a métodos de remuestreo para aproximar el \(p\)-valor.
Uno de los procedimientos más antiguos es el denominado
\emph{contraste de permutaciones} (Fisher, 1935; Pitman, 1937; Welch, 1937).
Aunque el bootstrap paramétrico y el semiparamétrico son los
procedimientos de remuestreo más empleados para aproximar
la distribución del estadístico de contraste bajo la hipótesis nula.

La idea es obtener remuestras de una aproximación de la distribución del
estadístico bajo \(H_0\).
En el bootstrap paramétrico y semiparamétrico se estima la distribución
de los datos bajo la hipótesis nula, \(\hat{F}_0\), y se obtienen réplicas del
estadístico a partir de remuestras de esta distribución (no sería adecuado
emplear directamente la distribución empírica).
En el caso de los contrastes de permutaciones las remuestras se obtienen
directamente de los datos, remuestreando sin reemplazamiento los valores
de la respuesta (y manteniendo fijas las covariables).

Finalmente, se emplean las réplicas bootstrap
del estadístico \(d_1^{\ast},\ldots, d_B^{\ast}\) para aproximar el \(p\)-valor.
Por ejemplo, en el caso de un contraste unilateral en el que el estadístico del
contraste tiende a tomar valores grandes si la hipótesis nula es falsa,
se podría emplear como aproximación:
\[p_{boot} = \frac{1}{B}\#\left\{ d_i^{\ast} \geq \hat{d} \right\}.\]
Mientras que en el caso bilateral, asumiendo que la distribución del estadístico
no es necesariamente simétrica, habría que emplear:
\[p_{boot} = \frac{2}{B} \min \left(\#\left\{ d_i^{\ast} \leq \hat{d} \right\},
\#\left\{ d_i^{\ast} \geq \hat{d} \right\}\right).\]

\hypertarget{contrastes-parametricos}{%
\section{Contrastes bootstrap paramétricos}\label{contrastes-parametricos}}

En los casos en los que la hipótesis nula especifica por completo la distribución
(hipótesis nula simple) o solo desconocemos los valores de algunos parámetros
(hipótesis nula paramétrica compuesta) podemos emplear
bootstrap paramétrico para obtener las remuestras bootstrap de los datos
(realmente en el primer caso se trataría de simulaciones Monte Carlo).
Siempre hay que tener en cuenta que las réplicas bootstrap del estadístico se
deberían obtener empleando el mismo procedimiento utilizado en la muestra
(p.e. reestimando los parámetros si es el caso).

\hypertarget{ejemplo-contraste-de-kolmogorov-smirnov}{%
\subsection{Ejemplo: contraste de Kolmogorov-Smirnov}\label{ejemplo-contraste-de-kolmogorov-smirnov}}

Se trata de un contraste de bondad de ajuste (similar a la prueba de
Cramer-von Mises o a la de Anderson-Darling, implementadas en el paquete
\texttt{goftest} de R, que son en principio mejores).
A partir de \(X_1,\ldots ,X_n\) m.a.s. de \(X\) con función de distribución \(F\),
se pretende contrastar:
\[\left \{ 
\begin{array}{l}
H_0 : F = F_0 \\ 
H_1 : F \neq F_0 
\end{array}
\right. \]
siendo \(F_0\) una función de distribución continua.
El estadístico empleado para ello compara la función de distribución bajo
\(H_0\) (\(F_0\)) con la empírica (\(F_n\)):
\[\begin{aligned}
    D_n=&\sup_{x}|F_n(x)-F_0(x)| \\
    =&\max_{1 \leq i\leq n}\left \{
    |F_n(X_{(i)})-F_0(X_{(i)})|,|F_n(X_{(i-1)})-F_0(X_{(i)})|\right \} \\
    =&\max_{1 \leq i\leq n}\left \{ \frac{i}{n}-F_0(X_{(i)}), \ F_0(X_{(i)})-\frac{i-1}{n}\right \} \\
    =&\max_{1 \leq i\leq n}\left \{ D_{n,i}^{+},\ D_{n,i}^{-}\right \},
\end{aligned}\]
y su distribución bajo \(H_0\) no depende \(F_0\) (es de distribución libre),
si \(H_0\) es simple y \(F_0\) es continua.
Esta distribución está tabulada (para tamaños muestrales grandes se utiliza
la aproximación asintótica) y se rechaza \(H_0\) si el valor observado \(d\)
del estadístico es significativamente grande:
\[p = P \left( D_n \geq d \right) \leq \alpha.\]
Este método está implementado en la función \texttt{ks.test()} del paquete base de R:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ks.test}\NormalTok{(x, y, ...)}
\end{Highlighting}
\end{Shaded}

donde \texttt{x} es un vector que contiene los datos, \texttt{y} es una función de distribución
(o una cadena de texto que la especifica; también puede ser otro vector de datos
para el contraste de dos muestras) y \texttt{...} representa los parámetros de la distribución.

Si \(H_0\) es compuesta, el procedimiento habitual es estimar los parámetros desconocidos
por máxima verosimilitud y emplear \(\hat{F}_0\) en lugar de \(F_0\).
Sin embargo, al proceder de esta forma es de esperar que \(\hat{F}_0\) se aproxime más
que \(F_0\) a la distribución empírica, por lo que los cuantiles de la distribución de
\(D_n\) pueden ser demasiado conservativos (los \(p\)-valores tenderán a ser mayores de
lo que deberían) y se tenderá a aceptar la hipótesis nula.

Para evitar este problema, en el caso de contrastar normalidad se desarrolló el test
de Lilliefors, implementado en la función \texttt{lillie.test()} del paquete \texttt{nortest}
(también hay versiones en este paquete para los métodos de Cramer-von Mises y
Anderson-Darling). Como ejemplo analizaremos el comportamiento de ambos métodos
para contrastar normalidad considerando 1000 pruebas con muestras de tamaño 30 de
una \(\mathcal{N}(0,1)\) (estudiaremos el \emph{tamaño de los contrastes}).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Valores iniciales}
\FunctionTok{library}\NormalTok{(nortest)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{nx }\OtherTok{\textless{}{-}} \DecValTok{30}
\NormalTok{mx }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{sx }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{nsim }\OtherTok{\textless{}{-}} \DecValTok{1000}
\CommentTok{\# Realizar contrastes}
\NormalTok{pvalor.ks }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(nsim)}
\NormalTok{pvalor.lil }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(nsim)}
\ControlFlowTok{for}\NormalTok{(isim }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{nsim) \{}
\NormalTok{  rx }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(nx, mx, sx)}
\NormalTok{  pvalor.ks[isim] }\OtherTok{\textless{}{-}} \FunctionTok{ks.test}\NormalTok{(rx, }\StringTok{"pnorm"}\NormalTok{, }\FunctionTok{mean}\NormalTok{(rx), }\FunctionTok{sd}\NormalTok{(rx))}\SpecialCharTok{$}\NormalTok{p.value}
\NormalTok{  pvalor.lil[isim] }\OtherTok{\textless{}{-}} \FunctionTok{lillie.test}\NormalTok{(rx)}\SpecialCharTok{$}\NormalTok{p.value}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Bajo la hipótesis nula el \(p\)-valor debería de seguir una distribución uniforme,
por lo que podríamos generar el correspondiente histograma para estudiar el
tamaño del contraste. Alternativamente podríamos representar su función de
distribución empírica, que se correspondería con la proporción de rechazos
para los distintos niveles de significación.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{old.par }\OtherTok{\textless{}{-}} \FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\CommentTok{\# Histograma}
\FunctionTok{hist}\NormalTok{(pvalor.ks, }\AttributeTok{freq=}\ConstantTok{FALSE}\NormalTok{, }\AttributeTok{main =} \StringTok{"p{-}valor del test de KS"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{h=}\DecValTok{1}\NormalTok{, }\AttributeTok{lty=}\DecValTok{2}\NormalTok{)   }\CommentTok{\# curve(dunif(x,0,1), add=TRUE)}
\CommentTok{\# Distribución empírica}
\FunctionTok{curve}\NormalTok{(}\FunctionTok{ecdf}\NormalTok{(pvalor.ks)(x), }\AttributeTok{type =} \StringTok{"s"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }
      \AttributeTok{main =} \StringTok{\textquotesingle{}Tamaño del contraste KS\textquotesingle{}}\NormalTok{, }\AttributeTok{ylab =} \StringTok{\textquotesingle{}Proporción de rechazos\textquotesingle{}}\NormalTok{, }
      \AttributeTok{xlab =} \StringTok{\textquotesingle{}Nivel de significación\textquotesingle{}}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{a=}\DecValTok{0}\NormalTok{, }\AttributeTok{b=}\DecValTok{1}\NormalTok{, }\AttributeTok{lty=}\DecValTok{2}\NormalTok{)   }\CommentTok{\# curve(punif(x, 0, 1), add = TRUE)}
\CommentTok{\# Histograma}
\FunctionTok{hist}\NormalTok{(pvalor.lil, }\AttributeTok{freq=}\ConstantTok{FALSE}\NormalTok{, }\AttributeTok{main =} \StringTok{"p{-}valor del test de Lilliefors"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{h=}\DecValTok{1}\NormalTok{, }\AttributeTok{lty=}\DecValTok{2}\NormalTok{)   }\CommentTok{\# curve(dunif(x,0,1), add=TRUE)}
\CommentTok{\# Distribución empírica}
\FunctionTok{curve}\NormalTok{(}\FunctionTok{ecdf}\NormalTok{(pvalor.lil)(x), }\AttributeTok{type =} \StringTok{"s"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }
      \AttributeTok{main =} \StringTok{\textquotesingle{}Tamaño del contraste de Lilliefors\textquotesingle{}}\NormalTok{, }\AttributeTok{ylab =} \StringTok{\textquotesingle{}Proporción de rechazos\textquotesingle{}}\NormalTok{, }
      \AttributeTok{xlab =} \StringTok{\textquotesingle{}Nivel de significación\textquotesingle{}}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{a=}\DecValTok{0}\NormalTok{, }\AttributeTok{b=}\DecValTok{1}\NormalTok{, }\AttributeTok{lty=}\DecValTok{2}\NormalTok{)   }\CommentTok{\# curve(punif(x, 0, 1), add = TRUE)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{05-contrastes_files/figure-latex/unnamed-chunk-4-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(old.par)}
\end{Highlighting}
\end{Shaded}

En el caso del contraste de Kolmogorov-Smirnov (KS) se observa que el \(p\)-valor
tiende a tomar valores grandes y por tanto se rechaza la hipótesis nula
muchas menos veces de las que se debería.

En el caso de otras distribuciones se puede emplear bootstrap paramétrico para
aproximar la distribución del estadístico del contraste.
Es importante recordar que el bootstrap debería imitar el procedimiento
empleado sobre la muestra, por lo que en este caso también habría que estimar
los parámetros en cada remuestra
(en caso contrario aproximaríamos la distribución de \(D_n\)).

Por ejemplo, la siguiente función implementaría el contraste KS de
bondad de ajuste de una variable exponencial aproximando el
\(p\)-valor mediante bootstrap paramétrico:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ks.exp.boot }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x, }\AttributeTok{nboot =} \DecValTok{10}\SpecialCharTok{\^{}}\DecValTok{3}\NormalTok{) \{}
\NormalTok{  DNAME }\OtherTok{\textless{}{-}} \FunctionTok{deparse}\NormalTok{(}\FunctionTok{substitute}\NormalTok{(x))}
\NormalTok{  METHOD }\OtherTok{\textless{}{-}} \StringTok{"Kolmogorov{-}Smirnov Test of pexp by bootstrap"} 
\NormalTok{  n }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(x)}
\NormalTok{  RATE }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{/}\FunctionTok{mean}\NormalTok{(x)}
\NormalTok{  ks.exp.stat }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x, }\AttributeTok{rate =} \DecValTok{1}\SpecialCharTok{/}\FunctionTok{mean}\NormalTok{(x)) \{ }\CommentTok{\# se estima el parámetro}
\NormalTok{    DMinus }\OtherTok{\textless{}{-}} \FunctionTok{pexp}\NormalTok{(}\FunctionTok{sort}\NormalTok{(x), }\AttributeTok{rate=}\NormalTok{rate) }\SpecialCharTok{{-}}\NormalTok{ (}\DecValTok{0}\SpecialCharTok{:}\NormalTok{(n }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{))}\SpecialCharTok{/}\NormalTok{n}
\NormalTok{    DPlus }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{{-}}\NormalTok{ DMinus}
\NormalTok{    Dn }\OtherTok{=} \FunctionTok{max}\NormalTok{(}\FunctionTok{c}\NormalTok{(DMinus, DPlus))}
\NormalTok{  \}  }
\NormalTok{  STATISTIC }\OtherTok{\textless{}{-}} \FunctionTok{ks.exp.stat}\NormalTok{(x, }\AttributeTok{rate =}\NormalTok{ RATE) }
  \FunctionTok{names}\NormalTok{(STATISTIC) }\OtherTok{\textless{}{-}} \StringTok{"Dn"}
  \CommentTok{\# PVAL \textless{}{-} 0}
  \CommentTok{\# for(i in 1:nboot) \{}
  \CommentTok{\#   rx \textless{}{-} rexp(n, rate = RATE)}
  \CommentTok{\#   if (STATISTIC \textless{}= ks.exp.stat(rx)) PVAL \textless{}{-} PVAL + 1}
  \CommentTok{\# \}}
  \CommentTok{\# PVAL \textless{}{-} PVAL/nboot}
  \CommentTok{\# PVAL \textless{}{-} (PVAL + 1)/(nboot + 1) \# Alternativa para aproximar el p{-}valor}
\NormalTok{  rx }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{rexp}\NormalTok{(n}\SpecialCharTok{*}\NormalTok{nboot, }\AttributeTok{rate =}\NormalTok{ RATE), }\AttributeTok{ncol=}\NormalTok{n)}
\NormalTok{  PVAL }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(STATISTIC }\SpecialCharTok{\textless{}=} \FunctionTok{apply}\NormalTok{(rx, }\DecValTok{1}\NormalTok{, ks.exp.stat))}
  \FunctionTok{return}\NormalTok{(}\FunctionTok{structure}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{statistic =}\NormalTok{ STATISTIC, }\AttributeTok{alternative =} \StringTok{"two.sided"}\NormalTok{, }
                   \AttributeTok{p.value =}\NormalTok{ PVAL, }\AttributeTok{method =}\NormalTok{ METHOD, }\AttributeTok{data.name =}\NormalTok{ DNAME), }
                   \AttributeTok{class =} \StringTok{"htest"}\NormalTok{))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Como ejemplo estudiaremos el caso de contrastar una distribución exponencial
considerando 500 pruebas con muestras de tamaño 30 de una \(Exp(1)\)
y 200 réplicas bootstrap (para disminuir el tiempo de computación).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Valores iniciales}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{nx }\OtherTok{\textless{}{-}} \DecValTok{30}
\NormalTok{ratex }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{nsim }\OtherTok{\textless{}{-}} \DecValTok{500}
\CommentTok{\# Realizar contrastes}
\NormalTok{pvalor.ks }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(nsim)}
\NormalTok{pvalor.ks.boot }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(nsim)}
\ControlFlowTok{for}\NormalTok{(isim }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{nsim) \{}
\NormalTok{  rx }\OtherTok{\textless{}{-}} \FunctionTok{rexp}\NormalTok{(nx, ratex)}
\NormalTok{  pvalor.ks[isim] }\OtherTok{\textless{}{-}} \FunctionTok{ks.test}\NormalTok{(rx, }\StringTok{"pexp"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{/}\FunctionTok{mean}\NormalTok{(rx))}\SpecialCharTok{$}\NormalTok{p.value}
\NormalTok{  pvalor.ks.boot[isim] }\OtherTok{\textless{}{-}} \FunctionTok{ks.exp.boot}\NormalTok{(rx, }\AttributeTok{nboot =} \DecValTok{200}\NormalTok{)}\SpecialCharTok{$}\NormalTok{p.value}
\NormalTok{\}}
\CommentTok{\# Generar gráficos}
\NormalTok{old.par }\OtherTok{\textless{}{-}} \FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\CommentTok{\# Histograma}
\FunctionTok{hist}\NormalTok{(pvalor.ks, }\AttributeTok{freq=}\ConstantTok{FALSE}\NormalTok{, }\AttributeTok{main =} \StringTok{"p{-}valor del test KS"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{h=}\DecValTok{1}\NormalTok{, }\AttributeTok{lty=}\DecValTok{2}\NormalTok{)   }\CommentTok{\# curve(dunif(x,0,1), add=TRUE)}
\CommentTok{\# Distribución empírica}
\FunctionTok{curve}\NormalTok{(}\FunctionTok{ecdf}\NormalTok{(pvalor.ks)(x), }\AttributeTok{type =} \StringTok{"s"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }
      \AttributeTok{main =} \StringTok{\textquotesingle{}Tamaño del contraste KS\textquotesingle{}}\NormalTok{, }\AttributeTok{ylab =} \StringTok{\textquotesingle{}Proporción de rechazos\textquotesingle{}}\NormalTok{, }
      \AttributeTok{xlab =} \StringTok{\textquotesingle{}Nivel de significación\textquotesingle{}}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{a=}\DecValTok{0}\NormalTok{, }\AttributeTok{b=}\DecValTok{1}\NormalTok{, }\AttributeTok{lty=}\DecValTok{2}\NormalTok{)   }\CommentTok{\# curve(punif(x, 0, 1), add = TRUE)}
\CommentTok{\# Histograma}
\FunctionTok{hist}\NormalTok{(pvalor.ks.boot, }\AttributeTok{freq=}\ConstantTok{FALSE}\NormalTok{, }\AttributeTok{main =} \StringTok{"p{-}valor del test KS{-}boot"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{h=}\DecValTok{1}\NormalTok{, }\AttributeTok{lty=}\DecValTok{2}\NormalTok{)   }\CommentTok{\# curve(dunif(x,0,1), add=TRUE)}
\CommentTok{\# Distribución empírica}
\FunctionTok{curve}\NormalTok{(}\FunctionTok{ecdf}\NormalTok{(pvalor.ks.boot)(x), }\AttributeTok{type =} \StringTok{"s"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }
      \AttributeTok{main =} \StringTok{\textquotesingle{}Tamaño del contraste KS{-}boot\textquotesingle{}}\NormalTok{, }\AttributeTok{ylab =} \StringTok{\textquotesingle{}Proporción de rechazos\textquotesingle{}}\NormalTok{, }
      \AttributeTok{xlab =} \StringTok{\textquotesingle{}Nivel de significación\textquotesingle{}}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{a=}\DecValTok{0}\NormalTok{, }\AttributeTok{b=}\DecValTok{1}\NormalTok{, }\AttributeTok{lty=}\DecValTok{2}\NormalTok{)   }\CommentTok{\# curve(punif(x, 0, 1), add = TRUE)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{05-contrastes_files/figure-latex/unnamed-chunk-6-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(old.par)}
\end{Highlighting}
\end{Shaded}

\hypertarget{contrastes-de-permutaciones}{%
\section{Contrastes de permutaciones}\label{contrastes-de-permutaciones}}

Supongamos que a partir de una muestra
\(\left\{ \left( \mathbf{X}_i, Y_i\right): i=1,\ldots, n \right\}\)
estamos interesados en contrastar la hipótesis nula de independencia
entre \(\mathbf{X}\) e \(Y\):
\[H_0: F_{Y \mid \mathbf{X}} = F_Y\]
o equivalentemente que \(\mathbf{X}\) no influye en la distribución de \(Y\).

En este caso los valores de la respuesta serían intercambiables bajo la hipótesis nula,
por lo que podríamos obtener las remuestras manteniendo fijos los valores\footnote{Nótese que
  no se hace ninguna suposición sobre el tipo de covariables,
  podrían ser categóricas, numéricas o una combinación de ambas.}
\(\mathbf{X}_i\) y permutando los \(Y_i\). Es decir:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Generar \(Y^{\ast}_i\), con \(i=1,\ldots, n\), mediante muestreo
  sin reemplazamiento de \(\left\{ Y_i: i=1,\ldots, n \right\}\).
\item
  Considerar la remuestra bootstrap
  \(\left\{ \left( \mathbf{X}_i, Y^{\ast}_i\right): i=1,\ldots, n \right\}\).
\end{enumerate}

Se pueden realizar contrastes de este tipo con el paquete \texttt{boot} estableciendo
el parámetro \texttt{sim\ =\ "permutation"} al llamar a la función \texttt{boot()} (el argumento
\texttt{i} de la función \texttt{statistic} contendrá permutaciones del vector de índices).
Puede ser también de interés el paquete \href{https://cran.r-project.org/web/packages/coin/index.html}{\texttt{coin}},
que implementa muchos contrastes de este tipo.

\hypertarget{ejemplo-inferencia-sobre-el-coeficiente-de-correlaciuxf3n-lineal}{%
\subsection{Ejemplo: Inferencia sobre el coeficiente de correlación lineal}\label{ejemplo-inferencia-sobre-el-coeficiente-de-correlaciuxf3n-lineal}}

En esta sección consideraremos como ejemplo el conjunto de datos \texttt{dogs}
del paquete \texttt{boot}, que contiene observaciones sobre el consumo de
oxígeno cardíaco (\texttt{mvo}) y la presión ventricular izquierda (\texttt{lvp})
de 7 perros domésticos.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(boot)}
\FunctionTok{data}\NormalTok{(}\StringTok{\textquotesingle{}dogs\textquotesingle{}}\NormalTok{, }\AttributeTok{package =} \StringTok{"boot"}\NormalTok{)}
\CommentTok{\# plot(dogs)}
\end{Highlighting}
\end{Shaded}

Supongamos que estamos interesados en estudiar la correlación lineal
entre las variables \texttt{mvo} (\(X\)) y \texttt{lvp} (\(Y\)).
Para ello podemos considerar el coeficiente de correlación lineal de Pearson:
\[\rho =\frac{ Cov \left( X, Y \right) }
{ \sigma \left( X \right) \sigma \left( Y \right) }\]
Su estimador es el coeficiente de correlación muestral:
\[r=\frac{\sum_{i=1}^{n}(x_i-\overline{x})(y_i-\overline{y})}
{\sqrt{ \sum_{i=1}^{n}(x_i-\overline{x})^{2}} 
\sqrt{\sum_{i=1}^{n}(y_i-\overline{y})^{2}}},\]
que podemos calcular en \texttt{R} empleando la función \texttt{cor()}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{(dogs}\SpecialCharTok{$}\NormalTok{mvo, dogs}\SpecialCharTok{$}\NormalTok{lvp)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.8536946
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# with(dogs, cor(mvo, lvp))}
\end{Highlighting}
\end{Shaded}

Para realizar inferencias sobre \(\rho\) podemos emplear la función
\texttt{cor.test()}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor.test}\NormalTok{(dogs}\SpecialCharTok{$}\NormalTok{mvo, dogs}\SpecialCharTok{$}\NormalTok{lvp)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Pearson's product-moment correlation
## 
## data:  dogs$mvo and dogs$lvp
## t = 3.6655, df = 5, p-value = 0.01451
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.2818014 0.9780088
## sample estimates:
##       cor 
## 0.8536946
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# with(dogs, cor.test(mvo, lvp))}
\end{Highlighting}
\end{Shaded}

Esta función realiza el contraste \(H_0: \rho = 0\) empleando el estadístico:
\[\frac{r\sqrt{n - 2}}{\sqrt{1 - r^2}} \underset{aprox}{\sim } t_{n-2},\]
bajo la hipótesis nula de que la verdadera correlación es cero.
Alternativemente se pueden realizar contrastes unilaterales estableciendo
el parámetro \texttt{alternative} igual a \texttt{"less"} o \texttt{"greater"}.
Por ejemplo, para contrastar \(H_0: \rho \leq 0\) podríamos emplear:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor.test}\NormalTok{(dogs}\SpecialCharTok{$}\NormalTok{mvo, dogs}\SpecialCharTok{$}\NormalTok{lvp, }\AttributeTok{alternative =} \StringTok{"greater"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Pearson's product-moment correlation
## 
## data:  dogs$mvo and dogs$lvp
## t = 3.6655, df = 5, p-value = 0.007255
## alternative hypothesis: true correlation is greater than 0
## 95 percent confidence interval:
##  0.4195889 1.0000000
## sample estimates:
##       cor 
## 0.8536946
\end{verbatim}

Para realizar el contraste con la función \texttt{boot} podríamos
emplear el siguiente código:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(boot)}

\NormalTok{statistic }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, i) }\FunctionTok{cor}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{mvo, data}\SpecialCharTok{$}\NormalTok{lvp[i])}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{res.boot }\OtherTok{\textless{}{-}} \FunctionTok{boot}\NormalTok{(dogs, statistic, }\AttributeTok{R =} \DecValTok{1000}\NormalTok{, }\AttributeTok{sim =} \StringTok{"permutation"}\NormalTok{)}
\CommentTok{\# res.boot}
\end{Highlighting}
\end{Shaded}

Posteriormente emplearíamos las réplicas (almacenadas en \texttt{res.boot\$t}) y el valor
observado del estadístico del contraste (almacenado en \texttt{res.boot\$t0})
para aproximar el \(p\)-valor:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(res.boot}\SpecialCharTok{$}\NormalTok{t, }\AttributeTok{freq =} \ConstantTok{FALSE}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{v =}\NormalTok{ res.boot}\SpecialCharTok{$}\NormalTok{t0, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{05-contrastes_files/figure-latex/unnamed-chunk-12-1} \end{center}

Por ejemplo, para el contraste unilateral \(H_0: \rho \leq 0\)
(\texttt{alternative\ =\ "greater"}), obtendríamos:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pval.greater }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(res.boot}\SpecialCharTok{$}\NormalTok{t }\SpecialCharTok{\textgreater{}=}\NormalTok{ res.boot}\SpecialCharTok{$}\NormalTok{t0)}
\NormalTok{pval.greater}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.009
\end{verbatim}

Mientras que para realizar el contraste bilateral \(H_0: \rho = 0\)
(\texttt{alternative\ =\ "two.sided"}), sin asumir que
la distribución del estadístico de contraste es simétrica:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pval.less }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(res.boot}\SpecialCharTok{$}\NormalTok{t }\SpecialCharTok{\textless{}=}\NormalTok{ res.boot}\SpecialCharTok{$}\NormalTok{t0)}
\NormalTok{pval }\OtherTok{\textless{}{-}} \DecValTok{2}\SpecialCharTok{*}\FunctionTok{min}\NormalTok{(pval.less, pval.greater)}
\NormalTok{pval}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.018
\end{verbatim}

\hypertarget{contrastes-semiparametricos}{%
\section{Contrastes bootstrap semiparamétricos}\label{contrastes-semiparametricos}}

Este tipo de aproximación se emplearía en el caso de que la hipótesis nula
(o la alternativa) especifique un modelo semiparamétrico,
con una componente paramétrica y otra no paramétrica.
Típicamente se incluye el error en la componente no paramétrica, y podríamos emplear
el bootstrap residual (también denominado semiparamétrico o basado en modelos)
descrito en la Sección \ref{boot-residual}.

En esta sección consideraremos como ejemplo el conjunto de datos \texttt{Prestige}
del paquete \texttt{carData}, considerando como variable respuesta \texttt{prestige}
(puntuación de ocupaciones obtenidas a partir de una encuesta)
y como variables explicativas: \texttt{income} (media de ingresos en la ocupación)
y \texttt{education} (media de los años de educación).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(Prestige, }\AttributeTok{package =} \StringTok{"carData"}\NormalTok{)}
\CommentTok{\# ?Prestige}
\end{Highlighting}
\end{Shaded}

\hypertarget{ejemplo-inferencia-sobre-modelos-de-regresiuxf3n}{%
\subsection{Ejemplo: Inferencia sobre modelos de regresión}\label{ejemplo-inferencia-sobre-modelos-de-regresiuxf3n}}

En la mayoría de los casos nos interesa contrastar un \textbf{modelo reducido}
frente a un \textbf{modelo completo} (que generaliza el modelo reducido).
Por ejemplo, en el caso de modelos lineales (estimados por mínimos cuadrados)
se dispone del test \(F\) para realizar los contrastes de este tipo,
que emplea el estadístico:
\[F=\frac{n - q}{q - q_0}\frac{RSS_0 - RSS}{RSS},\]
siendo \(n\) el número de observaciones, \(RSS\) y \(q\) la suma de cuadrados residual y
el número de parámetros distintos del modelo completo,
y \(RSS_0\) y \(q_0\) los correspondientes al modelo reducido.
Este estadístico sigue una distribución \(\mathcal{F}_{q - q_0, n - q}\)
bajo \(H_0\) y las hipótesis habituales del modelo lineal
(\(\varepsilon_i\) i.i.d. \(\mathcal{N}(0, \sigma^2)\)).

El contraste de regresión sería un caso particular. Por ejemplo,
para contrastar si \texttt{income} y \texttt{education} influyen linealmente en \texttt{prestige}
podemos emplear el siguiente código:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{modelo }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(prestige }\SpecialCharTok{\textasciitilde{}}\NormalTok{ income }\SpecialCharTok{+}\NormalTok{ education, }\AttributeTok{data =}\NormalTok{ Prestige)}
\FunctionTok{summary}\NormalTok{(modelo)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = prestige ~ income + education, data = Prestige)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -19.4040  -5.3308   0.0154   4.9803  17.6889 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(>|t|)    
## (Intercept) -6.8477787  3.2189771  -2.127   0.0359 *  
## income       0.0013612  0.0002242   6.071 2.36e-08 ***
## education    4.1374444  0.3489120  11.858  < 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 7.81 on 99 degrees of freedom
## Multiple R-squared:  0.798,  Adjusted R-squared:  0.7939 
## F-statistic: 195.6 on 2 and 99 DF,  p-value: < 2.2e-16
\end{verbatim}

También podemos obtener el valor observado del estadístico \(F\)
a partir de los resultados del método \texttt{summary.lm()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res }\OtherTok{\textless{}{-}} \FunctionTok{summary}\NormalTok{(modelo)}
\CommentTok{\# names(res)}
\NormalTok{stat }\OtherTok{\textless{}{-}}\NormalTok{ res}\SpecialCharTok{$}\NormalTok{fstatistic[}\DecValTok{1}\NormalTok{]}
\NormalTok{df }\OtherTok{\textless{}{-}}\NormalTok{ res}\SpecialCharTok{$}\NormalTok{fstatistic[}\DecValTok{2}\NormalTok{]}
\NormalTok{dfr }\OtherTok{\textless{}{-}}\NormalTok{ res}\SpecialCharTok{$}\NormalTok{fstatistic[}\DecValTok{3}\NormalTok{]}
\NormalTok{res}\SpecialCharTok{$}\NormalTok{fstatistic}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    value    numdf    dendf 
## 195.5505   2.0000  99.0000
\end{verbatim}

o haciendo los cálculos a mano:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(Prestige)}
\NormalTok{q }\OtherTok{\textless{}{-}} \DecValTok{3}
\NormalTok{q0 }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{rss0 }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(Prestige, }\FunctionTok{sum}\NormalTok{((prestige }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(prestige))}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{))}
\NormalTok{rss }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(}\FunctionTok{residuals}\NormalTok{(modelo)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{inc.mse }\OtherTok{\textless{}{-}}\NormalTok{ (rss0 }\SpecialCharTok{{-}}\NormalTok{ rss)}\SpecialCharTok{/}\NormalTok{(q }\SpecialCharTok{{-}}\NormalTok{ q0)  }\CommentTok{\# Incremento en varibilidad explicada}
\NormalTok{msr }\OtherTok{\textless{}{-}}\NormalTok{  rss}\SpecialCharTok{/}\NormalTok{(n }\SpecialCharTok{{-}}\NormalTok{ q)               }\CommentTok{\# Variabilidad residual}
\NormalTok{inc.mse}\SpecialCharTok{/}\NormalTok{msr}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 195.5505
\end{verbatim}

Desde el punto de vista de comparación de modelos, el modelo
reducido bajo la hipótesis nula es:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{modelo0 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(prestige }\SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }\AttributeTok{data =}\NormalTok{ Prestige)}
\end{Highlighting}
\end{Shaded}

y podemos realizar el contraste mediante la función \texttt{anova()}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(modelo0, modelo)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Analysis of Variance Table
## 
## Model 1: prestige ~ 1
## Model 2: prestige ~ income + education
##   Res.Df     RSS Df Sum of Sq      F    Pr(>F)    
## 1    101 29895.4                                  
## 2     99  6038.9  2     23857 195.55 < 2.2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

Para aproximar la distribución de este estadístico bajo \(H_0\) podríamos adaptar
el bootstrap semiparamétrico\footnote{En este caso también podríamos emplear un contraste
  de permutaciones.} descrito en la Sección \ref{boot-residual}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(boot)}

\NormalTok{pres.dat }\OtherTok{\textless{}{-}}\NormalTok{ Prestige}
\CommentTok{\# pres.dat$fit0 \textless{}{-} mean(Prestige$prestige)}
\CommentTok{\# pres.dat$fit0 \textless{}{-} predict(modelo0)}
\NormalTok{pres.dat}\SpecialCharTok{$}\NormalTok{res0 }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(Prestige, prestige }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(prestige))}
\CommentTok{\# pres.dat$res0 \textless{}{-} residuals(modelo0)}

\NormalTok{mod.stat }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, i) \{}
\NormalTok{    data}\SpecialCharTok{$}\NormalTok{prestige }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{prestige) }\SpecialCharTok{+}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{res0[i]}
\NormalTok{    fit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(prestige }\SpecialCharTok{\textasciitilde{}}\NormalTok{ income }\SpecialCharTok{+}\NormalTok{ education, }\AttributeTok{data =}\NormalTok{ data)}
    \FunctionTok{summary}\NormalTok{(fit)}\SpecialCharTok{$}\NormalTok{fstatistic[}\DecValTok{1}\NormalTok{]}
\NormalTok{\}}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{boot.mod }\OtherTok{\textless{}{-}} \FunctionTok{boot}\NormalTok{(pres.dat, mod.stat, }\AttributeTok{R =} \DecValTok{1000}\NormalTok{)}
\NormalTok{boot.mod}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## ORDINARY NONPARAMETRIC BOOTSTRAP
## 
## 
## Call:
## boot(data = pres.dat, statistic = mod.stat, R = 1000)
## 
## 
## Bootstrap Statistics :
##     original    bias    std. error
## t1* 195.5505 -194.4866    1.096335
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(boot.mod}\SpecialCharTok{$}\NormalTok{t, }\AttributeTok{breaks =} \StringTok{"FD"}\NormalTok{, }\AttributeTok{freq =} \ConstantTok{FALSE}\NormalTok{)}
\FunctionTok{curve}\NormalTok{(}\FunctionTok{pf}\NormalTok{(x, df, dfr, }\AttributeTok{lower.tail =} \ConstantTok{FALSE}\NormalTok{), }\AttributeTok{lty =} \DecValTok{2}\NormalTok{, }\AttributeTok{add =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{05-contrastes_files/figure-latex/unnamed-chunk-21-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# pval \textless{}{-} mean(boot.mod$t \textgreater{}= boot.mod$t0)}
\NormalTok{pval }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(boot.mod}\SpecialCharTok{$}\NormalTok{t }\SpecialCharTok{\textgreater{}=}\NormalTok{ stat)}
\NormalTok{pval}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0
\end{verbatim}

Procediendo de esta forma sin embargo estaríamos sobreestimando la variabilidad
del error cuando la hipótesis nula es falsa (la variabilidad no explicada por la
tendencia es asumida por el error), lo que disminuirá la potencia del contraste.
Para mejorar la potencia, siguiendo la idea propuesta por González-Manteiga
y Cao (1993), se pueden remuestrear los residuos del modelo completo.
De esta forma reproduciríamos la variabilidad del error de forma consistente
tanto bajo la hipótesis alternativa como bajo la nula.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{old.par }\OtherTok{\textless{}{-}} \FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{hist}\NormalTok{(}\FunctionTok{residuals}\NormalTok{(modelo0), }\AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{50}\NormalTok{, }\DecValTok{50}\NormalTok{), }
     \AttributeTok{main =} \StringTok{\textquotesingle{}Variabilidad residual}\SpecialCharTok{\textbackslash{}n}\StringTok{ con el modelo reducido\textquotesingle{}}\NormalTok{)}
\FunctionTok{hist}\NormalTok{(}\FunctionTok{residuals}\NormalTok{(modelo), }\AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{50}\NormalTok{, }\DecValTok{50}\NormalTok{), }
     \AttributeTok{main =} \StringTok{\textquotesingle{}Variabilidad residual}\SpecialCharTok{\textbackslash{}n}\StringTok{ con el modelo completo\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{05-contrastes_files/figure-latex/unnamed-chunk-22-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(old.par)}
\end{Highlighting}
\end{Shaded}

Adicionalmente, como se mostró en la Sección \ref{boot-residual}, se puede emplear
la modificación propuesta en Davison y Hinkley (1997, Alg. 6.3, p.~271)
y remuestrear los residuos reescalados y centrados.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pres.dat }\OtherTok{\textless{}{-}}\NormalTok{ Prestige}
\CommentTok{\# pres.dat$fit0 \textless{}{-} mean(Prestige$prestige)}
\CommentTok{\# pres.dat$fit0 \textless{}{-} predict(modelo0)}
\CommentTok{\# pres.dat$res \textless{}{-} residuals(modelo)}
\NormalTok{pres.dat}\SpecialCharTok{$}\NormalTok{sres }\OtherTok{\textless{}{-}} \FunctionTok{residuals}\NormalTok{(modelo)}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{hatvalues}\NormalTok{(modelo))}
\NormalTok{pres.dat}\SpecialCharTok{$}\NormalTok{sres }\OtherTok{\textless{}{-}}\NormalTok{ pres.dat}\SpecialCharTok{$}\NormalTok{sres }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(pres.dat}\SpecialCharTok{$}\NormalTok{sres)}

\NormalTok{mod.stat }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, i) \{}
    \CommentTok{\# data$prestige \textless{}{-} mean(data$prestige) + data$res[i]}
\NormalTok{    data}\SpecialCharTok{$}\NormalTok{prestige }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{prestige) }\SpecialCharTok{+}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{sres[i]}
\NormalTok{    fit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(prestige }\SpecialCharTok{\textasciitilde{}}\NormalTok{ income }\SpecialCharTok{+}\NormalTok{ education, }\AttributeTok{data =}\NormalTok{ data)}
    \FunctionTok{summary}\NormalTok{(fit)}\SpecialCharTok{$}\NormalTok{fstatistic[}\DecValTok{1}\NormalTok{]}
\NormalTok{\}}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{boot.mod }\OtherTok{\textless{}{-}} \FunctionTok{boot}\NormalTok{(pres.dat, mod.stat, }\AttributeTok{R =} \DecValTok{1000}\NormalTok{)}
\NormalTok{boot.mod}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## ORDINARY NONPARAMETRIC BOOTSTRAP
## 
## 
## Call:
## boot(data = pres.dat, statistic = mod.stat, R = 1000)
## 
## 
## Bootstrap Statistics :
##       original   bias    std. error
## t1* 0.01164396 1.029746    1.029715
\end{verbatim}

En la aproximación del \(p\)-valor hay que tener en cuenta que al modificar los residuos
\texttt{boot.mod\$t0} no va a coincidir con el valor observado del estadístico,
almacenado en \texttt{stat} (por tanto habría que ignorar \texttt{original} y \texttt{bias}
en \texttt{Bootstrap\ Statistics};
la función \texttt{Boot()} del paquete \texttt{car} corrige este problema).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(boot.mod}\SpecialCharTok{$}\NormalTok{t, }\AttributeTok{breaks =} \StringTok{"FD"}\NormalTok{, }\AttributeTok{freq =} \ConstantTok{FALSE}\NormalTok{)}
\FunctionTok{curve}\NormalTok{(}\FunctionTok{pf}\NormalTok{(x, df, dfr, }\AttributeTok{lower.tail =} \ConstantTok{FALSE}\NormalTok{), }\AttributeTok{lty =} \DecValTok{2}\NormalTok{, }\AttributeTok{add =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{05-contrastes_files/figure-latex/unnamed-chunk-24-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pval }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(boot.mod}\SpecialCharTok{$}\NormalTok{t }\SpecialCharTok{\textgreater{}=}\NormalTok{ stat)}
\NormalTok{pval}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0
\end{verbatim}

En el caso de modelos no lineales (o otros tipos de modelos lineales) puede ser
complicado aproximar los grados de libertad para el cáculo del estadístico \(F\),
pero si empleamos bootstrap, vamos a obtener los mismos resultados considerando
como estadístico:
\[\tilde F =\frac{RSS_0 - RSS}{RSS},\]
que se puede interpretar también como una medida del incremento en la variabilidad residual
al considerar el modelo reducido (ya que únicamente difieren en una constante).
En este caso también se suelen emplear los residuos sin reescalar, ya que también puede ser
difícil encontrar la transformación adecuada.

\hypertarget{ejercicio}{%
\subsection{Ejercicio}\label{ejercicio}}

Al estudiar el efecto de las variables explicativas en el modelo
anterior, podríamos pensar que no es adecuado asumir un efecto lineal
de alguna de las variables explicativas. Por ejemplo, si generamos los gráficos
parciales de residuos obtendríamos:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# library(car)}
\FunctionTok{crPlots}\NormalTok{(modelo)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{05-contrastes_files/figure-latex/unnamed-chunk-25-1} \end{center}

En este caso podría ser razonable considerar un efecto cuadrático
de la variable \texttt{income}\footnote{Para ajustar un modelo polinómico
  puede ser recomendable, especialmente si el grado del polinomio es alto,
  emplear la función \texttt{poly()} ya que utiliza polinomios ortogonales.
  En el caso cuadrático, al emplear \texttt{y\ \textasciitilde{}\ x\ +\ I(x\^{}2)}
  estaremos considerando \(1, x, x^2\), mientras que \texttt{y\ \textasciitilde{}\ poly(x,\ 2)} considerará
  polinomios de Legendre de la forma \(1, x, \frac{1}{2}(3x^2-1)\).
  En este caso concreto, obtendríamos una parametrización equivalente
  empleando \texttt{modelo\ \textless{}-\ lm(prestige\ \textasciitilde{}\ poly(income,\ 2)\ +\ education,\ data\ =\ Prestige)}.}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{modelo }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(prestige }\SpecialCharTok{\textasciitilde{}}\NormalTok{ income }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(income}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ education, }\AttributeTok{data =}\NormalTok{ Prestige)}
\FunctionTok{summary}\NormalTok{(modelo)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = prestige ~ income + I(income^2) + education, data = Prestige)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -15.732  -4.900  -0.057   4.598  18.459 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(>|t|)    
## (Intercept) -1.135e+01  3.272e+00  -3.470 0.000775 ***
## income       3.294e-03  5.669e-04   5.810 7.79e-08 ***
## I(income^2) -7.967e-08  2.169e-08  -3.673 0.000390 ***
## education    3.809e+00  3.407e-01  11.179  < 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 7.36 on 98 degrees of freedom
## Multiple R-squared:  0.8224, Adjusted R-squared:  0.817 
## F-statistic: 151.3 on 3 and 98 DF,  p-value: < 2.2e-16
\end{verbatim}

Para comparar el ajuste de este modelo respecto al del anterior, podemos
realizar un contraste empleando la función \texttt{anova()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{modelo0 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(prestige }\SpecialCharTok{\textasciitilde{}}\NormalTok{ income }\SpecialCharTok{+}\NormalTok{ education, }\AttributeTok{data =}\NormalTok{ Prestige)}
\FunctionTok{anova}\NormalTok{(modelo0, modelo)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Analysis of Variance Table
## 
## Model 1: prestige ~ income + education
## Model 2: prestige ~ income + I(income^2) + education
##   Res.Df    RSS Df Sum of Sq      F    Pr(>F)    
## 1     99 6038.9                                  
## 2     98 5308.0  1     730.8 13.492 0.0003904 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

Contrastar si el efecto de \texttt{income} es lineal mediante bootstrap residual,
empleando como estadístico el incremento en la variabilidad residual con el
modelo reducido y remuestreando los residuos del modelo completo (sin reescalar).
Aproximar el nivel crítico del contraste y el valor que tendría que superar el
estadístico para rechazar \(H_0\) con un nivel de significación \(\alpha = 0.05\).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(boot)}

\CommentTok{\# set.seed(DNI)}
\CommentTok{\# ...}
\end{Highlighting}
\end{Shaded}

\hypertarget{npden}{%
\chapter{Bootstrap y estimación no paramétrica de la densidad}\label{npden}}

En este capítulo se presentarán diversos métodos bootstrap adecuados
para realizar inferencia en algunos problemas en el contexto de la
estimación no paramétrica de la densidad. Concretamente, se abordará el
problema de construcción de intervalos de confianza para la funciones de
densidad en un punto dado, así como la selección del parámetro de
suavizado para el estimador tipo núcleo de la función de densidad. A
continuación se incluye una breve introducción a estos métodos no
paramétricos de estimación de curvas.

\hypertarget{estimaciuxf3n-no-paramuxe9trica-de-la-funciuxf3n-de-densidad}{%
\section{Estimación no paramétrica de la función de densidad}\label{estimaciuxf3n-no-paramuxe9trica-de-la-funciuxf3n-de-densidad}}

Como ya se introdujo en la Sección \ref{modunif-boot-suav}, si
\(\left( X_1, X_2, \ldots, X_n \right)\) es una muestra aleatoria simple
(m.a.s.),, de una población con función de distribución \(F\), absolutamente
continua, y función de densidad \(f\), el estimador tipo núcleo propuesto por
Parzen (1962) y Rosenblatt (1956) viene dado por
\[\hat{f}_{h}\left( x \right) =\frac{1}{nh}\sum_{i=1}^{n}K\left( \frac{x-X_i}{
h} \right) =\frac{1}{n}\sum_{i=1}^{n}K_{h}\left( x-X_i \right),\]
donde \(K_{h}\left( u \right) =\frac{1}{h}K\left( \frac{u}{h} \right)\),
\(K\) es una función núcleo (normalmente una densidad simétrica en torno al cero)
y \(h>0\) es una parámetro de suavizado, llamado ventana, que regula el
tamaño del entorno que se usa para llevar a cabo la estimación. Es
habitual exigir que la función núcleo \(K\) sea no negativa y su integral
sea uno:
\[K\left( u \right) \geq 0,~\forall u,~\int_{-\infty }^{\infty }
K\left( u \right) du=1.\]
Además también es frecuente exigir que \(K\) sea una
función simétrica (\(K\left( -u \right) =K\left( u \right)\)).

\hypertarget{sesgo-varianza-y-error-cuadruxe1tico-medio}{%
\section{Sesgo, varianza y error cuadrático medio}\label{sesgo-varianza-y-error-cuadruxe1tico-medio}}

\hypertarget{sesgo}{%
\subsection{Sesgo}\label{sesgo}}

Mediante cálculos sencillos puede obtenerse el sesgo del estimador de
Parzen-Rosenblatt:
\[\begin{aligned}
Sesgo\left( \hat{f}_{h}\left( x \right) \right) &= E\left( \hat{f}_{h}\left(
x \right) \right) -f\left( x \right) =\int \frac{1}{h}K\left( \frac{x-y}{h}
 \right) f\left( y \right) dy-f\left( x \right) \\
&= \left( K_{h}\ast f \right) \left( x \right) -f\left( x \right),
\end{aligned}\]
siendo \(\ast\) el operador convolución:
\[\left( f\ast g \right) \left( x \right) 
= \int f\left( x-y \right) g\left( y \right) dy.\]
A partir de la expresión del sesgo puede obtenerse otra
asintótica para el mismo:
\[E\left( \hat{f}_{h}\left( x \right) \right) -f\left( x \right) =\frac{d_{K}}{2}
h^2f^{\prime \prime }\left( x \right) +O\left( h^{4} \right),\]con
\(d_{K}=\int t^2K\left( t \right) dt\).

\hypertarget{varianza}{%
\subsection{Varianza}\label{varianza}}

La varianza puede tratarse análogamente:
\[\begin{aligned}
Var\left( \hat{f}_{h}\left( x \right) \right) &= \frac{1}{nh^2}Var\left(
K\left( \frac{x-X_1}{h} \right) \right) \\
&= \frac{1}{nh^2}\left[ \int K\left( \frac{x-y}{h} \right)^2f\left(
y \right) dy-\left( \int K\left( \frac{x-y}{h} \right) f\left( y \right)
dy \right)^2\right] \\
&= \frac{1}{n}\left[ \left( \left( K_{h} \right)^2\ast f \right) \left(
x \right) -\left( \left( K_{h}\ast f \right) \left( x \right) \right)^2
\right] \\
&= \frac{1}{nh}\left[ \left( K^2 \right) _{h}\ast f\right] \left( x \right) -
\frac{1}{n}\left[ \left( K_{h}\ast f \right) \left( x \right) \right]^2.\end{aligned}\]Su
expresión asintótica resulta:
\[Var\left( \hat{f}_{h}\left( x \right) \right) =\frac{c_{K}}{nh}f\left(
x \right) - \frac{1}{n}f\left( x \right)^2 + O\left( \frac{h}{n} \right),\]
con \(c_{K}=\int K\left( t \right)^2dt\).

\hypertarget{error-cuadruxe1tico-medio}{%
\subsection{Error cuadrático medio}\label{error-cuadruxe1tico-medio}}

Como consecuencia el error cuadrático medio del estimador es:
\[\begin{aligned}
MSE\left( \hat{f}_{h}\left( x \right) \right) =&\ E\left( \hat{f}_{h}\left(
x \right) -f\left( x \right) \right)^2=Sesgo\left( \hat{f}_{h}\left(
x \right) \right)^2+Var\left( \hat{f}_{h}\left( x \right) \right) \\
=&\ \left[ \left( K_{h}\ast f \right) \left( x \right) -f\left( x \right) \right]
^2+\frac{1}{nh}\left[ \left( K^2 \right) _{h}\ast f\right] \left( x \right) \\
&-\frac{1}{n}\left[ \left( K_{h}\ast f \right) \left( x \right) \right]^2.
\end{aligned}\]
Además, su expresión asintótica
es:\[MSE\left( \hat{f}_{h}\left( x \right) \right) =\frac{d_{K}^2}{4}
h^{4}f^{\prime \prime }\left( x \right)^2+\frac{c_{K}}{nh}f\left( x \right)
-\frac{1}{n}f\left( x \right)^2+O\left( h^{6} \right) +O\left( \frac{h}{n}
 \right).\]

\hypertarget{error-cuadruxe1tico-medio-integrado-mise}{%
\subsection{Error cuadrático medio integrado (MISE)}\label{error-cuadruxe1tico-medio-integrado-mise}}

Una medida global (no para un \(x\) particular) del error cometido por el
estimador es el error cuadrático medio integrado:
\[\begin{aligned}
& & MISE\left( \hat{f}_{h}\left( x \right) \right) =\int E\left[ \left( \hat{f}
_{h}\left( x \right) -f\left( x \right) \right)^2\right] dx=\int MSE\left( 
\hat{f}_{h}\left( x \right) \right) dx= \\
&&\int \left[ \left( K_{h}\ast f \right) \left( x \right) -f\left( x \right) 
\right]^2dx+\frac{c_{K}}{nh}-\frac{1}{n}\int \left[ \left( K_{h}\ast
f \right) \left( x \right) \right]^2dx.
\end{aligned}\]
Una expresión asintótica para el mismo es la siguiente:
\[\begin{aligned}
MISE\left( \hat{f}_{h}\left( x \right) \right) =&\ \frac{d_{K}^2}{4}h^4\int
f^{\prime \prime }\left( x \right)^2dx+\frac{c_{K}}{nh}-\frac{1}{n}\int
f\left( x \right)^2dx \\
&+O\left( h^{6} \right) +O \left( \frac{h}{n} \right).
\end{aligned}\]
En ella se puede ver el efecto negativo de tomar ventanas (\(h\)) demasiado
grandes o demasiado pequeñas.

\hypertarget{aproximacion-bootstrap}{%
\section{Aproximación Bootstrap de la distribución del estimador de Parzen-Rosenblatt}\label{aproximacion-bootstrap}}

Antes de proceder a abordar el bootstrap en este contexto conviene
presentar la distribución asintótica del estimador y otras
aproximaciones posibles. Pueden encontrarse más detalles sobre estos
resultados en Cao (1990).

\hypertarget{distribuciuxf3n-asintuxf3tica-del-estimador-de-parzen-rosenblatt}{%
\subsection{Distribución asintótica del estimador de Parzen-Rosenblatt}\label{distribuciuxf3n-asintuxf3tica-del-estimador-de-parzen-rosenblatt}}

Las condiciones mínimas necesarias para que el sesgo y la varianza del
estimador tiendan a cero cuando el tamaño muestral tiende a infinito son \(h\rightarrow 0\), \(nh\rightarrow \infty\).
En tales circunstancias se tiene
\[\sqrt{nh}\left( \hat{f}_{h}\left( x \right) -f\left( x \right) \right) \overset{d}{\rightarrow} \mathcal{N}\left( B,V \right).\]
Además, puede probarse que el valor asintóticamente óptimo de \(h\),
en el sentido del \(MSE\), es \(h=c_{0}n^{-1/5}\), con
\[c_{0}=\left( \frac{c_{K}f\left( x \right)}{d_{K}^2f^{\prime \prime }\left(x \right)^2} \right)^{1/5}.\]

Con esa elección de \(h\) los valores de media y varianza de la
distribución normal límite son\[\begin{aligned}
B &= \frac{1}{2}c_{0}^{5/2}d_{K}f^{\prime \prime }\left( x \right), \\
V &= c_{K}f\left( x \right).\end{aligned}\]

Para utilizar la distribución asintótica anterior en la construcción de
intervalos de confianza para \(f\left( x \right)\) podemos

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Estimar \(B\) y \(V\) y utilizarlos en la correspondiente distribución
  normal (\textbf{metodo plug-in}).
\item
  Diseñar un plan de remuestreo y utilizar el \textbf{método bootstrap}.
\end{enumerate}

\hypertarget{aproximaciuxf3n-plug-in}{%
\subsection{Aproximación plug-in}\label{aproximaciuxf3n-plug-in}}

Pasa por estimar \(B\) y \(V\) mediante\[\begin{aligned}
\hat{B} &= \frac{1}{2}\hat{c}_{0}^{5/2}d_{K}\hat{f}_{g}^{\prime \prime
}\left( x \right), \\
\hat{V} &= c_{K}\hat{f}_{h}\left( x \right),\end{aligned}\]siendo \(g\)
una ventana adecuada para estimar la derivada segunda de la función de
densidad. Utilizando la desigualdad de Berry-Esséen se obtiene el
siguiente orden de
convergencia:\[\sup_{z\in \boldsymbol{R}}\left\vert P\left[ \sqrt{nh}\left( \hat{f}
_{h}\left( x \right) -f\left( x \right) \right) \leq z\right] -\Phi \left( 
\frac{z-\hat{B}}{\hat{V}^{1/2}} \right) \right\vert =O_{P}\left(
n^{-1/5} \right),\]

que empeora la tasa teórica de la aproximación normal basada en la media
y varianza exactas (\(B_n=E\left[ \sqrt{nh}\left( \hat{f}_{h}\left( x \right) -f\left( x \right) \right) \right]\) y
\(V_n=Var\left[ \sqrt{nh} \left( \hat{f}_{h}\left( x \right) -f\left( x \right) \right) \right]\)):
\[\sup_{z\in \boldsymbol{R}}\left\vert P\left[ \sqrt{nh}\left( \hat{f}
_{h}\left( x \right) -f\left( x \right) \right) \leq z\right] -\Phi \left( 
\frac{z-B_n}{V_n^{1/2}} \right) \right\vert =O\left( n^{-2/5} \right),\]
aunque no la de la normal asintótica, \(\mathcal{N}\left( B,V \right)\), cuya tasa es
igualmente de orden \(O_{P}\left( n^{-1/5} \right)\).

\hypertarget{aproximaciuxf3n-bootstrap}{%
\subsection{Aproximación bootstrap}\label{aproximaciuxf3n-bootstrap}}

Se procede según el siguiente plan de remuestreo.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  A partir de la muestra \(\left( X_1,X_2,\ldots ,X_n \right)\) y
  utilizando una \textbf{ventana piloto} \(g\), se calcula el estimador de
  Parzen-Rosenblatt \(\hat{f}_{g}\).
\item
  Se arrojan remuestras bootstrap \(\left( X_1^{\ast},X_2^{\ast },\ldots ,X_n^{\ast} \right)\) a partir de la densidad
  \(\hat{f}_{g}\).
\item
  Se construye el análogo bootstrap del estimador de Parzen-Rosenblatt
  \[\hat{f}_{h}^{\ast}\left( x \right) =\frac{1}{nh}\sum_{i=1}^{n}K\left( \frac{
  x-X_i^{\ast}}{h} \right).\]
\item
  Se aproxima la distribución en el muestreo de \(\sqrt{nh}\left( \hat{f}_{h}\left( x \right) -f\left( x \right) \right)\) por la
  distribución en el remuestreo de
  \(\sqrt{nh}\left( \hat{f}_{h}^{\ast}\left( x \right) - \hat{f}_{g}\left( x \right) \right)\).
\end{enumerate}

Si nuestro interés estuviese en el sesgo o la varianza de \(\hat{f} _{h}\left( x \right)\) entonces utilizaríamos, en el paso 4 del algoritmo
anterior, los análogos bootstrap del sesgo o la varianza:
\(E^{\ast}\left( \hat{f}_{h}^{\ast }\left( x \right) -\hat{f}_{g}\left( x \right) \right)\) o
\(Var^{\ast}\left( \hat{f}_{h}^{\ast}\left( x \right) \right)\).

En el algoritmo anterior, la ventana \(g\) ha de ser asintóticamente mayor
que \(h\). De hecho, una elección razonable para \(g\) es aquella que
minimiza \(E\left[ \left( \hat{f}_{g}^{\prime \prime }\left( x \right) -f^{\prime \prime }\left( x \right) \right)^2\right]\).
Asintóticamente esa ventana viene dada por
\[g\simeq \left( \frac{5f\left( x \right) \int K^{\prime \prime }\left( t
\right)^2dt}{d_{K}^2f^{\left( 4 \right)}\left( x \right)^2n} \right)^{1/9}.\]

El orden de convergencia de para la aproximación bootstrap viene dado por
\[\begin{aligned}
&\sup_{z\in \boldsymbol{R}}\left\vert P\left[ \sqrt{nh}\left( \hat{f}
_{h}\left( x \right) -f\left( x \right) \right) \leq z\right] -P^{\ast}\left[ 
\sqrt{nh}\left( \hat{f}_{h}^{\ast}\left( x \right) -\hat{f}_{g}\left(
x \right) \right) \leq z\right] \right\vert \\
&= O_{P}\left( n^{-2/9} \right),\end{aligned}\]que mejora los ofrecidos
por la aproximación normal teórica y el método plug-in.

\hypertarget{el-bootstrap-en-la-selecciuxf3n-del-paruxe1metro-de-suavizado.}{%
\section{El Bootstrap en la selección del parámetro de suavizado.}\label{el-bootstrap-en-la-selecciuxf3n-del-paruxe1metro-de-suavizado.}}

\hypertarget{expresiuxf3n-asintuxf3tica-de-la-ventana-uxf3ptima}{%
\subsection{Expresión asintótica de la ventana óptima}\label{expresiuxf3n-asintuxf3tica-de-la-ventana-uxf3ptima}}

El \(MISE\) tiene una expresión asintótica que puede usarse como criterio
para obtener un valor óptimo del parámetro de
suavizado:\[MISE\left( h \right) =AMISE\left( h \right) +O\left( h^{6} \right) +O\left( 
\frac{h}{n} \right),\]con\[AMISE\left( h \right) =\frac{d_{K}^2}{4}h^{4}\int f^{\prime \prime }\left(
x \right)^2dx+\frac{c_{K}}{nh}-\frac{1}{n}\int f\left( x \right)^2dx.\]El
parámetro de suavizado que minimiza el \(AMISE\)
es\[h_{AMISE}=\left( \frac{c_{K}}{nd_{K}^2\int f^{\prime \prime }\left(
x \right)^2dx} \right)^{1/5}.\]

Existen multitud de métodos encaminados a dar respuesta al problema de
selección del parámetro de suavizado. Entre ellos destacamos los métodos
plug-in, los de validación cruzada (suavizada o no) y, desde luego, los
métodos bootstrap (ver, por ejemplo, Marron (1992)).

\hypertarget{anuxe1logo-bootstrap-del-mise}{%
\subsection{\texorpdfstring{Análogo bootstrap del \(MISE\)}{Análogo bootstrap del MISE}}\label{anuxe1logo-bootstrap-del-mise}}

La idea básica (Cao (1993)) consiste en diseñar un plan de remuestreo,
del tipo bootstrap suavizado, para estimar el \(MISE\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  A partir de la muestra \(\left( X_1,X_2,\ldots ,X_n \right)\) y
  utilizando una ventana piloto \(g\), se calcula el estimador de
  Parzen-Rosenblatt \(\hat{f}_{g}\).
\item
  Se arrojan remuestras bootstrap \(\left( X_1^{\ast},X_2^{\ast },\ldots ,X_n^{\ast} \right)\) de la densidad \(\hat{f}_{g}\).
\item
  Para cada \(h>0\), se obtiene el análogo bootstrap del estimador de
  Parzen-Rosenblatt
  \[\hat{f}_{h}^{\ast}\left( x \right) =\frac{1}{nh}\sum_{i=1}^{n}K\left( \frac{
  x-X_i^{\ast}}{h} \right).\]
\item
  Se construye la versión bootstrap del
  \(MISE\):\[MISE^{\ast}\left( h \right) =\int E^{\ast}\left[ \left( \hat{f}_{h}^{\ast
  }\left( x \right) -\hat{f}_{g}\left( x \right) \right)^2\right] dx.\]
\item
  Se minimiza \(MISE^{\ast}\left( h \right)\) en \(h>0\) y se obtiene el
  selector bootstrap:
  \[h_{MISE}^{\ast}=\arg \min_{h>0}MISE^{\ast}\left( h \right)\]
\end{enumerate}

\hypertarget{expresiuxf3n-cerrada-para-miseast}{%
\subsection{\texorpdfstring{Expresión cerrada para \(MISE^{\ast}\)}{Expresión cerrada para MISE\^{}\{\textbackslash ast\}}}\label{expresiuxf3n-cerrada-para-miseast}}

A diferencia de lo que es habitual, en este contexto es posible obtener
una expresión cerrada para el análogo bootstrap del \(MISE\):
\[\begin{aligned}
MISE^{\ast}\left( h \right) =&\ \int \left[ \left( K_{h}\ast 
\hat{f}_{g} \right) \left( x \right) -\hat{f}_{g}\left( x \right) \right]^2dx \\
&+\frac{c_{K}}{nh}-\frac{1}{n}\int \left[ \left( K_{h}\ast 
\hat{f}_{g} \right) \left( x \right) \right]^2dx \\
=&\ \frac{c_{K}}{nh}-\frac{1}{n^{3}}\sum_{i,j=1}^{n}\left[ \left( K_{h}\ast
K_{g} \right) \ast \left( K_{h}\ast K_{g} \right) \right] \left(
X_i-X_j \right) \\
&+\frac{1}{n^2}\sum_{i,j=1}^{n}\left[ \left( K_{h}\ast K_{g}-K_{g} \right)
\ast \left( K_{h}\ast K_{g}-K_{g} \right) \right] \left( X_i-X_j \right).\end{aligned}\]

\hypertarget{elecciuxf3n-de-la-ventana-piloto}{%
\subsection{Elección de la ventana piloto}\label{elecciuxf3n-de-la-ventana-piloto}}

De nuevo ocurre que el problema de elección óptima de la ventana piloto,
\(g\), viene ligado al de estimación óptima de la curvatura de la función
de densidad. Así, una buena elección de \(g\) es la que
minimiza\[E\left[ \left( \int \hat{f}_{g}^{\prime \prime }\left( x \right)^2dx-\int
f^{\prime \prime }\left( x \right)^2dx \right)^2\right] .\]El valor
asintótico de dicha ventana \(g\)
es\[g_{0}=\left( \frac{\int K^{\prime \prime }\left( t \right)^2dt}{nd_{K}\int
f^{\left( 3 \right)}\left( x \right)^2dx} \right)^{1/7}.\]

\hypertarget{resultados-teuxf3ricos}{%
\subsection{Resultados teóricos}\label{resultados-teuxf3ricos}}

Utilizando cualquier ventana piloto determinística que cumpla
\(\frac{g-g_{0}}{g_{0}}=O\left( n^{-1/14} \right)\), se tiene
\[\begin{aligned}
\frac{h_{MISE}^{\ast}-h_{MISE}}{h_{MISE}} &= O_{P}\left( n^{-5/14} \right),\\
\frac{MISE\left( h_{MISE}^{\ast} \right) -MISE\left( h_{MISE} \right)}{
MISE\left( h_{MISE} \right)} &= O_{P}\left( n^{-5/7} \right).
\end{aligned}\]

Mediante técnicas más sofisticadas que permiten que \(g\) dependa de
\(h\) pueden obtenerse tasas ligeramente
mejores:\[\frac{h_{MISE}^{\ast}-h_{MISE}}{h_{MISE}}=O_{P}\left( n^{-1/2} \right).\]

\hypertarget{caso-particular-de-nuxfacleo-gaussiano}{%
\subsection{Caso particular de núcleo gaussiano}\label{caso-particular-de-nuxfacleo-gaussiano}}

Cuando el núcleo \(K\) es la función de densidad de una
\(\mathcal{N}\left( 0,1 \right)\):

\begin{itemize}
\item
  \(K_{h}\) es la densidad de una \(\mathcal{N}\left( 0,h^2 \right)\)
\item
  \(K_{g}\) es la densidad de una \(\mathcal{N}\left( 0,g^2 \right)\)
\item
  \(K_{h}\ast K_{g}\) es la densidad de una
  \(\mathcal{N}\left( 0,h^2+g^2 \right)\)
\item
  \(\left( K_{h}\ast K_{g} \right) \ast \left( K_{h}\ast K_{g} \right)\)
  es la densidad de una \(\mathcal{N}\left( 0,2h^2+2g^2 \right)\)
\item
  \(\left( K_{h}\ast K_{g} \right) \ast K_{g}\) es la densidad de una
  \(\mathcal{N}\left( 0,h^2+2g^2 \right)\)
\item
  \(K_{g}\ast K_{g}\) es la densidad de una \(\mathcal{N}\left( 0,2g^2 \right)\)
\end{itemize}

con lo cual
\[\begin{aligned}
MISE^{\ast}\left( h \right) =&\ \frac{c_{K}}{nh}-\frac{1}{n^{3}}
\sum_{i,j=1}^{n}K_{\sqrt{2h^2+2g^2}}\left( X_i-X_j \right) \\
&+\frac{1}{n^2}\sum_{i,j=1}^{n}\left[ K_{\sqrt{2h^2+2g^2}}\left(
X_i-X_j \right) \right. \\
&\left. -2K_{\sqrt{h^2+2g^2}}\left( X_i-X_j \right) +K_{\sqrt{2g^2}
}\left( X_i-X_j \right) \right] .
\end{aligned}\]

\hypertarget{comparaciuxf3n-con-otros-selectores}{%
\subsection{Comparación con otros selectores}\label{comparaciuxf3n-con-otros-selectores}}

El método bootstrap presentado es muy semejante al de validación cruzada
suavizada (SCV) propuesto por Hall, Marron y Park (1992). En estudios de
simulación comparativos (ver Cao, Cuevas y González-Manteiga (1993))
puede verse como este método ofrece resultados muy competitivos con
otros métodos de selección del parámetro de suavizado. En general es el
que mejor comportamiento ofrece junto con el método plug-in tipo
solve-the-equation de Sheather y Jones (1991) y el método SCV.

Otros selectores bootstrap con mucho peor comportamiento son:

\begin{itemize}
\item
  Hall (1990), en el que se remuestrea de la distribución empírica,
  con lo cual no se imita el sesgo.
\item
  Faraway y Jhun (1990), que eligen \(g\) como la ventana de validación
  cruzada, que resulta ser demasiado pequeña.
\item
  Taylor (1989), que elige \(g=h\) , con lo cual \(MISE^{\ast}\left( h \right) \rightarrow 0\), cuando \(h\rightarrow \infty\), lo cual
  produce un mínimo global de \(MISE^{\ast}\) inconsistente con
  \(h_{MISE}\).
\end{itemize}

\hypertarget{npden-r}{%
\section{Estimación no paramétrica de la densidad en R}\label{npden-r}}

Como ya se comentó en la Sección \ref{modunif-boot-suav},
en \texttt{R} podemos emplear la función \texttt{density()} del paquete base para obtener
una estimación tipo núcleo de la densidad.
Los principales parámetros (con los valores por defecto) son los siguientes:

\begin{verbatim}
density(x, bw = "nrd0", adjust = 1, kernel = "gaussian", n = 512, from, to)
\end{verbatim}

\begin{itemize}
\item
  \texttt{bw}: ventana, puede ser un valor numérico o una cadena de texto que la determine
  (en ese caso llamará internamente a la función \texttt{bw.xxx()} donde \texttt{xxx} se corresponde
  con la cadena de texto). Las opciones son:

  \begin{itemize}
  \item
    \texttt{"nrd0"}, \texttt{"nrd"}: Reglas del pulgar de Silverman (1986, page 48, eqn (3.31)) y
    Scott (1992), respectivamente. Como es de esperar que la densidad objetivo
    no sea tan suave como la normal, estos criterios tenderán a seleccionar
    ventanas que producen un sobresuavizado de las observaciones.
  \item
    \texttt{"ucv"}, \texttt{"bcv"}: Métodos de validación cruzada insesgada y sesgada, respectivamente.
  \item
    \texttt{"sj"}, \texttt{"sj-ste"}, \texttt{"sj-dpi"}: Métodos de Sheather y Jones (1991),
    ``solve-the-equation'' y ``direct plug-in'', respectivamente.
  \end{itemize}
\item
  \texttt{adjust}: parameto para reescalado de la ventana, las estimaciones se calculan
  con la ventana \texttt{adjust*bw}.
\item
  \texttt{kernel}: cadena de texto que determina la función núcleo, las opciones son: \texttt{"gaussian"},
  \texttt{"epanechnikov"}, \texttt{"rectangular"}, \texttt{"triangular"}, \texttt{"biweight"}, \texttt{"cosine"} y \texttt{"optcosine"}.
\item
  \texttt{n}, \texttt{from}, \texttt{to}: permiten establecer la rejilla en la que se obtendrán las estimaciones
  (si \(n>512\) se emplea \texttt{fft()} por lo que se recomienda establecer \texttt{n} a un múltiplo de 2;
  por defecto \texttt{from} y \texttt{to} se establecen como \texttt{cut\ =\ 3} veces la ventana desde los extremos
  de las observaciones).
\end{itemize}

Utilizaremos como punto de partida el código empleado en la Sección \ref{modunif-boot-suav}.
Considerando el conjunto de datos \texttt{precip} (que contiene el promedio de precipitación,
en pulgadas de lluvia, de 70 ciudades de Estados Unidos).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}}\NormalTok{ precip}
\NormalTok{h }\OtherTok{\textless{}{-}} \FunctionTok{bw.SJ}\NormalTok{(x)}
\NormalTok{npden }\OtherTok{\textless{}{-}} \FunctionTok{density}\NormalTok{(x, }\AttributeTok{bw =}\NormalTok{ h)}
\CommentTok{\# npden \textless{}{-} density(x, bw = "SJ")}

\CommentTok{\# plot(npden)}
\FunctionTok{hist}\NormalTok{(x, }\AttributeTok{freq =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{main =} \StringTok{"Kernel density estimation"}\NormalTok{,}
     \AttributeTok{xlab =} \FunctionTok{paste}\NormalTok{(}\StringTok{"Bandwidth ="}\NormalTok{, }\FunctionTok{formatC}\NormalTok{(h)), }\AttributeTok{lty =} \DecValTok{2}\NormalTok{,}
     \AttributeTok{border =} \StringTok{"darkgray"}\NormalTok{, }\AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{80}\NormalTok{), }\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.04}\NormalTok{))}
\FunctionTok{lines}\NormalTok{(npden, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\FunctionTok{rug}\NormalTok{(x, }\AttributeTok{col =} \StringTok{"darkgray"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{06-npden_files/figure-latex/unnamed-chunk-2-1} \end{center}

Alternativamente podríamos emplear implementaciones en otros paquetes de \texttt{R}.
Uno de los más empleados es \texttt{ks} (Duong, 2019), que admite estimación
incondicional y condicional multidimensional.
También se podrían emplear los paquetes \texttt{KernSmooth} (Wand y Ripley, 2019),
\texttt{sm} (Bowman y Azzalini, 2019), \texttt{np} (Tristen y Jeffrey, 2019),
\texttt{kedd} (Guidoum, 2019), \texttt{features} (Duong y Matt, 2019) y \texttt{npsp} (Fernández-Casal, 2019),
entre otros.

\hypertarget{ejemplos-1}{%
\section{Ejemplos}\label{ejemplos-1}}

En esta sección nos centraremos en el bootstrap en la estimación tipo núcleo de la densidad para la aproximación de la precisión y el sesgo, y también para el cálculo de intervalos de confianza.

\hypertarget{bootstrap-y-estimaciuxf3n-del-sesgo}{%
\subsection{Bootstrap y estimación del sesgo}\label{bootstrap-y-estimaciuxf3n-del-sesgo}}

La idea sería aproximar la distribución del error de estimación \(\hat f_h(x) - f(x)\) por la distribución bootstrap de \(\hat f^{\ast}_h(x) - \hat f_g(x)\) (bootstrap percentil básico).

Como se comentó en la Sección \ref{aproximacion-bootstrap} la ventana \(g\) debería ser asintóticamente mayor que \(h\) (de orden \(n^{-1/5}\)) y la recomendación sería emplear la ventana óptima para la estimación de \(f^{\prime \prime }\left( x \right)\), de orden \(n^{-1/9}\).
Sin embargo, en la práctica es habitual emplear \(g=h\) para evitar la selección de esta ventana (lo que además facilita emplear herramientas como el paquete \texttt{boot}).
Otra alternativa podría ser asumir que \(g \simeq n^{1/5}h/n^{1/9}\) como se hace a continuación.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Remuestreo}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{n }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(x)}
\NormalTok{g }\OtherTok{\textless{}{-}}\NormalTok{ h }\SpecialCharTok{*}\NormalTok{ n}\SpecialCharTok{\^{}}\NormalTok{(}\DecValTok{4}\SpecialCharTok{/}\DecValTok{45}\NormalTok{) }\CommentTok{\# h*n\^{}({-}1/9)/n\^{}({-}1/5)}
\CommentTok{\# g \textless{}{-} h}
\NormalTok{range\_x }\OtherTok{\textless{}{-}} \FunctionTok{range}\NormalTok{(npden}\SpecialCharTok{$}\NormalTok{x) }\CommentTok{\# Para fijar las posiciones de estimación}
\NormalTok{B }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{stat\_den\_boot }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\AttributeTok{nrow =} \FunctionTok{length}\NormalTok{(npden}\SpecialCharTok{$}\NormalTok{x), }\AttributeTok{ncol =}\NormalTok{ B)}
\NormalTok{npdeng }\OtherTok{\textless{}{-}} \FunctionTok{density}\NormalTok{(x, }\AttributeTok{bw =}\NormalTok{ g, }\AttributeTok{from =}\NormalTok{ range\_x[}\DecValTok{1}\NormalTok{], }\AttributeTok{to =}\NormalTok{ range\_x[}\DecValTok{2}\NormalTok{])}

\ControlFlowTok{for}\NormalTok{ (k }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{B) \{}
    \CommentTok{\# x\_boot \textless{}{-} sample(x, n, replace = TRUE) + rnorm(n, 0, g)}
\NormalTok{    x\_boot }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n, }\FunctionTok{sample}\NormalTok{(x, n, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{), g)}
\NormalTok{    den\_boot }\OtherTok{\textless{}{-}} \FunctionTok{density}\NormalTok{(x\_boot, }\AttributeTok{bw =}\NormalTok{ h, }\AttributeTok{from =}\NormalTok{ range\_x[}\DecValTok{1}\NormalTok{], }\AttributeTok{to =}\NormalTok{ range\_x[}\DecValTok{2}\NormalTok{])}\SpecialCharTok{$}\NormalTok{y}
    \CommentTok{\# Si se quiere tener en cuenta la variabilidad debida a la selección de}
    \CommentTok{\# la ventana habría que emplear el mismo criterio en la función \textasciigrave{}density\textasciigrave{}.}
\NormalTok{    stat\_den\_boot[, k] }\OtherTok{\textless{}{-}}\NormalTok{ den\_boot }\SpecialCharTok{{-}}\NormalTok{ npdeng}\SpecialCharTok{$}\NormalTok{y}
\NormalTok{\}}

\CommentTok{\# Calculo del sesgo y error estándar }
\NormalTok{bias }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(stat\_den\_boot, }\DecValTok{1}\NormalTok{, mean)}
\NormalTok{std\_err }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(stat\_den\_boot, }\DecValTok{1}\NormalTok{, sd)}

\CommentTok{\# Representar estimación y corrección de sesgo bootstrap}
\FunctionTok{plot}\NormalTok{(npden, }\AttributeTok{type=}\StringTok{"l"}\NormalTok{, }\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.05}\NormalTok{), }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\CommentTok{\# lines(npden$x, pmax(npden$y {-} bias, 0))}
\FunctionTok{lines}\NormalTok{(npden}\SpecialCharTok{$}\NormalTok{x, npden}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{{-}}\NormalTok{ bias)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{06-npden_files/figure-latex/unnamed-chunk-3-1} \end{center}

\hypertarget{npden-r-ic}{%
\subsection{Estimación por intervalos de confianza}\label{npden-r-ic}}

Empleando la aproximación descrita en la Sección \ref{icboot-basic}
podemos cálcular de estimaciones por intervalo de confianza (puntuales)
por el método percentil (básico).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{alfa }\OtherTok{\textless{}{-}} \FloatTok{0.05}
\NormalTok{pto\_crit }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(stat\_den\_boot, }\DecValTok{1}\NormalTok{, quantile, }\AttributeTok{probs =} \FunctionTok{c}\NormalTok{(alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{))}
\CommentTok{\# ic\_inf\_boot \textless{}{-} npden$y {-} pto\_crit[2, ]}
\NormalTok{ic\_inf\_boot }\OtherTok{\textless{}{-}} \FunctionTok{pmax}\NormalTok{(npden}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{{-}}\NormalTok{ pto\_crit[}\DecValTok{2}\NormalTok{, ], }\DecValTok{0}\NormalTok{)}
\NormalTok{ic\_sup\_boot }\OtherTok{\textless{}{-}}\NormalTok{ npden}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{{-}}\NormalTok{ pto\_crit[}\DecValTok{1}\NormalTok{, ]}

\FunctionTok{plot}\NormalTok{(npden, }\AttributeTok{type=}\StringTok{"l"}\NormalTok{, }\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.05}\NormalTok{), }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(npden}\SpecialCharTok{$}\NormalTok{x, }\FunctionTok{pmax}\NormalTok{(npden}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{{-}}\NormalTok{ bias, }\DecValTok{0}\NormalTok{)) }\CommentTok{\# Ojo: no es una densidad}
\FunctionTok{lines}\NormalTok{(npden}\SpecialCharTok{$}\NormalTok{x, ic\_inf\_boot, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(npden}\SpecialCharTok{$}\NormalTok{x, ic\_sup\_boot, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{06-npden_files/figure-latex/unnamed-chunk-4-1} \end{center}

\hypertarget{npden-r-boot}{%
\subsection{\texorpdfstring{Implementación con el paquete \texttt{boot}}{Implementación con el paquete boot}}\label{npden-r-boot}}

Como también se comentó en la Sección \ref{modunif-boot-suav},
la recomendación es implementar el bootstrap suavizado como un bootstrap paramétrico:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(boot)}

\CommentTok{\# Los objetos necesarios para el cálculo del estadístico}
\CommentTok{\# hay que pasarlos a traves del argumento \textasciigrave{}data\textasciigrave{} de \textasciigrave{}boot\textasciigrave{}.}
\CommentTok{\# range\_x \textless{}{-} range(npden$x)}
\NormalTok{data.precip }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{h =}\NormalTok{ h, }\AttributeTok{range\_x =}\NormalTok{ range\_x)}

\NormalTok{ran.gen.smooth }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, mle) \{}
    \CommentTok{\# Función para generar muestras aleatorias mediante}
    \CommentTok{\# bootstrap suavizado con función núcleo gaussiana,}
    \CommentTok{\# mle contendrá la ventana}
\NormalTok{    n }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{x)}
\NormalTok{    g }\OtherTok{\textless{}{-}}\NormalTok{ mle}
\NormalTok{    xboot }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n, }\FunctionTok{sample}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{x, n, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{), g)}
\NormalTok{    out }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{x =}\NormalTok{ xboot, }\AttributeTok{h =}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{h, }\AttributeTok{range\_x =}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{range\_x)}
\NormalTok{\}}

\NormalTok{statistic }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data) }
                \FunctionTok{density}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{x, }\AttributeTok{bw =}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{h, }\AttributeTok{from =}\NormalTok{ range\_x[}\DecValTok{1}\NormalTok{], }\AttributeTok{to =}\NormalTok{ range\_x[}\DecValTok{2}\NormalTok{])}\SpecialCharTok{$}\NormalTok{y}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{res.boot }\OtherTok{\textless{}{-}} \FunctionTok{boot}\NormalTok{(data.precip, statistic, }\AttributeTok{R =}\NormalTok{ B, }\AttributeTok{sim =} \StringTok{"parametric"}\NormalTok{,}
                 \AttributeTok{ran.gen =}\NormalTok{ ran.gen.smooth, }\AttributeTok{mle =}\NormalTok{ g)}

\CommentTok{\# Calculo del sesgo y error estándar}
\CommentTok{\# Empleamos npdeng$y en lugar de resboot$t0}
\NormalTok{bias }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(res.boot}\SpecialCharTok{$}\NormalTok{t, }\DecValTok{2}\NormalTok{, mean, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{{-}}\NormalTok{  npdeng}\SpecialCharTok{$}\NormalTok{y}
\NormalTok{std\_err }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(res.boot}\SpecialCharTok{$}\NormalTok{t, }\DecValTok{2}\NormalTok{, sd, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Además, la función \texttt{boot.ci()} solo permite el cálculo del intervalo de
confianza para cada valor de \(x\) de forma independiente (parámetro \texttt{index}).
Por lo que podría ser recomendable obtenerlo a partir de las réplicas
bootstrap del estimador:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Método percentil básico calculado directamente }
\CommentTok{\# a partir de las réplicas bootstrap del estimador}
\NormalTok{alfa }\OtherTok{\textless{}{-}} \FloatTok{0.05}
\NormalTok{pto\_crit }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(res.boot}\SpecialCharTok{$}\NormalTok{t, }\DecValTok{2}\NormalTok{, quantile, }\AttributeTok{probs =} \FunctionTok{c}\NormalTok{(alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{))}
\NormalTok{ic\_inf\_boot }\OtherTok{\textless{}{-}} \FunctionTok{pmax}\NormalTok{(npden}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{+}\NormalTok{ npdeng}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{{-}}\NormalTok{ pto\_crit[}\DecValTok{2}\NormalTok{, ], }\DecValTok{0}\NormalTok{)}
\NormalTok{ic\_sup\_boot }\OtherTok{\textless{}{-}}\NormalTok{ npden}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{+}\NormalTok{ npdeng}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{{-}}\NormalTok{ pto\_crit[}\DecValTok{1}\NormalTok{, ]}

\FunctionTok{plot}\NormalTok{(npden, }\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.05}\NormalTok{), }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(npden}\SpecialCharTok{$}\NormalTok{x, }\FunctionTok{pmax}\NormalTok{(npden}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{{-}}\NormalTok{ bias, }\DecValTok{0}\NormalTok{))}

\FunctionTok{lines}\NormalTok{(npden}\SpecialCharTok{$}\NormalTok{x, ic\_inf\_boot, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(npden}\SpecialCharTok{$}\NormalTok{x, ic\_sup\_boot, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{06-npden_files/figure-latex/unnamed-chunk-6-1} \end{center}

En la práctica, en muchas ocasiones se trabaja directamente con
las réplicas bootstrap del estimador. Por ejemplo, es habitual
generar envolventes como medida de la precisión de la estimación
(que se interpretan de forma similar a una banda de confianza):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{matplot}\NormalTok{(npden}\SpecialCharTok{$}\NormalTok{x, }\FunctionTok{t}\NormalTok{(res.boot}\SpecialCharTok{$}\NormalTok{t), }\AttributeTok{type =} \StringTok{"l"}\NormalTok{, }\AttributeTok{col =} \StringTok{"darkgray"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(npden, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(npden}\SpecialCharTok{$}\NormalTok{x, npden}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{{-}}\NormalTok{ bias)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{06-npden_files/figure-latex/unnamed-chunk-7-1} \end{center}

Pero la recomendación es emplear bootstrap básico (o percentil-\emph{t}) en lugar
de bootstrap percentil (directo) en la presencia de sesgo:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# matplot(npden$x, npden$y + npdeng$y {-} t(res.boot$t), type = "l", col = "darkgray")}
\FunctionTok{matplot}\NormalTok{(npden}\SpecialCharTok{$}\NormalTok{x, }\FunctionTok{pmax}\NormalTok{(npden}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{+}\NormalTok{ npdeng}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{{-}} \FunctionTok{t}\NormalTok{(res.boot}\SpecialCharTok{$}\NormalTok{t), }\DecValTok{0}\NormalTok{), }\AttributeTok{type =} \StringTok{"l"}\NormalTok{, }\AttributeTok{col =} \StringTok{"darkgray"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(npden, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(npden}\SpecialCharTok{$}\NormalTok{x, npden}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{{-}}\NormalTok{ bias)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{06-npden_files/figure-latex/unnamed-chunk-8-1} \end{center}

\hypertarget{npreg}{%
\chapter{Bootstrap y regresión no paramétrica}\label{npreg}}

Por simplicidad nos centraremos en el caso bivariante, el caso multivariante sería análogo.
Supongamos que \(\left\{ \left( X_1,Y_1 \right),\left( X_2,Y_2 \right), \ldots, \left( X_n,Y_n \right) \right\}\) es una m.a.s. de una población bidimensional \(\left( X,Y \right)\), con \(E\left( \left\vert Y\right\vert \right) <\infty\), y que el objetivo es realizar inferencias sobre la distribución condicional \(\left. Y \right\vert_{X=x}\), principalmente estimar la función de regresión de \(Y\) dada \(X\):
\[m\left( x \right) =E\left( \left. Y\right\vert_{X=x} \right).\]

En esta sección se introducirá la estimación no paramétrica de la función de regresión.
En primer lugar se considerará el \emph{estimador de Nadaraya-Watson} (en la Sección \ref{locpol-r} se mostrarán generalizaciones de este estimador desde un punto de vista práctico en R).
En la siguiente sección se introducirán distintos métodos de remuestreo, diseñados inicialmente para este estimador, y resultados para ellos.

\hypertarget{nadaraya-watson}{%
\section{Estimador de Nadaraya-Watson}\label{nadaraya-watson}}

La función de regresión \(m\left( x \right) =E\left( \left. Y\right\vert_{X=x} \right)\) puede escribirse así:
\[\begin{aligned}
m\left( x \right) &= \int yf_{2|1}\left( \left. y\right\vert _{x} \right)
dy=\int y\frac{f\left( x,y \right)}{f_1\left( x \right)}dy=\frac{\int
yf\left( x,y \right) dy}{f_1\left( x \right)} \\
&= \frac{\int yf_{1|2}\left( \left. x\right\vert _{y} \right) f_2\left(
y \right) dy}{f_1\left( x \right)}=\frac{\Psi \left( x \right)}{f_1\left(
x \right)},
\end{aligned}\]
siendo \(f_1\left( x \right)\) la función de densidad marginal de \(X\) y
\[\Psi \left( x \right) =\int yf_{1|2}\left( \left. x\right\vert _{y} \right)
f_2\left( y \right) dy=E\left( Yf_{1|2}\left( \left. x\right\vert_{Y} \right)
\right).\]

Las funciones \(\Psi \left( x \right)\) y \(f_1\left( x \right)\) pueden
estimarse mediante el método núcleo:
\[\begin{aligned}
\hat{f}_{1,h}\left( x \right) &= \frac{1}{nh}\sum_{i=1}^{n}K\left( \frac{
x-X_i}{h} \right), \\
\hat{\Psi}_{h}\left( x \right) &= \frac{1}{nh}\sum_{i=1}^{n}K\left( \frac{
x-X_i}{h} \right) Y_i,
\end{aligned}\]
resultando así el estimador tipo núcleo de Nadaraya-Watson
(ver Nadaraya (1964) y Watson (1964)):
\[\hat{m}_{h}\left( x \right) =\frac{\hat{\Psi}_{h}\left( x \right)}{\hat{f}
_{1,h}\left( x \right)}=\frac{\frac{1}{n}\sum_{i=1}^{n}K_{h}\left(
x-X_i \right) Y_i}{\frac{1}{n}\sum_{i=1}^{n}K_{h}\left( x-X_i \right)},\]
donde \(K_{h}\left( x-X_i \right) =\frac{1}{h}K\left( \frac{x-X_i}{h} \right)\).

Para este estimador se pueden probar propiedades semejantes a las
mencionadas para el estimador de Parzen-Rosenblatt de la función de
densidad.

En esta sección se presentarán métodos de remuestreo bootstrap adecuados
para el contexto de la función de regresión. El objetivo es aproximar la
distribución en el muestreo del estimador de Nadaraya-Watson. Los
resultados reflejan el comportamiento de los métodos de remuestreo
bootstrap, tanto en un aspecto condicional a la muestra de la variable
explicativa como incondicionalmente.

\hypertarget{distribuciuxf3n-asintuxf3tica-del-estimador-de-nadaraya-watson}{%
\subsection{Distribución asintótica del estimador de Nadaraya-Watson}\label{distribuciuxf3n-asintuxf3tica-del-estimador-de-nadaraya-watson}}

Antes de proceder a abordar el bootstrap en este contexto conviene
presentar la distribución asintótica del estimador de Nadaraya-Watson,
dado por
\[\hat{m}_{h}\left( x \right) =\frac{\frac{1}{n}\sum_{i=1}^{n}K_{h}\left(
x-X_i \right) Y_i}{\frac{1}{n}\sum_{i=1}^{n}K_{h}\left( x-X_i \right)}.\]

De forma semejante al caso de la densidad, puede comprobarse que las
condiciones mínimas necesarias para la consistencia del estimador, en
términos del parámetro de suavizado, son \(h\rightarrow 0\),
\(nh\rightarrow \infty\), cuando \(n\rightarrow \infty\). En tales
circunstancias se tiene
\[\sqrt{nh}\left( \hat{m}_{h}\left( x \right) -m\left( x \right) \right) \overset
{d}{\rightarrow }\mathcal{N}\left( B,V \right) \text{.}\]

Además, puede probarse que el valor asintóticamente óptimo de \(h\), en el
sentido del \(MSE\), es de la forma \(h=c_{0}n^{-1/5}\). En tal caso, los
valores de media y varianza de la distribución normal límite son
\[\begin{aligned}
B &= \frac{1}{2}c_{0}^{5/2}d_{K}\frac{m^{\prime \prime }\left( x \right)
f\left( x \right) +2m^{\prime}\left( x \right) f^{\prime}\left( x \right)}{
f\left( x \right)}, \\
V &= c_{K}\frac{\sigma^2\left( x \right)}{f\left( x \right)},
\end{aligned}\]
siendo \(f\left( x \right)\) la función de densidad marginal de \(X\) y
\(\sigma^2\left( x \right) =Var\left( \left. Y\right\vert _{X=x} \right)\)
la varianza condicional de \(Y\) dado \(X=x\).

Al igual que en el caso de la densidad, para utilizar la distribución
asintótica anterior en la construcción de intervalos de confianza para
\(m\left( x \right)\) podemos

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Estimar \(B\) y \(V\) y utilizarlos en la correspondiente distribución
  normal (\textbf{metodo plug-in}).
\item
  Diseñar un plan de remuestreo y utilizar el \textbf{método bootstrap}.
\end{enumerate}

\hypertarget{uxf3rdenes-de-convergencia-de-la-distribuciuxf3n-del-estimador-de-nadaraya-watson-a-su-distribuciuxf3n-asintuxf3tica}{%
\subsection{Órdenes de convergencia de la distribución del estimador de Nadaraya-Watson a su distribución asintótica}\label{uxf3rdenes-de-convergencia-de-la-distribuciuxf3n-del-estimador-de-nadaraya-watson-a-su-distribuciuxf3n-asintuxf3tica}}

Los órdenes de convergencia de la aproximación de la distribución
(condicional o incondicional) del estadístico a la distribución normal
límite vienen dados por:
\[\begin{aligned}
\sup_{z\in \boldsymbol{R}}\left\vert P^{\left. Y\right\vert _{X}}\left[ 
\sqrt{nh}\left( \hat{m}_{h}\left( x \right) -m\left( x \right) \right) \leq z
\right] -\Phi \left( \frac{z-B}{V^{1/2}} \right) \right\vert &= O_{P}\left(
n^{-1/5} \right), \\
\sup_{z\in \boldsymbol{R}}\left\vert P\left[ \sqrt{nh}\left( \hat{m}
_{h}\left( x \right) -m\left( x \right) \right) \leq z\right] -\Phi \left( 
\frac{z-B}{V^{1/2}} \right) \right\vert &= O\left( n^{-2/5} \right),
\end{aligned}\]
donde \(P^{\left. Y\right\vert_{X}}\left( A \right)\) denota
\(P\left( \left. A \right\vert_{X_1,X_2,\ldots ,X_n} \right)\).

\hypertarget{aproximaciuxf3n-plug-in-1}{%
\subsection{Aproximación plug-in}\label{aproximaciuxf3n-plug-in-1}}

Consiste en estimar \(B\) y \(V\) mediante estimadores apropiados de
\(f\left(x \right)\), \(f^{\prime}\left( x \right)\), \(m\left( x \right)\),
\(m^{\prime}\left( x \right)\), \(m^{\prime \prime }\left( x \right)\) y
\(\sigma^2\left( x \right)\). Usando, para cada una de estas seis curvas,
selectores de los parámetros de suavizado encaminados a aproximar las
ventanas óptimas para cada una de ellas (proceso bastante laborioso),
pueden obtenerse estimadores del sesgo, \(\hat{B}\), y la varianza,
\(\hat{V}\), que cumplen \(\hat{B}-B=O_{P}\left( n^{-2/9} \right)\) y
\(\hat{V}-V=O_{P}\left( n^{-2/5} \right)\).
Como consecuencia se tienen los siguientes órdenes de convergencia
(condicional e incondicional) para la aproximación plug-in:
\[\begin{aligned}
\sup_{z\in \boldsymbol{R}}\left\vert P^{\left. Y\right\vert _{X}}\left[ 
\sqrt{nh}\left( \hat{m}_{h}\left( x \right) -m\left( x \right) \right) \leq z
\right] -\Phi \left( \frac{z-\hat{B}}{\hat{V}^{1/2}} \right) \right\vert
&= O_{P}\left( n^{-1/5} \right), \\
\sup_{z\in \boldsymbol{R}}\left\vert P\left[ \sqrt{nh}\left( \hat{m}
_{h}\left( x \right) -m\left( x \right) \right) \leq z\right] -\Phi \left( 
\frac{z-\hat{B}}{\hat{V}^{1/2}} \right) \right\vert &= O_{P}\left(
n^{-2/9} \right).
\end{aligned}\]
que iguala y empeora, respectivamente,
la tasa teórica de la aproximación normal límite (ver Cao (1991)).

\hypertarget{muxe9todos-de-remuestreo-en-regresiuxf3n-no-paramuxe9trica}{%
\section{Métodos de remuestreo en regresión no paramétrica}\label{muxe9todos-de-remuestreo-en-regresiuxf3n-no-paramuxe9trica}}

En este caso se podría emplear también bootstrap uniforme (Sección \ref{boot-unif-reg}) y el bootstrap residual, de forma totálmente análoga a como se mostró en la Sección \ref{boot-residual}.
Sin embargo, en el caso heterocedástico es habitual emplear \emph{Wild bootstrap} y en el caso de diseño aleatorio podría ser recomendable emplear \emph{bootstrap suavizado en la variable explicativa}.

\hypertarget{wild-bootstrap}{%
\subsection{Wild bootstrap}\label{wild-bootstrap}}

Este método de remuestreo bootstrap, propuesto por Wu (1986) y estudiado
por Härdle y Marron (1991), procede del siguiente modo:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  A partir del estimador de Nadaraya-Watson de \(m\left( x \right)\) y
  tomando el parámetro ventana de partida, \(h\), se construyen los
  residuos
  \(r_i = Y_i - \hat{m}_{h}\left( X_i \right)\),
  \(i=1, 2, \ldots, n\).
\item
  Para cada índice \(i=1,2,\ldots ,n\), se arroja, condicionalmente a la
  muestra observada, \(\left\{ \left( X_1,Y_1 \right), \ \left( X_2,Y_2 \right),\right.\)
  \(\left.\ldots ,\ \left( X_n,Y_n \right) \right\}\),
  un error bootstrap \(\hat{\varepsilon}_i^{\ast}\) de una
  distribución de probabilidad que cumpla,
  \(E^{\ast}\left( \hat{\varepsilon}_i^{\ast} \right) =0\),
  \(E^{\ast}\left( \hat{\varepsilon}_i^{\ast 2} \right) =r_i^2\) y
  \(E^{\ast}\left( \hat{\varepsilon}_i^{\ast 3} \right) =r_i^{3}\).
  Aunque la condición del momento de orden 3 no es estrictamente necesaria,
  es útil para las demostraciones de validez del método.
\item
  Usando una ventana piloto \(g\), asintóticamente mayor que \(h\) (i.e.
  \(g/h\rightarrow \infty\)), se arrojan análogos bootstrap de las
  observaciones de la variable respuesta:
  \(Y_i^{\ast}=\hat{m}_{g}\left(X_i \right)  +\hat{\varepsilon}_i^{\ast}\), \(i=1,2,\ldots ,n\).
\item
  A partir de la remuestra bootstrap \(\left\{ \left( X_1,Y_1^{\ast } \right),\left( X_2,Y_2^{\ast} \right),\ldots ,\left( X_n,Y_n^{\ast} \right) \right\}\) se construye el análogo
  bootstrap del estimador de Nadaraya-Watson:
  \[\hat{m}_{h}^{\ast}\left( x \right) =\frac{\frac{1}{n}\sum_{i=1}^{n}K_{h}
  \left( x-X_i \right) Y_i^{\ast}}{\frac{1}{n}\sum_{i=1}^{n}K_{h}\left(
  x-X_i \right)}.\]
\item
  Se aproxima la distribución en el muestreo de \(\sqrt{nh}\left( \hat{m}_{h}\left( x \right) -m\left( x \right) \right)\) por la
  distribución en el remuestreo de
  \(\sqrt{nh}\left( \hat{m}_{h}^{\ast}\left( x \right) - \hat{m}_{g}\left( x \right) \right)\).
\end{enumerate}

El paso 2 suele llevarse a cabo encontrando una variable aleatoria,
\(V^{\ast}\), que cumpla \(E^{\ast}\left( V^{\ast} \right) =0\), \(E^{\ast}\left( V^{\ast 2} \right) =1\) y \(E^{\ast}\left( V^{\ast 3} \right) =1\),
arrojando una muestra de tamaño \(n\) de la misma,
\(\left( V_1^{\ast},V_n^{\ast},\ldots ,V_n^{\ast} \right)\), y luego
definiendo \(\hat{\varepsilon}_i^{\ast} = r_iV_i^{\ast}\)
para \(i=1, 2, \ldots, n\).

Una de las elecciones más habituales para la distribución de
\(V^{\ast}\) es la distribución discreta con masa de probabilidad en dos
puntos (\(P^{\ast}\left( V^{\ast}=a \right) =p\) y
\(P^{\ast}\left( V^{\ast}=b \right) =1-p\)) que es solución del sistema
de tres ecuaciones dadas por los tres primeros momentos:
\[\begin{aligned}
ap+b\left( 1-p \right) &= 0, \\
a^2p+b^2\left( 1-p \right) &= 1, \\
a^{3}p+b^{3}\left( 1-p \right) &= 1.
\end{aligned}\]
Esto da lugar al llamado bootstrap de la sección aurea
(golden section bootstrap), con
\(a=\frac{1-\sqrt{5}}{2}\), \(b=\frac{1+\sqrt{5}}{2}\), \(p=\frac{ 5+\sqrt{5}}{10}\), es decir
\[\begin{aligned}
P^{\ast}\left( V^{\ast}=\frac{1-\sqrt{5}}{2} \right) &= \frac{5+\sqrt{5}}{10} \\
P^{\ast}\left( V^{\ast}=\frac{1+\sqrt{5}}{2} \right) &= \frac{5-\sqrt{5}}{10}
\end{aligned}\]

La elección de la ventana \(g\), que aparece en el paso 3, guarda relación
con la estimación de \(m^{\prime \prime }\left( x \right)\), pues esa es
la cantidad crítica a la hora de estimar \(B\) y \(V\). Tomando una ventana
piloto de orden óptimo en ese sentido, \(g_{0}\simeq d_{0}n^{-1/9}\),
se obtienen las siguientes tasas de convergencia (condicionales e
incondicionales) para la aproximación dada por el wild bootstrap:
\[\begin{gathered}
\sup_{z\in \boldsymbol{R}} \left\vert P^{\left. Y\right\vert _{X}}\left[ 
\sqrt{nh}\left( \hat{m}_{h}\left( x \right) -m\left( x \right) \right) \leq z
\right] - P^{\ast}\left[ \sqrt{nh}\left( \hat{m}_{h}^{\ast}\left( x \right) -
\hat{m}_{g}\left( x \right) \right) \leq z\right] \right\vert = O_{P}\left( n^{-2/9} \right), 
\\
\sup_{z\in \boldsymbol{R}} \left\vert P\left[ \sqrt{nh}\left( \hat{m}
_{h}\left( x \right) -m\left( x \right) \right) \leq z\right] 
 - P^{\ast}\left[ \sqrt{nh}\left( \hat{m}_{h}^{\ast}\left( x \right) -\hat{m}_{g}\left(
x \right) \right) \leq z\right] \right\vert = O_{P}\left( n^{-1/5} \right).
\end{gathered}\]

\hypertarget{bootstrap-suavizado-en-la-variable-explicativa}{%
\subsection{Bootstrap suavizado en la variable explicativa}\label{bootstrap-suavizado-en-la-variable-explicativa}}

La idea es tratar, por un lado, de considerar la variabilidad inherente
a la variable explicativa (en el wild bootstrap esa parte de la
remuestra se mantiene fija) y, por otro, que la distribución en el
remuestreo de \(\left. Y^{\ast}\right\vert _{X^{\ast}=X_i}\) no sea
degenerada (como sí lo sería en un bootstrap naïve bidimensional).

El plan de remuestreo, propuesto por Cao y González-Manteiga (1993)
consta de los siguientes pasos:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Dada la muestra \(\left\{ \left( X_1,Y_1 \right),\left( X_2,Y_2 \right),\ldots ,\left( X_n,Y_n \right) \right\}\), se
  construye un estimador (empírico en la variable respuesta y
  suavizado en la explicativa) de la distribución conjunta de
  \(\left( X,Y \right)\):
  \[\hat{F}_{g}\left( x,y \right) =\frac{1}{n}\sum_{i=1}^{n}\mathbf{1}_{\left\{
  Y_i\leq y\right\} }\int_{-\infty }^{x}K_{g}\left( t-X_i \right) dt.\]
\item
  Se arrojan remuestras bootstrap, \(\left\{ \left( X_1^{\ast },Y_1^{\ast} \right),\left( X_2^{\ast},Y_2^{\ast} \right),\ldots ,\left( X_n^{\ast},Y_n^{\ast} \right) \right\}\), con
  distribución
  \(\hat{F}_{g}\left( x,y \right)\).
\item
  Se construye el análogo bootstrap del estimador de Nadaraya-Watson:
  \[\hat{m}_{h}^{\ast}\left( x \right) =\frac{\frac{1}{n}\sum_{i=1}^{n}K_{h}
  \left( x-X_i^{\ast} \right) Y_i^{\ast}}{\frac{1}{n}\sum_{i=1}^{n}K_{h}
  \left( x-X_i^{\ast} \right)}.\]
\item
  Se utiliza la distribución en el remuestreo de \(\sqrt{nh}\left( \hat{m}_{h}^{\ast}\left( x \right) -\hat{m}_{g}\left( x \right) \right)\)
  para aproximar la distribución del estadístico de interés:
  \(\sqrt{nh}\left( \hat{m}_{h}\left( x \right) -m\left( x \right) \right)\).
\end{enumerate}

La ventana piloto, \(g\), óptima vuelve a ser de orden \(n^{-1/9}\), es
decir asintóticamente mayor que \(h\).

La distribución bidimensional de la que se remuestrea en el paso 2,
\(\hat{F}_{g}\left( x,y \right)\), puede sustituirse por una distribución
suavizada en ambas variables:
\[\tilde{F}_{g}\left( x,y \right) =\frac{1}{n}\sum_{i=1}^{n}
\int_{-\infty}^{y}K_{g}\left( s-Y_i \right) ds \int_{-\infty }^{x}
K_{g}\left( t-X_i \right) dt.\]
Esto es lo mismo que remuestrear de la densidad bidimensional
\[\hat{f}_{g}\left( x,y \right) = \frac{1}{n}\sum_{i=1}^{n}
K_{g}\left( x-X_i \right) K_{g}\left( y-Y_i \right),\]
que es el estimador tipo
núcleo de Parzen-Rosenblatt de la variable bidimensional
\(\left( X,Y \right)\).

Cálculos sencillos permiten demostrar que si \(\left( X^{\ast},Y^{\ast} \right)\) tiene distribución \(\hat{F}_{g}\left( x,y \right)\), entonces,

\begin{itemize}
\item
  \(X^{\ast}\) tiene densidad marginal bootstrap \(\hat{f}_{g}\left( x \right)\).
\item
  La distribución marginal bootstrap de \(Y^{\ast}\) es la empírica de
  las \(Y_i\): \(\hat{F}_n^{Y}\left( y \right) =\frac{1}{n} \sum_{i=1}^{n}\mathbf{1}_{\left\{ Y_i\leq y\right\} }\).
\item
  La función de regresión del plan de remuestreo bootstrap coincide
  con la estimación de Nadaraya-Watson con ventana \(g\), es decir,
  \[E^{\ast}\left( \left. Y^{\ast}\right\vert _{X^{\ast}=x} \right)
  =\hat{m}_{g}\left( x \right).\]
\item
  De hecho, la distribución condicional \(\left. Y^{\ast}\right\vert _{X^{\ast}=x}\) es
  \[\hat{F}_{g}\left( \left. y\right\vert _{x} \right) =\frac{\frac{1}{n}
  \sum_{i=1}^{n}K_{g}\left( x-X_i \right) \mathbf{1}_{\left\{ Y_i\leq
  y\right\} }}{\frac{1}{n}\sum_{i=1}^{n}K_{g}\left( x-X_i \right)},\]
  es decir, el estimador tipo núcleo Nadaraya-Watson de la distribución
  condicional.
\end{itemize}

Esta última observación da pie a diseñar un método que permita simular
valores de \(\left( X^{\ast},Y^{\ast} \right)\), tal y como se requiere
en el paso 2 del plan de remuestreo. Para ello basta con simular
\(X^{\ast}\) a partir del estimador de Parzen-Rosenblatt construído con
la muestra de la variable explicativa (es decir, el bootstrap suavizado
clásico) y luego simular \(Y^{\ast}\) a partir de la distribución
discreta que da a cada dato \(Y_i\) la probabilidad
\[w_i\left( X^{\ast} \right) =\frac{\frac{1}{n}K_{g}\left( X^{\ast}
-X_i \right)}{\frac{1}{n}\sum_{j=1}^{n}K_{g}\left( X^{\ast}-X_j \right)}
\text{, }i=1,2,\ldots ,n\text{.}\]

Las tasas de convergencia de la aproximación bootstrap proporcionadas
por este método resultan:
\[\begin{gathered}
\sup_{z\in \boldsymbol{R}}\left\vert P^{\left. Y\right\vert _{X}}\left[ 
\sqrt{nh}\left( \hat{m}_{h}\left( x \right) -m\left( x \right) \right) \leq z
\right]  -P^{\left. Y^{\ast}\right\vert _{X^{\ast}}}\left[ \sqrt{nh}\left( 
\hat{m}_{h}^{\ast}\left( x \right) -\hat{m}_{g}\left( x \right) \right) \leq z
\right] \right\vert \\
=O_{P^{\ast}}\left( n^{-2/9} \right) \text{, en probabilidad }P, \\
\sup_{z\in \boldsymbol{R}}\left\vert P\left[ \sqrt{nh}\left( \hat{m}
_{h}\left( x \right) -m\left( x \right) \right) \leq z\right] -P^{\ast}\left[ 
\sqrt{nh}\left( \hat{m}_{h}^{\ast}\left( x \right) -\hat{m}_{g}\left(
x \right) \right) \leq z\right] \right\vert \\
=O_{P}\left( n^{-2/9} \right).
\end{gathered}\]

\hypertarget{resumen-comparativo}{%
\subsection{Resumen comparativo}\label{resumen-comparativo}}

La siguiente tabla recoge un resumen de las tasas de convergencia
obtenidas con cada una de las aproximaciones estudiadas:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.28}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.40}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.32}}@{}}
\toprule
Aproximación & condicional & incondicional \\
\midrule
\endhead
Normal teórica & \(O_{P}\left( n^{-1/5}\right)\) & \(O\left(n^{-2/5}\right)\) \\
Plug-in & \(O_{P}\left( n^{-1/5}\right)\) & \(O_{P}\left( n^{-2/9}\right)\) \\
Wild bootstrap & \(O_{P}\left( n^{-2/9}\right)\) & \(O_{P}\left(n^{-1/5}\right)\) \\
Bootstrap suavizado
en la variable
explicativa & \(O_{P^{\ast}}\left( n^{-2/9}\right)\)
en probabilidad \(P\) & \(O_{P}\left(n^{-2/9}\right)\) \\
\bottomrule
\end{longtable}

Exceptuando las tasas de convergencia de la normal teórica (aproximación
inutilizable en la práctica) se observa que el método bootstrap
suavizado en la variable explicativa presenta órdenes que igualan o
mejoran al resto de los métodos, tanto en un aspecto condicional como
condicionalmente. Así, condicionalmente los dos remuestreos bootstrap
son los que ofrecen una mejor tasa de convergencia (\(n^{-2/9}\),
frente a \(n^{-1/5}\) de la aproximación plug-in). En el
sentido incondicional el bootstrap suavizado en la variable explicativa
y la aproximación plug-in son los que presentan un mejor orden
(\(n^{-2/9}\), frente a \(n^{-1/5}\) del wild bootstrap).

\hypertarget{locpol-r}{%
\section{Regresión polinómica local en R}\label{locpol-r}}

El estimador de Nadaraya-Watson de \(m(x)\) descrito en la Sección \ref{nadaraya-watson}
es un caso particular de una clase más amplia de estimadores no paramétricos,
denominados estimadores polinómicos locales.

En el caso univariante, para cada \(x_0\) se ajusta un polinomio:
\[\beta_0+\beta_{1}\left(x - x_0\right) + \cdots 
+ \beta_{p}\left( x-x_0\right)^{p}\]
por mínimos cuadrados ponderados, con pesos
\(w_{i} = \frac{1}{h}K\left(\frac{x-x_0}{h}\right)\).

\begin{itemize}
\item
  La estimación en \(x_0\) es \(\hat{m}_{h}(x_0)=\hat{\beta}_0\).
\item
  Adicionalmente\footnote{Se puede pensar que se están estimando los coeficientes de
    un desarrollo de Taylor de \(m(x_0)\).}:
  \(\widehat{m_{h}^{(r)}}(x_0) = r!\hat{\beta}_{r}\).
\end{itemize}

Por tanto, la estimación polinómica local de grado \(p\), \(\hat{m}_{h}(x)=\hat{\beta}_0\), se obtiene al minimizar:
\[\begin{aligned}
    \min_{\beta_0 ,\beta_1, \ldots, \beta_p}
    \sum_{i=1}^{n}\left\{ Y_{i} - \beta_0 
    -\beta_1(x - X_i) - \ldots \right. \nonumber \\
    \left. -\beta_p(x - X_i)^p \right\}^{2}
    K_{h}(x - X_i).
\end{aligned}\]

Explícitamente:
\[\hat{m}_{h}(x) = \mathbf{e}_{1}^{T} \left(
X_{x}^{T} {W}_{x} 
X_{x} \right)^{-1} X_{x}^{T} 
{W}_{x}\mathbf{Y} \equiv {s}_{x}^{T}\mathbf{Y},\]
donde \(\mathbf{e}_{1} = \left( 1, \cdots, 0\right)^{T}\), \(X_{x}\)
es la matriz con \((1,x - X_i, \ldots, (x - X_i)^p)\) en la fila \(i\),
\(W_{x} = \mathtt{diag} \left( K_{h}(x_{1} - x), \ldots, K_{h}(x_{n} - x) \right)\)
es la matriz de pesos, e \(\mathbf{Y} = \left( Y_1, \cdots, Y_n\right)^{T}\) es el vector de observaciones de la respuesta.

Se puede pensar que se obtiene aplicando un suavizado polinómico a
\((X_i, Y_i)\):
\[\hat{\boldsymbol{m}} = S\mathbf{Y},\]
siendo \(S\) la matriz de suavizado con \(\mathbf{s}_{X_{i}}^{T}\) en la fila \(i\).

Habitualmente se considera:

\begin{itemize}
\item
  \(p=0\): Estimador Nadaraya-Watson.
\item
  \(p=1\): Estimador lineal local.
\end{itemize}

Asintóticamente el estimador lineal local tiene un sesgo menor que el de
Nadaraya-Watson (pero del mismo orden) y la misma varianza (e.g.~Fan and Gijbels, 1996).
Sin embargo, su principal ventaja es que se ve menos afectado por el denominado
efecto frontera (\emph{edge effect}).

Aunque el paquete base de \texttt{R} incluye herramientas para la estimación
tipo núcleo de la regresión (\texttt{lowess()}, \texttt{ksmooth()}), recomiendan
el uso del paquete \texttt{KernSmooth} (Wand y Ripley, 2019).
Otros paquetes incluyen más funcionalidades: \texttt{sm} (Bowman y Azzalini, 2019),
\texttt{np} (Tristen y Jeffrey, 2019), \texttt{npsp} (Fernández-Casal, 2019), entre otros.

Como ejemplo emplearemos el conjunto de datos \texttt{MASS::mcycle} que contiene mediciones
de la aceleración de la cabeza en una simulación de un accidente de motocicleta,
utilizado para probar cascos protectores.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(mcycle, }\AttributeTok{package =} \StringTok{"MASS"}\NormalTok{)}
\NormalTok{x }\OtherTok{\textless{}{-}}\NormalTok{ mcycle}\SpecialCharTok{$}\NormalTok{times}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ mcycle}\SpecialCharTok{$}\NormalTok{accel  }

\FunctionTok{library}\NormalTok{(KernSmooth)}
\NormalTok{h }\OtherTok{\textless{}{-}} \FunctionTok{dpill}\NormalTok{(x, y) }\CommentTok{\# Método plug{-}in de Ruppert, Sheather y Wand (1995)}
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{locpoly}\NormalTok{(x, y, }\AttributeTok{bandwidth =}\NormalTok{ h) }\CommentTok{\# Estimación lineal local}
\FunctionTok{plot}\NormalTok{(x, y)}
\FunctionTok{lines}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{07-npreg_files/figure-latex/unnamed-chunk-2-1} \end{center}

Hay que tener en cuenta que el paquete \texttt{KernSmooth} no implementa los métodos
\texttt{predict()} y \texttt{residuals()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{est }\OtherTok{\textless{}{-}} \FunctionTok{approx}\NormalTok{(fit, }\AttributeTok{xout =}\NormalTok{ x)}\SpecialCharTok{$}\NormalTok{y }\CommentTok{\# est \textless{}{-} predict(fit)}
\NormalTok{resid }\OtherTok{\textless{}{-}}\NormalTok{ y }\SpecialCharTok{{-}}\NormalTok{ est }\CommentTok{\# resid \textless{}{-} residuals(fit)}
\end{Highlighting}
\end{Shaded}

Tampoco calcula medidas de bondad de ajuste, aunque podríamos obtener fácilmente un (pseudo) R-cuadrado:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{r.squared }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{sum}\NormalTok{(resid}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{((y }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(y))}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{r.squared}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.8023864
\end{verbatim}

\hypertarget{estimaciuxf3n-de-la-varianza}{%
\subsection{Estimación de la varianza}\label{estimaciuxf3n-de-la-varianza}}

En el caso heterocedástico, se puede obtener una estimación de la varianza
\(\sigma^2(x)\) mediante suavizado local de los residuos al cuadrado
(Fan y Yao, 1998). Mientras que en el caso homocedástico, se puede obtener
una estimación de la varianza a partir de la suma de cuadrados residual y la
matriz de suavizado:
\[\hat\sigma^2 = \frac{RSS}{df_e},\]
siendo \(RSS=\Sigma_{i=1}^n \left( Y_i - \hat m(X_i) \right)^2\)
y \(df_e = tr(I - S)\) (de forma análoga al caso lineal), o alternativamente \(df_e = tr \left( (I - S^{T})(I - S)\right)\), es una aproximación de los grados de libertad del error.

Adicionalmente:
\[\widehat{Var}\left(\hat{m}_{h}(x_i)\right) = \hat\sigma^2\sum_{j=1}^n s^2_{ij}\]

Uno de los pocos paquetes de \texttt{R} que implementan la estimación de la varianza
y el cálculo de intervalos de confianza es el paquete\footnote{El paquete \texttt{np} calcula
  estimaciones similares aunque no documenta la aproximación que emplea. También
  implementa bootstrap uniforme y bootstrap por bloques.} \texttt{sm} (Bowman y Azzalini, 2019).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(sm)}
\NormalTok{hcv }\OtherTok{\textless{}{-}} \FunctionTok{hcv}\NormalTok{(x, y) }\CommentTok{\# Método de validación cruzada}
\NormalTok{fit.sm }\OtherTok{\textless{}{-}} \FunctionTok{sm.regression}\NormalTok{(x, y, }\AttributeTok{h =}\NormalTok{ hcv, }\AttributeTok{display =} \StringTok{"se"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{07-npreg_files/figure-latex/unnamed-chunk-5-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit.sm}\SpecialCharTok{$}\NormalTok{sigma}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 22.82508
\end{verbatim}

Alternativamente se podría emplear bootstrap.

\hypertarget{ejemplos-2}{%
\section{Ejemplos}\label{ejemplos-2}}

En esta sección nos centraremos en el bootstrap en la estimación tipo núcleo de la función de regresión, para la aproximación de la precisión y el sesgo, y también para el cálculo de intervalos de confianza y de predicción.

\hypertarget{bootstrap-residual}{%
\subsection{Bootstrap residual}\label{bootstrap-residual}}

El modelo ajustado de regresión se puede emplear para estimar la respuesta media \(m(x_0)\) cuando la variable explicativa toma un valor concreto \(x_0\).
En este caso también podemos emplear el bootstrap residual (Sección \ref{boot-residual}) para realizar inferencias acerca de la media.
La idea sería aproximar la distribución del error de estimación \(\hat{m}_h(x_0) - m(x_0)\) por la distribución bootstrap de \(\hat{m}^{\ast}_h(x_0) - \hat{m}_g(x_0)\).

Para reproducir adecuadamente el sesgo del estimador, la ventana \(g\) debería ser asintóticamente mayor que \(h\) (de orden \(n^{-1/5}\)).
Análogamente al caso de la densidad, la recomendación sería emplear la ventana óptima para la estimación de \(m^{\prime \prime }\left( x_0 \right)\), de orden \(n^{-1/9}\) (Sección \ref{wild-bootstrap}).
Sin embargo, en la práctica es habitual emplear \(g=h\) para evitar la selección de esta ventana (lo que además facilita emplear herramientas como el paquete \texttt{boot}).
Otra alternativa podría ser asumir que \(g \simeq n^{1/5}h/n^{1/9}\) como se hace a continuación.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(x)}
\NormalTok{g }\OtherTok{\textless{}{-}}\NormalTok{ h }\SpecialCharTok{*}\NormalTok{ n}\SpecialCharTok{\^{}}\NormalTok{(}\DecValTok{4}\SpecialCharTok{/}\DecValTok{45}\NormalTok{) }\CommentTok{\# h*n\^{}({-}1/9)/n\^{}({-}1/5)}
\CommentTok{\# g \textless{}{-} h}
\NormalTok{fitg }\OtherTok{\textless{}{-}} \FunctionTok{locpoly}\NormalTok{(x, y, }\AttributeTok{bandwidth =}\NormalTok{ g) }\CommentTok{\# puntos de estimación/predicción}
\CommentTok{\# fitg$y \textless{}{-} predict(fitg, newdata = fitg$x) }
\NormalTok{estg }\OtherTok{\textless{}{-}} \FunctionTok{approx}\NormalTok{(fitg, }\AttributeTok{xout =}\NormalTok{ x)}\SpecialCharTok{$}\NormalTok{y }\CommentTok{\# puntos observaciones}
\CommentTok{\# estg \textless{}{-} predict(fitg)}
\CommentTok{\# resid2 \textless{}{-} y {-} estg \# resid2 \textless{}{-} residuals(fitg)}

\CommentTok{\# Remuestreo}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{B }\OtherTok{\textless{}{-}} \DecValTok{2000}
\NormalTok{stat\_fit\_boot }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\AttributeTok{nrow =} \FunctionTok{length}\NormalTok{(fit}\SpecialCharTok{$}\NormalTok{x), }\AttributeTok{ncol =}\NormalTok{ B)}
\NormalTok{resid0 }\OtherTok{\textless{}{-}}\NormalTok{ resid }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(resid)}
\ControlFlowTok{for}\NormalTok{ (k }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{B) \{}
\NormalTok{    y\_boot }\OtherTok{\textless{}{-}}\NormalTok{ estg }\SpecialCharTok{+} \FunctionTok{sample}\NormalTok{(resid0, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{    fit\_boot }\OtherTok{\textless{}{-}} \FunctionTok{locpoly}\NormalTok{(x, y\_boot, }\AttributeTok{bandwidth =}\NormalTok{ h)}\SpecialCharTok{$}\NormalTok{y}
\NormalTok{    stat\_fit\_boot[ , k] }\OtherTok{\textless{}{-}}\NormalTok{ fit\_boot }\SpecialCharTok{{-}}\NormalTok{ fitg}\SpecialCharTok{$}\NormalTok{y}
\NormalTok{\}}

\CommentTok{\# Calculo del sesgo y error estándar }
\NormalTok{bias }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(stat\_fit\_boot, }\DecValTok{1}\NormalTok{, mean)}
\NormalTok{std.err }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(stat\_fit\_boot, }\DecValTok{1}\NormalTok{, sd)}

\CommentTok{\# Representar estimación y corrección de sesgo bootstrap}
\FunctionTok{plot}\NormalTok{(x, y)}
\FunctionTok{lines}\NormalTok{(fit, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(fit}\SpecialCharTok{$}\NormalTok{x, fit}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{{-}}\NormalTok{ bias)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{07-npreg_files/figure-latex/unnamed-chunk-6-1} \end{center}

NOTA: De forma análoga al caso lineal (Sección \ref{boot-residual}), se podrían reescalar los residuos a partir de la matriz de suavizado (empleando los paquetes \texttt{sm} o \texttt{npsp}).

\hypertarget{intervalos-de-confianza-y-predicciuxf3n}{%
\subsection{Intervalos de confianza y predicción}\label{intervalos-de-confianza-y-predicciuxf3n}}

De forma análoga al caso de la estimación de la densidad mostrado en la Sección \ref{npden-r-ic}, podemos calcular estimaciones por intervalo de confianza (puntuales) por el método percentil (básico):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{alfa }\OtherTok{\textless{}{-}} \FloatTok{0.05}
\NormalTok{pto\_crit }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(stat\_fit\_boot, }\DecValTok{1}\NormalTok{, quantile, }\AttributeTok{probs =} \FunctionTok{c}\NormalTok{(alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{))}
\NormalTok{ic\_inf\_boot }\OtherTok{\textless{}{-}}\NormalTok{ fit}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{{-}}\NormalTok{ pto\_crit[}\DecValTok{2}\NormalTok{, ]}
\NormalTok{ic\_sup\_boot }\OtherTok{\textless{}{-}}\NormalTok{ fit}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{{-}}\NormalTok{ pto\_crit[}\DecValTok{1}\NormalTok{, ]}

\FunctionTok{plot}\NormalTok{(x, y)}
\FunctionTok{lines}\NormalTok{(fit, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(fit}\SpecialCharTok{$}\NormalTok{x, fit}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{{-}}\NormalTok{ bias)}
\FunctionTok{lines}\NormalTok{(fit}\SpecialCharTok{$}\NormalTok{x, ic\_inf\_boot, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(fit}\SpecialCharTok{$}\NormalTok{x, ic\_sup\_boot, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{07-npreg_files/figure-latex/unnamed-chunk-7-1} \end{center}

El modelo ajustado también es empleado para predecir una nueva respuesta individual \(Y(x_0)\) para un valor concreto \(x_0\) de la variable explicativa.\\
En el caso de errores independientes \(\hat{Y}(x_0) = \hat{m}_h(x_0)\), pero si estamos interesados en realizar inferencias sobre el error de predicción \(r(x_0) = Y(x_0) - \hat{Y}(x_0)\), a la variabilidad de \(\hat{m}_h(x_0)\) debida a la muestra, se añade la variabilidad del error \(\varepsilon(x_0)\).

La idea sería aproximar la distribución del error de predicción:
\[r(x_0) = Y(x_0) - \hat{Y}(x_0) = m(x_0) + \varepsilon(x_0) - \hat{m}_h(x_0)\]
por la distribución bootstrap de:
\[r^{\ast}(x_0) = Y^{\ast}(x_0) - \hat{Y}^{\ast}(x_0) = \hat{m}_g(x_0) + \varepsilon^{\ast}(x_0) - \hat{m}^{\ast}_h(x_0)\]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Remuestreo}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{n\_pre }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(fit}\SpecialCharTok{$}\NormalTok{x)}
\NormalTok{stat\_pred\_boot }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\AttributeTok{nrow =}\NormalTok{ n\_pre, }\AttributeTok{ncol =}\NormalTok{ B)}
\ControlFlowTok{for}\NormalTok{ (k }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{B) \{}
\NormalTok{    y\_boot }\OtherTok{\textless{}{-}}\NormalTok{ estg }\SpecialCharTok{+} \FunctionTok{sample}\NormalTok{(resid0, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{    fit\_boot }\OtherTok{\textless{}{-}} \FunctionTok{locpoly}\NormalTok{(x, y\_boot, }\AttributeTok{bandwidth =}\NormalTok{ h)}\SpecialCharTok{$}\NormalTok{y}
\NormalTok{    pred\_boot }\OtherTok{\textless{}{-}}\NormalTok{ fitg}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{+} \FunctionTok{sample}\NormalTok{(resid0, n\_pre, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{    stat\_pred\_boot[ , k] }\OtherTok{\textless{}{-}}\NormalTok{ pred\_boot }\SpecialCharTok{{-}}\NormalTok{ fit\_boot}
\NormalTok{\}}

\CommentTok{\# Cálculo de intervalos de predicción}
\CommentTok{\# por el método percentil (básico)}
\NormalTok{alfa }\OtherTok{\textless{}{-}} \FloatTok{0.05}
\NormalTok{pto\_crit\_pred }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(stat\_pred\_boot, }\DecValTok{1}\NormalTok{, quantile, }\AttributeTok{probs =} \FunctionTok{c}\NormalTok{(alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alfa}\SpecialCharTok{/}\DecValTok{2}\NormalTok{))}
\NormalTok{ip\_inf\_boot }\OtherTok{\textless{}{-}}\NormalTok{ fit}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{+}\NormalTok{ pto\_crit\_pred[}\DecValTok{1}\NormalTok{, ]}
\NormalTok{ip\_sup\_boot }\OtherTok{\textless{}{-}}\NormalTok{ fit}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{+}\NormalTok{ pto\_crit\_pred[}\DecValTok{2}\NormalTok{, ]}

\FunctionTok{plot}\NormalTok{(x, y, }\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{150}\NormalTok{, }\DecValTok{75}\NormalTok{))}
\FunctionTok{lines}\NormalTok{(fit, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(fit}\SpecialCharTok{$}\NormalTok{x, fit}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{{-}}\NormalTok{ bias)}
\FunctionTok{lines}\NormalTok{(fit}\SpecialCharTok{$}\NormalTok{x, ic\_inf\_boot, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(fit}\SpecialCharTok{$}\NormalTok{x, ic\_sup\_boot, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(fit}\SpecialCharTok{$}\NormalTok{x, ip\_inf\_boot, }\AttributeTok{lty =} \DecValTok{3}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(fit}\SpecialCharTok{$}\NormalTok{x, ip\_sup\_boot, }\AttributeTok{lty =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{07-npreg_files/figure-latex/unnamed-chunk-8-1} \end{center}

En este caso puede no ser recomendable considerar errores i.i.d., sería de esperar heterocedásticidad (e incluso dependencia temporal).
El bootstrap residual se puede extender al caso heterocedástico y/o dependencia (e.g.~Castillo-Páez \emph{et al.}, 2019, 2020).

\hypertarget{r-wild-bootstrap}{%
\subsection{Wild bootstrap}\label{r-wild-bootstrap}}

Como se describe en la Sección \ref{wild-bootstrap} en el caso heterocedástico se puede emplear wild bootstrap.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Remuestreo}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{B }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{fit\_boot }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\AttributeTok{nrow =}\NormalTok{ n\_pre, }\AttributeTok{ncol =}\NormalTok{ B)}
\ControlFlowTok{for}\NormalTok{ (k }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{B) \{}
\NormalTok{        rwild }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\FunctionTok{c}\NormalTok{((}\DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{sqrt}\NormalTok{(}\DecValTok{5}\NormalTok{))}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{sqrt}\NormalTok{(}\DecValTok{5}\NormalTok{))}\SpecialCharTok{/}\DecValTok{2}\NormalTok{), n, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{, }
                        \AttributeTok{prob =} \FunctionTok{c}\NormalTok{((}\DecValTok{5} \SpecialCharTok{+} \FunctionTok{sqrt}\NormalTok{(}\DecValTok{5}\NormalTok{))}\SpecialCharTok{/}\DecValTok{10}\NormalTok{, }\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ (}\DecValTok{5} \SpecialCharTok{+} \FunctionTok{sqrt}\NormalTok{(}\DecValTok{5}\NormalTok{))}\SpecialCharTok{/}\DecValTok{10}\NormalTok{))}
\NormalTok{    y\_boot }\OtherTok{\textless{}{-}}\NormalTok{ estg }\SpecialCharTok{+}\NormalTok{ resid}\SpecialCharTok{*}\NormalTok{rwild}
\NormalTok{    fit\_boot[ , k] }\OtherTok{\textless{}{-}} \FunctionTok{locpoly}\NormalTok{(x, y\_boot, }\AttributeTok{bandwidth =}\NormalTok{ h)}\SpecialCharTok{$}\NormalTok{y}
    \CommentTok{\# OJO: bootstrap percetil directo}
\NormalTok{\}}
        

\CommentTok{\# Calculo del sesgo y error estándar}
\NormalTok{bias }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(fit\_boot, }\DecValTok{1}\NormalTok{, mean, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{{-}}\NormalTok{ fitg}\SpecialCharTok{$}\NormalTok{y}
\NormalTok{std.err }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(fit\_boot, }\DecValTok{1}\NormalTok{, sd, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}

\CommentTok{\# Representar estimación y corrección de sesgo bootstrap}
\FunctionTok{plot}\NormalTok{(x, y)}
\FunctionTok{lines}\NormalTok{(fit, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(fit}\SpecialCharTok{$}\NormalTok{x, fit}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{{-}}\NormalTok{ bias)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{07-npreg_files/figure-latex/unnamed-chunk-9-1} \end{center}

\hypertarget{ejercicio-1}{%
\subsection{Ejercicio}\label{ejercicio-1}}

Siguiendo con el conjunto de datos \texttt{MASS::mcycle}, emplear wild bootstrap para
obtener estimaciones por intervalo de confianza de la función de regresión
de \texttt{accel} a partir de \texttt{times} mediante bootstrap percentil básico.
Comparar los resultados con los obtenidos mediante bootstrap residual (comentar las diferencias y cuál de las aproximaciones sería más adecuada para este caso).

\hypertarget{bootcen}{%
\chapter{El Bootstrap con datos censurados}\label{bootcen}}

En este capítulo se hace una introducción a los datos censurados y se
presentan diversos métodos de remuestreo para este contexto, analizando
la validez de los mismos.

\hypertarget{introducciuxf3n-a-los-datos-censurados}{%
\section{Introducción a los datos censurados}\label{introducciuxf3n-a-los-datos-censurados}}

Considérese una variable de interés, \(X\), no negativa que no siempre es
posible observar (por ejemplo un tiempo de vida) pues, en ocasiones,
ocurre otro fenómeno previo, cuyo tiempo hasta su ocurrencia, \(C\), puede
ser anterior a la variable de interés (es decir, \(C<X\)). Cuando \(X\) es
un tiempo de vida ante una enfermedad mortal, la variable \(C\) suele
representar el tiempo hasta el fin del estudio, el tiempo hasta que el
individuo fallezca por otra causa o el tiempo hasta que se produce una
pérdida de seguimiento. Es habitual definir el indicador de no censura
\(\delta =\mathbf{1}_{\left\{ X\leq C\right\} }\). Si \(C<X\) diremos que la
observación es censurada y sólo seremos capaces de observar \(C\) junto
con el valor de \(\delta\). Cuando \(X\leq C\) entonces somos capaces de
observar la variable de interés y además el valor de \(\delta\).

En resumen, en lugar de observar la muestra
\(\left( X_1, X_2, \ldots, X_n \right)\), sólo podemos observar
\[\left( \left( T_1, \delta _1 \right), \left( T_2, \delta _2 \right),
\ldots ,\left( T_n, \delta_n \right) \right),\]
siendo \(T_i=\min \left\{ X_i,C_i\right\}\)
los tiempos de vida observados y
\[\delta _i=\mathbf{1}_{\left\{ X_i\leq
C_i\right\} }=\mathbf{1}_{\left\{ T_i=X_i\right\} }\]
los indicadores de censura, para \(i=1,2,\ldots ,n\).
En el modelo de censura aleatoria por la derecha,
que es el más habitual, se supone que \(X_i\)
y \(C_i\) (\(i=1,2,\ldots ,n\)) son independientes.
Además \(\left( X_1, X_2, \ldots ,X_n \right)\) son mutuamente independientes,
como también lo son \(\left( C_1, C_2,\ldots ,C_n \right)\).

Denotando por \(F\) (respectivamente \(G\) y \(H\)) la función de distribución
de la variable aleatoria \(X\) (respectivamente \(C\) y \(T\)), la condición
de independencia implica que
\[1-H\left( t \right) =\left( 1-F\left(
t \right) \right) \left( 1-G\left( t \right) \right).\]

\hypertarget{estimador-de-kaplan-meier}{%
\subsection{Estimador de Kaplan-Meier}\label{estimador-de-kaplan-meier}}

Puede verse fácilmente que, bajo censura aleatoria por la derecha, la
distribución empírica \(F_n\left( t \right) =\frac{1}{n} \sum_{i=1}^{n}\mathbf{1}_{\left\{ T_i\leq t\right\} }\) deja de ser
consistente. En este contexto el estimador no paramétrico de máxima
verosimilitud de la función de distribución es el estimador
límite-producto, propuesto por Kaplan y Meier (1958), obtenido
a partir de la función de supervivencia
(\(S\left( t \right) = 1-F\left( t \right)\)):
\[\hat{S}\left( t \right) = 1-\hat{F}\left( t \right) =
\prod_{T_{(i)}\leq t}\left( \frac{n-i}{n-i+1} \right)^{\delta _{(i)}},\]
siendo \(\left( T_{(1)},T_{\left( 2 \right)},\ldots ,T_{\left( n \right)} \right)\) la muestra de estadísticos ordenados de los tiempos
de vida observados y \(\left( \delta _{(1)},\delta _{\left(2 \right)}, \ldots ,\delta _{(n)} \right)\) los correspondientes concomitantes.

\begin{example}[Estimación de Kaplan-Meier]
\protect\hypertarget{exm:kaplan-meier}{}{\label{exm:kaplan-meier} \iffalse (Estimación de Kaplan-Meier) \fi{} }
\vspace{0.5cm}

Se observan los datos censurados: \(\left( 2.1,0 \right)\),
\(\left(3.2,1 \right)\), \(\left( 1.2,1 \right)\), \(\left( 4.3,0 \right)\), \(\left( 1.8,1 \right)\), \(\left( 3.9,1 \right)\), \(\left( 2.7,0 \right)\), \(\left( 2.5,1 \right)\). El estimador resulta:

\[\hat{F}\left( t \right) =\left\{ 
\begin{array}{ll}
0& \text{si } t<1.2 \\ 
0.125& \text{si } 1.2\leq t<1.8 \\ 
0.25& \text{si } 1.8\leq t<2.5 \\ 
0.4& \text{si } 2.5\leq t<3.2 \\ 
0.6& \text{si } 3.2\leq t<3.9 \\ 
0.8& \text{si } 3.9\leq t
\end{array}
\right.\]
\end{example}

En \texttt{R} se recomienda emplear el paquete \texttt{survival} para el análisis de
datos censurados. Podemos utilizar la función \texttt{survfit()} para obtener
la estimación Kaplan-Meier de la función de supervivencia
(y a partir de ella la de la distribución).
En este caso podríamos utilizar el siguiente código {[}Figura \ref{fig:survival}{]}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{datcen }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{t =} \FunctionTok{c}\NormalTok{(}\FloatTok{2.1}\NormalTok{, }\FloatTok{3.2}\NormalTok{, }\FloatTok{1.2}\NormalTok{, }\FloatTok{4.3}\NormalTok{, }\FloatTok{1.8}\NormalTok{, }\FloatTok{3.9}\NormalTok{, }\FloatTok{2.7}\NormalTok{, }\FloatTok{2.5}\NormalTok{), }
                 \AttributeTok{cen =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\FunctionTok{library}\NormalTok{(survival)}
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{survfit}\NormalTok{(}\FunctionTok{Surv}\NormalTok{(t, cen)}\SpecialCharTok{\textasciitilde{}}\DecValTok{1}\NormalTok{, }\AttributeTok{data =}\NormalTok{ datcen)}
\FunctionTok{summary}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Call: survfit(formula = Surv(t, cen) ~ 1, data = datcen)
## 
##  time n.risk n.event survival std.err lower 95% CI upper 95% CI
##   1.2      8       1    0.875   0.117       0.6734            1
##   1.8      7       1    0.750   0.153       0.5027            1
##   2.5      5       1    0.600   0.182       0.3315            1
##   3.2      3       1    0.400   0.203       0.1477            1
##   3.9      2       1    0.200   0.174       0.0363            1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{old.par }\OtherTok{\textless{}{-}} \FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(fit, }\AttributeTok{main =} \StringTok{"Método plot de un objeto \textquotesingle{}survfit\textquotesingle{}"}\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}\StringTok{"bottomleft"}\NormalTok{,  }\FunctionTok{c}\NormalTok{(}\StringTok{"supervivencia"}\NormalTok{, }\StringTok{"conf.int"}\NormalTok{), }\AttributeTok{lty =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{)}

\FunctionTok{with}\NormalTok{(fit, \{}
  \FunctionTok{plot}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, time), }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, surv), }\AttributeTok{type =} \StringTok{"s"}\NormalTok{, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{,}
       \AttributeTok{main =} \StringTok{"Estimaciones funciones supervicencia y distribución"}\NormalTok{, }
       \AttributeTok{xlab =} \StringTok{"t"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{""}\NormalTok{, }\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{))}
  \FunctionTok{lines}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, time), }\DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, surv), }\AttributeTok{type =} \StringTok{"s"}\NormalTok{)}
  \FunctionTok{legend}\NormalTok{(}\StringTok{"bottomright"}\NormalTok{,  }\FunctionTok{c}\NormalTok{(}\StringTok{"supervivencia"}\NormalTok{, }\StringTok{"distribución"}\NormalTok{), }\AttributeTok{lty =} \DecValTok{2}\SpecialCharTok{:}\DecValTok{1}\NormalTok{)}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

\begin{figure}[!htb]

{\centering \includegraphics[width=0.7\linewidth]{08-dat_cen_files/figure-latex/survival-1} 

}

\caption{Estimaciones Kaplan-Meier de la función de supervivencia y de la función de distribución.}\label{fig:survival}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(old.par)}
\end{Highlighting}
\end{Shaded}

\hypertarget{distribuciuxf3n-asintuxf3tica-del-estimador-de-kaplan-meier}{%
\subsection{Distribución asintótica del estimador de Kaplan-Meier}\label{distribuciuxf3n-asintuxf3tica-del-estimador-de-kaplan-meier}}

El estimador de Kaplan-Meier sólo otorga pesos positivos a los datos no
censurados, aunque la forma de distribuirse los datos censurados en
medio de los no censurados afecta a los pesos de estos últimos. Por otra
parte, en ausencia de censura (es decir \(\delta _i=1\),
\(i=1,2,\ldots ,n\)), el estimador de Kaplan-Meier coincide con la
distribución empírica.

La obtención de la propiedades de sesgo y varianza asintóticos y
distribución límite del estimador de Kaplan-Meier es mucho más laboriosa
que en el caso de la distribución empírica, en un contexto sin censura.
Esto es así porque el estimador de Kaplan-Meier deja de ser una suma de
variables iid, como sí ocurre con la empírica.

Breslow y Crowley (1974) obtienen el siguiente resultado para la distribución
límite para el estimador de Kaplan-Meier:
\[\sqrt{n}\left( \hat{F}\left( t \right) -F\left( t \right) \right) 
\overset{d}{\longrightarrow} \mathcal{N}\left( 0,\sigma^2\left( t \right)
\right),\]
siendo
\[\begin{aligned}
\sigma^2\left( t \right) &= \left( 1-F\left( t \right) \right)
^2\int_{0}^{t}\frac{dH_1\left( u \right)}{\left( 1-H\left( u \right)
 \right)^2}\text{, }t\leq H^{-1}(1) , \\
H_1\left( u \right) &= P\left( X\leq u,X\leq C \right) =P\left( T\leq
u,\delta =1 \right).
\end{aligned}\]
También existen resultados de convergencia en distribución del
proceso estocástico
\[\left\{ \sqrt{n}\left( \hat{F}\left( t \right) - F\left( t \right)
 \right) : t \in \left[ 0,H^{-1}(1) \right] \right\}\]
a un proceso gaussiano límite.

\hypertarget{remuestreos-bootstrap-en-presencia-de-censura}{%
\section{Remuestreos Bootstrap en presencia de censura}\label{remuestreos-bootstrap-en-presencia-de-censura}}

Estos métodos tratan del mecanismo bootstrap para aproximar la
distribución de un estadístico, \(R\left( \mathbf{T}, \boldsymbol{\delta} \right)\), siendo \(\mathbf{T}=\left( T_1, T_2, \ldots,T_n \right)\) y \(\boldsymbol{\delta}=\left( \delta _1,\delta_2, \ldots ,\delta _n \right)\). Los dos siguientes métodos de
remuestreo fueron propuestos por Efron (1981).

\hypertarget{el-bootstrap-simple}{%
\subsection{El bootstrap simple}\label{el-bootstrap-simple}}

Procede de la siguiente forma:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Construir la distribución empírica bidimensional,
  \(F_n^{T,\delta }\), de la muestra
  \(\left\{ \left( T_1,\delta _1 \right), \left( T_2,\delta _2 \right), \ldots, \left( T_n,\delta _n \right) \right\}\).
\item
  Arrojar remuestras \(\left\{ \left( T_1^{\ast},\delta _1^{\ast} \right), \left( T_2^{\ast},\delta _2^{\ast} \right), \ldots, \left( T_n^{\ast},\delta _n^{\ast} \right) \right\}\)
  a partir de dicha distribución empírica. Esto es tanto como decir
  que\[P^{\ast}\left( \left( T^{\ast},\delta^{\ast} \right) =\left( T_i,\delta
  _i \right) \right) =\frac{1}{n}\text{, para }i=1,2,\ldots ,n\text{.}\]
\item
  Evaluar el estadístico de interés en el vector que contiene la
  remuestra bootstrap: \(R^{\ast}=R\left( \mathbf{T}^{\ast}, \boldsymbol{\delta}^{\ast} \right)\), con
  \(\mathbf{T}^{\ast} =\left( T_1^{\ast},T_2^{\ast},\ldots ,T_n^{\ast} \right)\) y

  \(\boldsymbol{\delta}^{\ast}=\left( \delta _1^{\ast},\delta _2^{\ast},\ldots ,\delta _n^{\ast} \right)\).
\item
  Aproximar la distribución en el muestreo del estadístico
  \(R\left( \mathbf{T}, \boldsymbol{\delta} \right)\) por la
  distribución en el remuestreo de \(R\left( \mathbf{T}^{\ast},\boldsymbol{\delta}^{\ast} \right)\).
\end{enumerate}

Este método es de muy rápida implementación y ejecución.

\hypertarget{bootcen-obvio}{%
\subsection{El bootstrap obvio}\label{bootcen-obvio}}

Para detallar el método es necesario definir el estimador de
Kaplan-Meier, \(\hat{G}\left( t \right)\), de la variable censurante, a
partir
de\[1-\hat{G}\left( t \right) =\prod_{T_{(i)}\leq t}\left( \frac{n-i
}{n-i+1} \right)^{1-\delta _{(i)}}.\] Observemos que este
estimador es totalmente semejante al de Kaplan-Meier de la variable de
interés pero simplemente reemplazando cada valor
\(\delta _{(i)}\) por \(1-\delta _{(i)}\).

El mecanismo de remuestreo procede como sigue:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Construir los estimadores de Kaplan-Meier de las distribuciones de
  la variable de interés, \(\hat{F}\left( t \right)\), y de la variable
  censurante, \(\hat{G}\left( t \right)\).
\item
  Para cada índice \(i=1,2,\ldots ,n\), arrojar observaciones bootstrap
  independientes, \(X_i^{\ast}\) con distribución \(\hat{F}\ \)y
  \(C_i^{\ast}\) con distribución \(\hat{G}\).
\item
  Definir \(T_i^{\ast}=\min \left\{ X_i^{\ast},C_i^{\ast}\right\}\) y
  \(\delta_i^{\ast}=\mathbf{1}_{\left\{ X_i^{\ast}\leq C_i^{\ast}\right\}}\), para \(i = 1, 2, \ldots, n\),
  y considerar la remuestra bootstrap
  \(\left( \mathbf{T}^{\ast},\boldsymbol{\delta}^{\ast}\right)\), con
  \(\mathbf{T}^{\ast}=\left( T_1^{\ast},T_2^{\ast}, \ldots, T_n^{\ast} \right)\) y \(\boldsymbol{\delta}^{\ast} = \left( \delta_1^{\ast}, \delta_2^{\ast},\ldots ,\delta_n^{\ast} \right)\).
\item
  Aproximar la distribución en el muestreo del estadístico
  \(R\left( \mathbf{T},\boldsymbol{\delta} \right)\) por la
  distribución en el remuestreo de su análogo bootstrap, \(R\left( \mathbf{T}^{\ast},\boldsymbol{\delta}^{\ast} \right)\).
\end{enumerate}

Obviamente, este método de remuestreo imita fielmente el modelo de datos
censurados por la derecha. Su ejecución es considerablemente más lenta
que la del método simple, pues necesita de la construcción de los
estimadores de Kaplan-Meier, de la obtención de remuestras a partir de
ellos y de algunos cálculos adicionales.

\hypertarget{relaciones-entre-los-muxe9todos-de-remuestreo-bajo-censura}{%
\section{Relaciones entre los métodos de remuestreo bajo censura}\label{relaciones-entre-los-muxe9todos-de-remuestreo-bajo-censura}}

\hypertarget{equivalencia-entre-el-bootstrap-simple-y-el-obvio}{%
\subsection{Equivalencia entre el bootstrap simple y el obvio}\label{equivalencia-entre-el-bootstrap-simple-y-el-obvio}}

Es fácil demostrar que el bootstrap simple y el obvio son planes de
remuestreo equivalentes (cuando se supone que en la muestra no existe
ninguna observación no censurada que esté empatada con otra censurada).
Esta equivalencia se establece en el sentido de que la distribución
bootstrap de \(\left( T^{\ast},\delta^{\ast} \right)\) es la misma para
cualquiera de los dos métodos.

Así, si \(\left( T^{\ast},\delta^{\ast} \right)\) se genera mediante el
método obvio, entonces

\[\begin{aligned}
P^{\ast}\left( T^{\ast}>t \right) &= P^{\ast}\left( X^{\ast}>t,C^{\ast
}>t \right) \\
&= P^{\ast}\left( X^{\ast}>t \right) P^{\ast}\left( C^{\ast}>t \right)
=\left( 1-\hat{F}\left( t \right) \right) \left( 1-\hat{G}\left( t \right)
 \right) \\
&= \left[ \prod_{T_{(i)}\leq t}\left( \frac{n-i}{n-i+1} \right)
^{\delta _{(i)}}\right] \left[ \prod_{T_{(i)}\leq
t}\left( \frac{n-i}{n-i+1} \right)^{1-\delta _{(i)}}\right] \\
&= \prod_{T_{(i)}\leq t}\frac{n-i}{n-i+1}=\prod_{i=1}^{\#\left
\{ T_{(j)}\leq t\right\} }\frac{n-i}{n-i+1} \\
&= \frac{n-1}{n}\cdot \frac{n-2}{n-1}\cdot \cdots \cdot \frac{n-\#\left\{
T_{(j)}\leq t\right\} }{n-\#\left\{ T_{(j)}\leq
t\right\} +1} \\
&= \frac{n-\#\left\{ T_{(j)}\leq t\right\} }{n}=1-H_n\left(
t \right) =\frac{\#\left\{ T_{(j)}>t\right\} }{n},
\end{aligned}\]

siendo \(H_n\left( t \right)\) la distribución empírica de la muestra
\(\left( T_1,T_2,\ldots ,T_n \right)\).

Esto demuestra que la distribución bootstrap marginal de \(T^{\ast}\) es
la misma para ambos remuestreos. Sólo resta probar pues que la
distribución condicionada
\(\left. \delta^{\ast}\right\vert _{T^{\ast}=T_i}\) es idéntica en
ambos casos. Pero esto es inmediato ya que, en los dos remuestreos esa
distribución condicionada es la degenerada en el valor
\(\delta _i\).

\hypertarget{bootcen-reid}{%
\subsection{El bootstrap de Reid}\label{bootcen-reid}}

Es otro método alternativo propuesto por Reid (1981). Consta de los
siguientes pasos:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Construir el estimador de Kaplan-Meier, \(\hat{F}\left( t \right)\),
  de la muestra original.
\item
  Arrojar remuestras bootstrap (todas formadas por observaciones no
  censuradas, \(T_i^{\ast}\), \(i=1,2,\ldots ,n\)) a partir de
  \(\hat{F}\left(t \right)\).
\item
  Aproximar la distribución en el muestreo de \(R\left( \mathbf{T},\boldsymbol{\delta} \right)\), por la
  distribución bootstrap de
  \(R\left( \mathbf{T}^{\ast},\mathbf{1} \right)\),
  siendo \(\mathbf{1}\) el vector formado
  por \(n\) unos.
\end{enumerate}

\hypertarget{validez-de-los-planes-de-remuestreo}{%
\subsection{Validez de los planes de remuestreo}\label{validez-de-los-planes-de-remuestreo}}

Akritas (1986) demuestra que los procesos bootstrap
\[\begin{aligned}
\sqrt{n}\left( \hat{F}^{\ast}_{Efron}\left( t \right) 
- \hat{F}\left( t \right) \right) \\ 
\sqrt{n}\left( \hat{F}^{\ast}_{Reid} \left( t \right) 
- \hat{F}\left( t \right) \right)
\end{aligned}\]
tienden a sendos procesos límite distintos. Aquí
\(\hat{F}^{\ast}_{Efron}\) denota la versión bootstrap del estimador
de Kaplan-Meier bajo el remuestreo de Efron (cualquiera de ellos, ya que
el remuestreo simple y el obvio son equivalentes) y
\(\hat{F}^{\ast}_{Reid}\) es la correspondiente versión bootstrap del
estimador de Kaplan-Meier bajo el remuestreo de Reid (una distribución
empírica, al fin y al cabo, porque en el remuestreo de Reid todas las
observaciones son no censuradas).

Además el proceso límite del estimador de Kaplan-Meier, \(\sqrt{n} \left( \hat{F}\left( t \right) -F\left( t \right) \right)\), es el mismo
que el del bootstrap de Efron. Como consecuencia el remuestreo de Efron
es consistente y el de Reid es inconsistente.

\hypertarget{implementaciuxf3n-en-r-con-los-paquetes-boot-y-survival}{%
\section{\texorpdfstring{Implementación en \texttt{R} (con los paquetes \texttt{boot} y \texttt{survival})}{Implementación en R (con los paquetes boot y survival)}}\label{implementaciuxf3n-en-r-con-los-paquetes-boot-y-survival}}

La función \texttt{censboot()} del paquete \texttt{boot} implementa distintos métodos
de remuestreo para datos censurados. Por defecto utiliza el bootstrap simple
(\texttt{sim\ =\ "ordinary"}) y su uso es prácticamente igual al del bootstrap uniforme
con la función \texttt{boot()} (descrita en la Sección \ref{intro-pkgboot}),
la única diferencia es que la función \texttt{statistic} solo tiene los datos
como único parámetro (aunque en este caso podríamos emplear también
la función \texttt{boot()}).

\hypertarget{bootstrap-simple}{%
\subsection{Bootstrap simple}\label{bootstrap-simple}}

Como ejemplo utilizaremos el conjunto de datos \texttt{channing} del paquete \texttt{boot},
que contiene la edad de entrada y de partida o muerte de las personas
que pasaron por el centro de retiro `Channing House' (Palo Alto, California),
desde su apertura en 1964 hasta el 1 de julio de 1975
(ver Sección 3.5 y `Practical 3.2', de Davison y Hinkley, 1997).
En primer lugar consideraremos únicamente la muestra de hombres:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Datos}
\FunctionTok{library}\NormalTok{(boot)}
\FunctionTok{data}\NormalTok{(channing)}
\CommentTok{\# Calcular edad (de partida o muerte) en años}
\NormalTok{channing}\SpecialCharTok{$}\NormalTok{age }\OtherTok{\textless{}{-}}\NormalTok{ (channing}\SpecialCharTok{$}\NormalTok{entry }\SpecialCharTok{+}\NormalTok{ channing}\SpecialCharTok{$}\NormalTok{time)}\SpecialCharTok{/}\DecValTok{12}
\CommentTok{\# Seleccionar hombres (y de paso hacer que \textasciigrave{}index = c(1, 2)\textasciigrave{} para \textasciigrave{}censboot()\textasciigrave{})}
\NormalTok{chan }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(channing, sex}\SpecialCharTok{==}\StringTok{"Male"}\NormalTok{, }\FunctionTok{c}\NormalTok{(age, cens))}

\CommentTok{\# Estimación supervivencia}
\FunctionTok{library}\NormalTok{(survival)}
\NormalTok{chan.F }\OtherTok{\textless{}{-}} \FunctionTok{survfit}\NormalTok{(}\FunctionTok{Surv}\NormalTok{(age, cens)}\SpecialCharTok{\textasciitilde{}}\DecValTok{1}\NormalTok{, }\AttributeTok{data =}\NormalTok{ chan)}
\NormalTok{chan.F}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Call: survfit(formula = Surv(age, cens) ~ 1, data = chan)
## 
##       n  events  median 0.95LCL 0.95UCL 
##    97.0    46.0    87.0    85.8    90.4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# plot(chan.F)}
\CommentTok{\# Estimaciones de interés}
\FunctionTok{with}\NormalTok{(chan.F, }
    \FunctionTok{c}\NormalTok{(}\AttributeTok{s75 =} \FunctionTok{max}\NormalTok{(surv[time }\SpecialCharTok{\textgreater{}} \DecValTok{75}\NormalTok{]), }\AttributeTok{s85 =} \FunctionTok{max}\NormalTok{(surv[time }\SpecialCharTok{\textgreater{}} \DecValTok{85}\NormalTok{]),}
      \AttributeTok{p75 =} \FunctionTok{min}\NormalTok{(time[surv }\SpecialCharTok{\textless{}=} \FloatTok{0.75}\NormalTok{]), }\AttributeTok{p50 =} \FunctionTok{min}\NormalTok{(time[surv }\SpecialCharTok{\textless{}=} \FloatTok{0.5}\NormalTok{])) }
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        s75        s85        p75        p50 
##  0.9160745  0.6347541 82.4166667 87.0000000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Bootstrap}
\CommentTok{\# library(boot)}
\NormalTok{chan.stat }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data) \{}
\NormalTok{    s }\OtherTok{\textless{}{-}} \FunctionTok{survfit}\NormalTok{(}\FunctionTok{Surv}\NormalTok{(age, cens)}\SpecialCharTok{\textasciitilde{}}\DecValTok{1}\NormalTok{, }\AttributeTok{data =}\NormalTok{ data)}
    \FunctionTok{with}\NormalTok{(s, }\FunctionTok{c}\NormalTok{(}\AttributeTok{s75 =} \FunctionTok{max}\NormalTok{(surv[time }\SpecialCharTok{\textgreater{}} \DecValTok{75}\NormalTok{]), }\AttributeTok{s85 =} \FunctionTok{max}\NormalTok{(surv[time }\SpecialCharTok{\textgreater{}} \DecValTok{85}\NormalTok{]),}
            \AttributeTok{p75 =} \FunctionTok{min}\NormalTok{(time[surv }\SpecialCharTok{\textless{}=} \FloatTok{0.75}\NormalTok{]), }\AttributeTok{p50 =} \FunctionTok{min}\NormalTok{(time[surv }\SpecialCharTok{\textless{}=} \FloatTok{0.5}\NormalTok{])))}
\NormalTok{\}}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{chan.boot }\OtherTok{\textless{}{-}} \FunctionTok{censboot}\NormalTok{(chan, chan.stat, }\AttributeTok{R =} \DecValTok{199}\NormalTok{) }\CommentTok{\# sim = "ordinary"}
\NormalTok{chan.boot}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## CASE RESAMPLING BOOTSTRAP FOR CENSORED DATA
## 
## 
## Call:
## censboot(data = chan, statistic = chan.stat, R = 199)
## 
## 
## Bootstrap Statistics :
##       original       bias    std. error
## t1*  0.9160745 -0.006995081  0.03133656
## t2*  0.6347541 -0.003538939  0.05595748
## t3* 82.4166667 -0.040619765  1.22517032
## t4* 87.0000000  0.195142379  1.07267425
\end{verbatim}

\hypertarget{otros-muxe9todos-de-remuestreo}{%
\subsection{Otros métodos de remuestreo}\label{otros-muxe9todos-de-remuestreo}}

La función \texttt{censboot()} implementa otros dos métodos de remuestreo,
\texttt{sim\ =\ c("cond",\ "weird")}, aunque en ambos casos hay que establecer en el
parámetro \texttt{F.surv} la estimación de Kaplan-Meier de la supervivencia y,
si \texttt{sim\ =\ "cond"}, la correspondiente a la variable censurante en \texttt{G.surv}.
Se recomienda estimarlas con la función \texttt{survfit()} del paquete \texttt{survival}
(ver Figura \ref{fig:survfit-f-g}):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Estimación supervivencia variable censurante}
\NormalTok{chan.G }\OtherTok{\textless{}{-}} \FunctionTok{survfit}\NormalTok{(}\FunctionTok{Surv}\NormalTok{(age, }\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{cens)}\SpecialCharTok{\textasciitilde{}}\DecValTok{1}\NormalTok{, }\AttributeTok{data =}\NormalTok{ chan)}
\CommentTok{\# Representación}
\NormalTok{old.par }\OtherTok{\textless{}{-}} \FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(chan.F, }\AttributeTok{main =} \StringTok{"Supervivencia (edad)"}\NormalTok{, }\AttributeTok{mark.time =} \ConstantTok{TRUE}\NormalTok{, }
    \AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\DecValTok{60}\NormalTok{, }\DecValTok{100}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(chan.G, }\AttributeTok{main =} \StringTok{"Supervivencia variable censurante (partida)"}\NormalTok{, }
     \AttributeTok{mark.time =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\DecValTok{60}\NormalTok{, }\DecValTok{100}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[!htb]

{\centering \includegraphics[width=0.7\linewidth]{08-dat_cen_files/figure-latex/survfit-f-g-1} 

}

\caption{Estimaciones de la supervivencia (izquierda; indicando los tiempos de las observaciones censuradas) y de la variable censurante (derecha; indicando los de las no censuradas).}\label{fig:survfit-f-g}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(old.par)}
\end{Highlighting}
\end{Shaded}

En el \emph{boostrap condicional} (\texttt{sim\ =\ "cond"}) se condiciona el muestreo al
patrón de censura observado (en lugar de fijarlo a \(\mathbf{1}\) como en el bootstrap de Reid; Sección \ref{bootcen-reid}).
El mecanismo es similar al del bootstrap obvio (Sección \ref{bootcen-obvio}):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Construir los estimadores de Kaplan-Meier de las distribuciones de
  la variable de interés, \(\hat{F}\left( t \right)\), y de la variable
  censurante, \(\hat{G}\left( t \right)\).
\item
  Para cada índice \(i=1,2,\ldots ,n\), generar \(X_i^{\ast}\) independientes
  con distribución \(\hat{F}\).
\item
  Si la \(i\)-ésima observación está censurada (\(\delta_i=0\)) se toma
  \(C_i^{\ast}=T_i\) y si no (\(\delta_i=1\)) se genera un valor de la estimación
  de la distribución de la variable censurante condicionada a \(C > T_i\):
  \[\hat G \left(\left. t \ \right\vert_{\ t > T_i} \right) 
  = \frac{\hat G(t) - \hat G(T_i)}{1- \hat G(T_i)}.\]
\item
  Definir \(T_i^{\ast}=\min \left\{ X_i^{\ast},C_i^{\ast}\right\}\) y
  \(\delta_i^{\ast}=\mathbf{1}_{\left\{ X_i^{\ast}\leq C_i^{\ast}\right\}}\), para \(i = 1, 2, \ldots, n\),
  y considerar la remuestra bootstrap
  \(\left( \mathbf{T}^{\ast},\boldsymbol{\delta}^{\ast}\right)\), con
  \(\mathbf{T}^{\ast}=\left( T_1^{\ast},T_2^{\ast}, \ldots, T_n^{\ast} \right)\) y \(\boldsymbol{\delta}^{\ast} = \left( \delta_1^{\ast}, \delta_2^{\ast},\ldots ,\delta_n^{\ast} \right)\).
\end{enumerate}

El otro método (\texttt{sim\ =\ "weird"}) es el denominado \emph{weird bootstrap}
(Andersen et al., 1993) que emplea la estimación de Nelson-Aalen de la
función de riesgo acumulada para generar los valores
(e.g.~Sección 3.5.2 de Davison y Hinkley, 1997).

El siguiente código muestra un ejemplo de la aplicación de ambos métodos:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{chan.boot2 }\OtherTok{\textless{}{-}} \FunctionTok{censboot}\NormalTok{(chan, chan.stat, }\AttributeTok{R =} \DecValTok{199}\NormalTok{, }\AttributeTok{F.surv =}\NormalTok{ chan.F, }
                  \AttributeTok{G.surv =}\NormalTok{ chan.G, }\AttributeTok{sim =} \StringTok{"cond"}\NormalTok{)}
\NormalTok{chan.boot2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## CONDITIONAL BOOTSTRAP FOR CENSORED DATA
## 
## 
## Call:
## censboot(data = chan, statistic = chan.stat, R = 199, F.surv = chan.F, 
##     G.surv = chan.G, sim = "cond")
## 
## 
## Bootstrap Statistics :
##       original       bias    std. error
## t1*  0.9160745  0.002340590  0.02796666
## t2*  0.6347541 -0.001057025  0.05382609
## t3* 82.4166667  0.019681742  1.21793756
## t4* 87.0000000  0.286013400  0.95041994
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{chan.boot3 }\OtherTok{\textless{}{-}} \FunctionTok{censboot}\NormalTok{(chan, chan.stat, }\AttributeTok{R =} \DecValTok{199}\NormalTok{, }\AttributeTok{F.surv =}\NormalTok{ chan.F, }
                  \AttributeTok{sim =} \StringTok{"weird"}\NormalTok{)}
\NormalTok{chan.boot3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## WEIRD BOOTSTRAP FOR CENSORED DATA
## 
## 
## Call:
## censboot(data = chan, statistic = chan.stat, R = 199, F.surv = chan.F, 
##     sim = "weird")
## 
## 
## Bootstrap Statistics :
##       original        bias    std. error
## t1*  0.9160745 -4.082326e-05  0.02825475
## t2*  0.6347541  1.639197e-03  0.05086460
## t3* 82.4166667  2.303183e-02  1.14079354
## t4* 87.0000000  2.458124e-01  0.96511648
\end{verbatim}

\hypertarget{ejercicios}{%
\section{Ejercicios}\label{ejercicios}}

\begin{exercise}[Bootstrap censurado por estratos]
\protect\hypertarget{exr:censboot-strata-ej}{}{\label{exr:censboot-strata-ej} \iffalse (Bootstrap censurado por estratos) \fi{} }
Analizar el conjunto de datos \texttt{channing} completo, teniendo en cuenta el sexo como estrato
(i.e.~\texttt{Surv(age,\ cens)\ \textasciitilde{}\ sex} y \texttt{strata\ =\ chan\$sex})
\end{exercise}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Datos}
\FunctionTok{data}\NormalTok{(channing)}
\CommentTok{\# Calcular edad (de partida o muerte) en años}
\NormalTok{channing}\SpecialCharTok{$}\NormalTok{age }\OtherTok{\textless{}{-}}\NormalTok{ (channing}\SpecialCharTok{$}\NormalTok{entry }\SpecialCharTok{+}\NormalTok{ channing}\SpecialCharTok{$}\NormalTok{time)}\SpecialCharTok{/}\DecValTok{12}
\CommentTok{\# Seleccionar variables}
\NormalTok{chan }\OtherTok{\textless{}{-}}\NormalTok{channing[}\FunctionTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{, }\StringTok{"cens"}\NormalTok{, }\StringTok{"sex"}\NormalTok{)]}

\CommentTok{\# Estimación supervivencia}
\FunctionTok{library}\NormalTok{(survival)}
\NormalTok{chan.F }\OtherTok{\textless{}{-}} \FunctionTok{survfit}\NormalTok{(}\FunctionTok{Surv}\NormalTok{(age, cens) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ sex, }\AttributeTok{data =}\NormalTok{ chan)}
\NormalTok{chan.F}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Call: survfit(formula = Surv(age, cens) ~ sex, data = chan)
## 
##              n events median 0.95LCL 0.95UCL
## sex=Female 365    130     88    86.7    89.5
## sex=Male    97     46     87    85.8    90.4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(chan.F, }\AttributeTok{lty =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{, }\AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\DecValTok{60}\NormalTok{, }\DecValTok{100}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[!htb]

{\centering \includegraphics[width=0.7\linewidth]{08-dat_cen_files/figure-latex/surv-strata-1} 

}

\caption{Estimaciones de la supervivencia.}\label{fig:surv-strata}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res }\OtherTok{\textless{}{-}} \FunctionTok{summary}\NormalTok{(chan.F)}
\CommentTok{\# res}
\FunctionTok{str}\NormalTok{(res)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## List of 19
##  $ n            : int [1:2] 365 97
##  $ time         : num [1:146] 67 68.5 69.2 70 70.4 ...
##  $ n.risk       : num [1:146] 364 359 355 353 352 346 344 340 335 334 ...
##  $ n.event      : num [1:146] 1 1 1 1 1 1 1 1 1 1 ...
##  $ n.censor     : num [1:146] 2 3 3 1 0 6 0 3 4 0 ...
##  $ surv         : num [1:146] 0.997 0.994 0.992 0.989 0.986 ...
##  $ std.err      : num [1:146] 0.00274 0.0039 0.00479 0.00554 0.00619 ...
##  $ cumhaz       : num [1:146] 0.00275 0.00553 0.00835 0.01118 0.01402 ...
##  $ std.chaz     : num [1:146] 0.00275 0.00391 0.00482 0.00559 0.00627 ...
##  $ strata       : Factor w/ 2 levels "sex=Female","sex=Male": 1 1 1 1 1 1 1 1 1 1 ...
##  $ type         : chr "right"
##  $ logse        : logi TRUE
##  $ conf.int     : num 0.95
##  $ conf.type    : chr "log"
##  $ lower        : num [1:146] 0.992 0.987 0.982 0.978 0.974 ...
##  $ upper        : num [1:146] 1 1 1 1 0.998 ...
##  $ call         : language survfit(formula = Surv(age, cens) ~ sex, data = chan)
##  $ table        : num [1:2, 1:9] 365 97 365 97 365 ...
##   ..- attr(*, "dimnames")=List of 2
##   .. ..$ : chr [1:2] "sex=Female" "sex=Male"
##   .. ..$ : chr [1:9] "records" "n.max" "n.start" "events" ...
##  $ rmean.endtime: num [1:2] 101 101
##  - attr(*, "class")= chr "summary.survfit"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Estimaciones de interés}
\NormalTok{res}\SpecialCharTok{$}\NormalTok{table[, }\FunctionTok{c}\NormalTok{(}\StringTok{"*rmean"}\NormalTok{, }\StringTok{"median"}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##              *rmean median
## sex=Female 88.46153     88
## sex=Male   86.89935     87
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{as.numeric}\NormalTok{(res}\SpecialCharTok{$}\NormalTok{table[, }\FunctionTok{c}\NormalTok{(}\StringTok{"*rmean"}\NormalTok{, }\StringTok{"median"}\NormalTok{)])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 88.46153 86.89935 88.00000 87.00000
\end{verbatim}

\begin{exercise}[Bootstrap censurado con riesgo proporcional de Cox]
\protect\hypertarget{exr:censboot-cox-ej}{}{\label{exr:censboot-cox-ej} \iffalse (Bootstrap censurado con riesgo proporcional de Cox) \fi{} }
Reproducir el ejemplo en Canty (2002, \href{http://cran.fhcrc.org/doc/Rnews/Rnews_2002-3.pdf}{Rnews\_2002-3}) del modelo de riesgo proporcional de Cox (Cox, 1972):
\end{exercise}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Datos}
\FunctionTok{data}\NormalTok{(melanoma)}
\NormalTok{mel }\OtherTok{\textless{}{-}}\NormalTok{ melanoma[melanoma}\SpecialCharTok{$}\NormalTok{ulcer }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, ]}
\NormalTok{mel}\SpecialCharTok{$}\NormalTok{cens }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{*}\NormalTok{ (mel}\SpecialCharTok{$}\NormalTok{status }\SpecialCharTok{==} \DecValTok{1}\NormalTok{)}
\CommentTok{\# Estimación supervivencia}
\FunctionTok{library}\NormalTok{(survival)}
\CommentTok{\# Modelo de riesgo proporcional de Cox}
\NormalTok{mel.cox }\OtherTok{\textless{}{-}} \FunctionTok{coxph}\NormalTok{(}\FunctionTok{Surv}\NormalTok{(time, cens) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ thickness, }\AttributeTok{data =}\NormalTok{ mel)}
\NormalTok{mel.cox}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Call:
## coxph(formula = Surv(time, cens) ~ thickness, data = mel)
## 
##              coef exp(coef) se(coef)    z      p
## thickness 0.09968   1.10481  0.04052 2.46 0.0139
## 
## Likelihood ratio test=5  on 1 df, p=0.02541
## n= 90, number of events= 41
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# summary(mel.cox)}
\CommentTok{\# Estadísticos de interés}
\NormalTok{mel.cox}\SpecialCharTok{$}\NormalTok{coefficients}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  thickness 
## 0.09967665
\end{verbatim}

\hypertarget{bootdep}{%
\chapter{El Bootstrap con datos dependientes}\label{bootdep}}

En este capítulo se presentan gran cantidad de métodos bootstrap para
realizar inferencia, así como predicción, en el contexto de datos
dependientes. En primer lugar se hace una introducción a las condiciones
habituales de dependencia y a los modelo paramétricos de dependencia,
para luego centrarse en los métodos de remuestreo en ambos contextos.

En cada uno de los dos contextos (estimación y predicción) se estudiarán
dos situaciones drásticamente diferentes. En la primera de ellas
consideraremos modelos en los que la estructura de dependencia está
explícitamente modelizada (normalmente a través de una ecuación de
autorregresión), mientras que la segunda trata el caso en que no existe
ninguna especificación explícita de la estructura de dependencia
(simplemente se asumen condiciones mixing, por ejemplo). Una revisión
sobre los resultados principales puede verse en Cao (1999).

\hypertarget{introducciuxf3n-a-las-condiciones-de-dependencia-y-modelos-habituales-de-datos-dependientes}{%
\section{Introducción a las condiciones de dependencia y modelos habituales de datos dependientes}\label{introducciuxf3n-a-las-condiciones-de-dependencia-y-modelos-habituales-de-datos-dependientes}}

\hypertarget{situaciones-de-dependencia-general}{%
\subsection{Situaciones de dependencia general}\label{situaciones-de-dependencia-general}}

Consideramos un proceso estocástico en tiempo discreto y con espacio de
estados continuo (p.~ej. \(\mathbb{R}\)), \(\left\{X_{t}\right\}_{t\in \mathbb{Z}}\),
del cual observamos parte de su trayectoria: \(\left( X_1,X_2,\ldots ,X_n \right)\),
es decir una muestra de datos dependientes.
Este tipo de procesos estocásticos suelen llamarse series temporales.

Normalmente supondremos que el proceso
\(\left\{ X_{t}\right\}_{t\in \mathbb{Z}}\) es estacionario.
En ocasiones se requerirá además que sea fuertemente mixing:
\[\sup_{A\in \mathcal{F}_1^{n},B\in \mathcal{F}_{n+k}^{\infty }}\left\vert
P\left( A\cap B \right) -P\left( A \right) P(B) \right\vert \leq
\alpha _{k}\text{, con }\alpha _{k}\rightarrow 0,\]
o bien uniformemente mixing:
\[\left\vert P\left( A\cap B \right) -P\left( A \right) P(B) \right\vert 
\leq \phi _{k}P\left( A \right) \text{, }\forall A\in \mathcal{F}_1^{n},
\forall B\in \mathcal{F}_{n+k}^{\infty }\text{, con }\phi_{k}\rightarrow 0,\]
siendo \(\mathcal{F}_{s}^{t}\) la \(\sigma\)-algebra generada por las variables aleatorias
\(X_{s},\ldots ,X_{t}\).

Estas condiciones establecen que la dependencia entre las variables
aleatorias que conforman las observaciones de la muestra se atenúa a
medida que sus instantes temporales se distancian.

\hypertarget{modelos-paramuxe9tricos-de-dependencia}{%
\subsection{Modelos paramétricos de dependencia}\label{modelos-paramuxe9tricos-de-dependencia}}

Los modelos paramétricos más habituales para datos dependientes y
estacionarios son los autorregresivos (\(AR\left( p \right)\)):
\[X_{t}=\phi _1X_{t-1}+\phi _2X_{t-2}+\cdots +\phi _{p}X_{t-p}+a_{t}
\text{, }t\in \mathbb{Z},\]
de medias móviles (\(MA\left( q \right)\)):
\[X_{t}=a_{t}-\theta _1a_{t-1}-\theta _2a_{t-2}-\cdots -\theta _{p}a_{t-q}
\text{, }t\in \mathbb{Z}\]
y la mezcla de ambos (\(ARMA\left( p,q \right)\)):
\[\begin{aligned}
X_{t} =&\ \phi _1X_{t-1}+\phi _2X_{t-2}+\cdots +\phi _{p}X_{t-p} \\
&+a_{t}-\theta _1a_{t-1}-\theta _2a_{t-2}-\cdots -\theta _{q}a_{t-q},\end{aligned}\]
En las anteriores expresiones los
\(\left\{ a_{t}\right\} _{t\in \mathbb{Z}}\) representan una sucesión de
variables independientes con la misma distribución (ruido blanco),
habitualmente con distribución normal.

\hypertarget{el-bootstrap-en-la-estimaciuxf3n-con-datos-dependientes}{%
\section{El bootstrap en la estimación con datos dependientes}\label{el-bootstrap-en-la-estimaciuxf3n-con-datos-dependientes}}

El objetivo de esta sección es mostrar distintos métodos de remuestreo
para realizar inferencia sobre los parámetros de una serie temporal.
Comenzaremos tratando los modelos de dependencia explícita para luego
abordar la situación en que tan sólo existen condiciones generales de
dependencia.

\hypertarget{modelos-paramuxe9tricos-de-dependencia-1}{%
\subsection{Modelos paramétricos de dependencia}\label{modelos-paramuxe9tricos-de-dependencia-1}}

Consideremos uno de los casos más simples, dado por el modelo \(AR(p)\):
\[X_{t}=\phi _1X_{t-1}+\phi _2X_{t-2}+\cdots +\phi _{p}X_{t-p}+a_{t},\]donde
\(\{a_{t}\}\) es una sucesión de variables aleatorias independientes de
media cero (ruido blanco), de tal forma que \(a_{t}\) es independiente del
pasado de \(X_{t}\): \(\{X_{t-1},X_{t-2},\ldots \}\).

En el contexto de estimación error cuadrático medio de predicción
(PMSE), Stine (1987) propone un método bootstrap que mejora el estimador
clásico de sustitución del PMSE del mejor predictor lineal estimado (ver
capítulo 2 de Fuller (1976)), cuando la distribución del ruido blanco no
tiene porqué ser normal. El método procede como sigue:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Obtener una estimación de los coeficientes de autorregresión:
  \[\widehat{\phi}_1,\widehat{\phi}_2,\ldots ,\widehat{\phi}_{p}.\]En
  el artículo de Stine estos estimadores se obtienen por el método de
  mínimos cuadrados.
\item
  Calcular los residuos (para aquellos índices que sea posible):
  \[\widehat{a}_{t}=X_{t}-\widehat{\phi}_1X_{t-1}-\widehat{\phi}
  _2X_{t-2}+\cdots -\widehat{\phi}_{p}X_{t-p},\quad t=p+1,p+2,\ldots ,n.\]
  Estos valores son sustitutos de los errores inobservables \(a_{t}\).
\item
  Calcular la distribución empírica de los residuos corregidos
  (recentrados y reescalados):
  \[F_n^{\widehat{a}}(x)=\frac{1}{n-p}\sum_{t=p+1}^{n}1_{\{\widehat{a}_{t}^{\prime}\leq x\}},\]
  donde \(\widehat{a}_{t}^{\prime}=\widehat{a}_{t}-\bar{a}\) y
  \(\bar{a}=\frac{1}{n-p}\sum_{t=p+1}^{n}\widehat{a}_{t}\).
\item
  Arrojar \(a_{t}^{\ast}\), \(t=1,2,\ldots ,n+k\) observaciones iid con
  distribución \(F_n^{\widehat{a}}\).
\item
  Fijar los primeros \(p\) valores de las réplicas bootstrap de la serie:
  \[X_1^{\ast},X_2^{\ast},\ldots ,X_{p}^{\ast}\]
  igual a cero (o con igual probabilidad de los \(n-p+1\) bloques posibles
  de observaciones consecutivas de la serie original) y definir:
  \[X_{t}^{\ast}=\widehat{\phi}_1X_{t-1}^{\ast}+\widehat{\phi}_2
  X_{t-2}^{\ast}+\cdots +\widehat{\phi}_{p}X_{t-p}^{\ast}+a_{t}^{\ast},
  \quad t=p+1,\ldots ,n+k.\]
\item
  A partir de la remuestra bootstrap (hasta el instante \(n\)), calcular
  las versiones bootstrap,
  \(\widehat{\phi}_1^{\ast},\widehat{\phi} _2^{\ast},\ldots ,\widehat{\phi}_{p}^{\ast}\), de los
  estimadores y obtener \(\widehat{X}_{n+k}^{\ast}\), la predicción de
  \(X_{n+k}^{\ast}\), usando la versión bootstrap de los estimadores de
  los parámetros y las últimas observaciones de la remuestra
  bootstrap.
\item
  Aproximar el PMSE mediante su análogo bootstrap:
  \[PMSE^{\ast}=E^{\ast}\left[ \left( \widehat{X}_{n+k}^{\ast}-X_{n+k}^{\ast
  } \right)^2\right] .\]
\end{enumerate}

Ferretti y Romo (1996) demuestran la consistencia de un bootstrap basado
en los residuos (en el sentido de convergencia débil de la distribución
bootstrap) para contrastes de raíz unitaria en series temporales del
tipo \(AR(1)\), tanto en el caso de errores iid como cuando el error sigue
también un modelo \(AR(1)\). Heimann y Kreiss (1996) dan un resultado
similar, sólo para el caso de errores iid, cuando el tamaño muestral de
las remuestras bootstrap es \(m_n\), de forma que \(\frac{m_n}{n} \rightarrow 0\) (subremuestreo).

Las ideas generales sobre el bootstrap para modelos autorregresivos
pueden extenderse al bootstrap de series temporales autorregresivas y de
media móvil. Consideremos un modelo \(ARMA(p,q)\): \[\begin{aligned}
X_{t} =&\ \phi _1X_{t-1}+\phi _2X_{t-2}+\cdots +\phi _{p}X_{t-p} \\
&+a_{t}-\theta _1a_{t-1}-\theta _2a_{t-2}-\cdots -\theta _{q}a_{t-q},\end{aligned}\]o,
equivalentemente,
\[\phi (B)X_{t}=\theta (B)a_{t},\]
donde
\[\phi (B)=1-\phi _1B-\phi _2B^2-\cdots -\phi _{p}B^{p}\\ 
\theta(B)=1-\theta _1B-\theta _2B^2-\cdots -\theta _{q}B^{q}\]
y \(B\) es el operador retardo: \(BX_{t}=X_{t-1}\).
La diferencia principal con
respecto al caso autorregresivo es que ahora se necesitan estimar los
coeficientes la parte de media móvil, al objeto de calcular los
residuos, \(\widehat{a}_{t}\).
Así, el algoritmo bootstrap para una serie \(AR(p)\) puede
adaptarse a este caso de manera inmediata.
En este contexto, Kreiss y Franke (1992) usan la representación\\
\(MA(\infty )\) del proceso de error en términos de la series original,
\[a_{t}=\theta (B)^{-1}\phi(B) X_{t},\]
para construir los residuos
(utilizando los parámetros estimados en la fórmula anterior) y demuestran
la validez asintótica del bootstrap (en el sentido de la distancia de Mallows)
para aproximar la distribución en el muestreo del \(M\)-estimador de los
parámetros de un modelo \(ARMA(p,q)\).

Paparoditis (1996) demuestra la validez del bootstrap para la inferencia
acerca de los parámetros de un proceso \(ARMA\) multidimensional de orden
infinito. El autor propone arrojar réplicas bootstrap de modelos \(ARMA\)
de orden finito creciente, de forma que ese orden tienda a infinito a
cierta tasa, según crece el tamaño muestral. Bühlmann (1997) desarrolla
ideas semejantes en el contexto de procesos \(AR\left( \infty \right)\),
introduciendo el llamado \emph{sieve bootstrap}. Este método se ha extendido
estimación no paramétrica de la regresión cuando la variable explicativa
sigue un modelo \(AR\left( \infty \right)\) (ver Bühlmann (1998)).

\hypertarget{situaciones-de-dependencia-general-1}{%
\subsection{Situaciones de dependencia general}\label{situaciones-de-dependencia-general-1}}

En esta sección se estudia el caso en que no se asume ningún tipo de
estructura autorregresiva sobre el proceso estocástico. De hecho,
asumiremos condiciones generales de dependencia, como condiciones mixing
o de \(m\)-dependencia, por ejemplo.

El problema de no tener una ecuación explícita que relacione el valor
actual de la serie con sus valores pasados provoca que no sea posible
diseñar un plan de remuestreo a partir de un modelo de dependencia
explícita.

\hypertarget{el-bootstrap-por-bloques}{%
\subsection{El bootstrap por bloques}\label{el-bootstrap-por-bloques}}

La primeras propuestas para evitar el problema de carecer de una
expresión explícita para modelizar la dependencia corresponden a Künsch
(1989) y Liu y Singh (1992), que propusieron de forma independiente el
llamado bootstrap por bloques (moving blocks bootstrap o MBB). El método
procede del siguiente modo:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Fijar un entero positivo, \(b\), el tamaño del bloque, y tomar \(k\)
  igual al menor entero mayor o igual que \(\frac{n}{b}\).
\item
  Definir los bloques (o submuestras): \(B_{i,b}=(X_i,X_{i+1},\ldots ,X_{i+b-1})\), o simplemente \(B_i\), para \(i=1,2,\ldots ,q\)
  (\(q=n-b+1\))\(.\)
\item
  Arrojar \(k\) observaciones (bloques), \(\xi _1,\xi _2,\ldots ,\xi _{k}\), con distribución equiprobable sobre el conjunto de posibles
  bloques: \(\{B_1,B_2,\ldots ,B_{q}\}\). Cada \(\xi _i\) es un
  vector \(b\)-dimensional \((\xi _{i,1},\xi _{i,2},\ldots ,\xi _{i,b})\).
\item
  Definir \(\mathbf{X}^{\ast}\) como el vector formado por las \(n\)
  primeras componentes de
  \[(\xi _{1,1},\xi _{1,2},\ldots ,\xi _{1,b},\xi _{2,1},\xi _{2,2},\ldots ,\xi
  _{2,b},\ldots ,\xi _{k,1},\xi _{k,2},\ldots ,\xi _{k,b}).\]
\end{enumerate}

Si tomamos \(b=1\), entonces \(k=n\) y se obtiene el bootstrap ordinario.
Por otra parte, si \(b=n\), tenemos \(k=1\) y se obtiene el remuestreo
degenerado, ya que todas las réplicas bootstrap coincidirían con la
muestra original.

Künsch (1989) y Liu y Singh (1992) demuestran la validez asintótica de
este método bajo condiciones poco restrictivas sobre el grado de
dependencia y el tamaño del bloque. Por ejemplo, Liu y Singh (1992)
demuestran que si el proceso estocástico es \(m\)-dependiente (i.e.,
\((X_{t},X_{t+1},\ldots )\) y \((X_{s},X_{s-1},\ldots )\) son independientes
siempre que \(s+m<t\)), si \(T\) es un funcional dos veces Frechet
diferenciable y el tamaño del bloque satisface \(b\rightarrow \infty\) y
\(b\log n/n\rightarrow 0\), entonces
\[\sup_{x\in \mathbb{R}}\left\vert P^{\ast}\left\{ \sqrt{n}\left(
T(F_n^{\ast})-T\left( F_n \right) \right) \leq x\right\} -P\left\{ \sqrt{
n}\left( T(F_n)-T\left( F \right) \right) \leq x\right\} \right\vert
\rightarrow 0\mathrm{,}\]en probabilidad. Naik-Nimbalkar y Rajarshi
(1994) demuestran la consistencia del proceso empírico MBB bajo la
condición de que \(b=O(n^{1/2-\varepsilon })\), para algún
\(\varepsilon \in (\frac{1}{4},\frac{1}{2})\). Bühlmann (1994) lo extiende
al caso multivariante y debilita la condición sobre \(\varepsilon\),
siendo \(\varepsilon \in (0,\frac{1}{2})\).

Carlstein, Do, Hall, Hesterberg y Künsch (1998) propusieron una
modificación del MBB. Su idea consiste en seleccionar las remuestras de
bloques de acuerdo a una cadena de Markov. El primer bloque de la
remuestra bootstrap se genera igual que para el MBB ordinario. Una vez
que se se seleccionado en la remuestra el bloque \(B_i\), el siguiente
bloque de la remuestra bootstrap se elige dentro de todos los posibles
bloques, \(B_j\), poniendo más probabilidad a aquellos que son
precedidos por un bloque,
\(B_{j-1}\), cuyo último valor, \(X_{j+b-2}\), es más cercano al último
valor, \(X_{i+b-1}\), del bloque \(B_i\). En el caso \(j=1\), esta regla no
tiene sentido, ya que no existe un bloque anterior al \(B_1\), así que,
en ese caso los autores proponen hacer que la probabilidad dependa de la
distancia entre \(X_1\) (el primer valor del bloque \(B_1\)) y el valor
siguiente al último del bloque \(B_i\), es decir \(X_{i+b}\). De nuevo
esto sólo es posible si \(i<q\). Si \(i=q\) usan \(X_1\) en lugar de
\(X_{i+b}\). Estas probabilidades se calculan usando pesos de tipo núcleo.
Estos autores demuestran la consistencia de esta versión del MBB para el
estimador bootstrap de la varianza de la media muestral.

\hypertarget{elecciuxf3n-de-b}{%
\subsection{\texorpdfstring{Elección de \(b\)}{Elección de b}}\label{elecciuxf3n-de-b}}

Un asunto importante en el método bootstrap por bloques es la elección
del tamaño del bloque, \(b\). Hall, Horowitz y Jing (1995) considera este
problema en el contexto de la estimación bootstrap del sesgo y la
varianza. Obtienen una expresión asintótica para el error cuadrático
medio:
\[n^{-2}(C_1b^{-2}+C_2n^{-1}b),\]
donde \(C_1\) y \(C_2\) son
constantes desconocidas que dependen del problema de estimación del que
se trate. Está claro entonces que el tamaño óptimo del bloque (en el
sentido del error cuadrático medio) es de orden \(n^{1/3}\).

Un resultado importante de utilidad para probar la validez del MBB en
muchos contextos es el dado por Radulović (1996). Este autor demuestra
que siempre que una sucesión de variables aleatorias fuertemente mixing
satisface el Teorema Central del Límite, dicho resultado también es
válido para la versión bootstrap por bloques.

\hypertarget{el-bootstrap-estacionario}{%
\subsection{El bootstrap estacionario}\label{el-bootstrap-estacionario}}

Consideremos el bootstrap por bloques para una muestra,
\((X_1,X_2,\ldots ,X_n)\), de tamaño \(n=100\) y el tamaño del bloque \(b=10\). Podemos
calcular fácilmente las distribuciones bootstrap conjuntas de
\((X_{10}^{\ast},X_{11}^{\ast})\) y \((X_{9}^{\ast},X_{10}^{\ast})\):
\[\begin{aligned}
P^{\ast}\left\{ (X_{10}^{\ast},X_{11}^{\ast})=(X_i,X_j)\right\} =
\frac{1}{91^2},\quad \hbox{para}\quad i &= 10,11,\ldots ,100; \\
j &= 1,2,\ldots ,91 \\
P^{\ast}\left\{ (X_{9}^{\ast},X_{10}^{\ast})=(X_i,X_j)\right\} =\frac{
1}{91},\quad \hbox{para}\quad i &= 9,10,\ldots ,99;\,\,j=i+1.\end{aligned}\]Como
estas distribuciones bootstrap son diferentes entonces el MBB no es
estacionario.

Con el fin de remediar la falta de estacionariedad del MBB, Politis y
Romano (1994a) proponen el llamado bootstrap estacionario (stationary
bootstrap, SB). El método necesita de la elección de un número \(p\in \lbrack 0,1]\) y puede presentarse de dos formas equivalentes:

\textbf{SB1:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Arrojar \(X_1^{\ast}\) de \(F_n\), la distribución empírica
  construida con las muestra \((X_1,X_2,\ldots ,X_n).\)
\item
  Una vez que se ha arrojado el valor \(X_i^{\ast}=X_j\) (para
  algún \(j\in \{1,2,\ldots ,n-1\}\)) con \(i<n\), se define la siguiente
  observación bootstrap, \(X_{i+1}^{\ast}\), como \(X_{j+1}\), con
  probabilidad \(1-p\) y arrojada de la función de distribución empírica
  de la muestra, con probabilidad \(p\). En el caso \(j=n\), la
  observación \(X_{j+1}\) se reemplaza por \(X_1\).
\end{enumerate}

\textbf{SB2:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Definir los bloques circulares \(B_{i,b}=(X_i,X_{i+1},\ldots ,X_{i+b-1})\) con \(b\in \mathbb{N}\), \(i=1,2,\ldots ,n\) y
  \(X_{t}=X_{\left(\left( t-1 \right) \mathrm{mod\ }n \right) +1}\) si \(t>n\).
\item
  Arrojar realizaciones iid, \(L_1,L_2,\ldots\), con distribución
  geométrica de parámetro \(p\), i.e.
  \[P(L_1=m)=p(1-p)^{m-1},m=1,2,\ldots\]
\item
  Obtener enteros aleatorios, \(I_1,I_2,\ldots\), con distribución
  equiprobable sobre el conjunto \(\{1,2,\ldots ,n\}\).
\item
  Definir \(X_1^{\ast},X_2^{\ast},\ldots ,X_n^{\ast}\) como los
  \(n\) primeros valores obtenidos al unir los bloques
  \(B_{I_1,L_1},B_{I_2,L_2},\ldots\)
\end{enumerate}

A continuación se comentan algunos aspectos interesantes en relación con
el SB.

\begin{itemize}
\item
  El número mínimo de bloques necesario, \(k\), en el método de
  remuestreo SB2, de forma que el conjunto de bloques
  \(B_{I_1,L_1},B_{I_2,L_2},\ldots ,B_{I_{k},L_{k}}\) tenga, al
  menos, \(n\) observaciones, coincide con el menor entero \(k\) para el
  cual \(\sum_{i=1}^{k}L_i\geq n\).
\item
  Eligiendo \(p=1\) se tiene el bootstrap clásico. La elección
  \(p=0\) corresponde a una permutación circular aleatoria de la
  muestra, que conduce a una distribución bootstrap degenerada si el
  estadístico es funcional (i.e., si es sólo función de la
  distribución empírica, pero no depende del orden de los datos).
\item
  Condicionalmente a la muestra observada, el proceso bootstrap,
  \(\{X_i^{\ast}\}\), es estacionario. Más aún, si no hay datos
  empatados, entonces el proceso bootstrap es un proceso de Markov. En
  general, es un proceso markoviano de orden \(r+1\), donde
  \[r=\max \left\{ b\in \mathbb{N}\,/\,\,\exists i,j,\,\,i\neq j\quad \hbox{con}
  \quad B_{i,b}=B_{j,b}\right\} .\]
\item
  Observando el esquema de remuestreo SB2 resulta fácil generalizar el
  método a casos en los que la distribución de \(L_i\) no es
  geométrica y la distribución de los \(I_i\) no tiene porqué ser
  equiprobable. En tales casos, debe ponerse mucho cuidado en la
  elección de esas distribuciones para no destruir la estacionariedad
  del proceso bootstrap. Con esta generalización del remuestreo SB2,
  el MBB puede pensarse como un caso particular, tomando
  \[\begin{aligned}
  P(L_i &= m)=\left\{ 
  \begin{array}{lll}
  1 & \mathrm{si} & m=b \\ 
  0 & \mathrm{si} & m\neq b
  \end{array}
     \right. \\
  P(I_i &= j)=\left\{ 
  \begin{array}{lll}
  1/q & \mathrm{si} & j=1,2,\ldots ,q \\ 
  0 & \mathrm{si} & j=q+1,q+2,\ldots ,n
  \end{array}
     \right. \\
  \quad \hbox{con}\quad q&= n-b+1.\end{aligned}\]
\item
  Como el tamaño medio del bloque en el SB es \(\frac{1}{p}\), en cierto
  sentido el valor \(p\) juega el papel inverso del tamaño del bloque en
  el MBB (\(p=1\) es comparable con \(b=1\) y \(p=0\) con
  \(b\rightarrow \infty\)).
\end{itemize}

Dado un proceso estocástico estrictamente estacionario con función de
autocovarianza \(\gamma\), cumpliendo \(\gamma (0)+\sum_{r}\left\vert r\gamma (r)\right\vert <\infty ,\) con momento finito de orden \(d+2\)
(para algún \(d>0\)) y la siguiente condición para los coeficientes
mixing: \[\sum_{k}\alpha _{k}^{\frac{d}{d+2}}<\infty ,\]Politis y Romano
(1994a) demostraron la validez asintótica del bootstrap estacionario:
\[\sup_{x\in \mathbb{R}}\left\vert P^{\ast}\left\{ \sqrt{n}(\bar{X}_n^{\ast
}-\bar{X}_n)\leq x\right\} -P\left\{ \sqrt{n}(\bar{X}_n-\mu )\leq
x\right\} \right\vert \rightarrow 0,\]en probabilidad, siempre que
\(p\rightarrow 0\) y \(np\rightarrow \infty\). Estos autores también dan
una idea acerca de cómo generalizar este resultado a estadísticos
funcionales, \(T(F_n)\), donde \(T\) es un funcional Frechet
diferenciable. Politis y Romano (1994c) también demostraron que el
método funciona para una amplia clase de estimadores, incluyendo los de
mínima distancia.

\hypertarget{el-muxe9todo-del-submuestreo}{%
\subsection{El método del submuestreo}\label{el-muxe9todo-del-submuestreo}}

Politis y Romano (1994b) proporcionan un método bootstrap que es válido
bajo condiciones minimales. Estos autores presentan dos versiones de
este método: una para datos independientes y otra para datos
dependientes.

Para enunciar el método del submuestreo de forma unificada consideremos
las observaciones, \(X_1,X_2,\ldots ,X_n\), que provienen o bien de
(a) variables aleatorias iid con distribución \(F\) o (b) un proceso
estocástico fuertemente mixing. Consideremos un parámetro
\(\theta =\theta (F)\), \(T_n=T_n(X_1,X_2,\ldots ,X_n)\) un
estimador de él, y \(J_n(\cdot ,F)\) la función de distribución en el
muestreo de \(\tau _n(T_n-\theta )\). Se fija un entero \(b<n\) y se
define:

\begin{itemize}
\item
  en el caso iid, \(S_{n,i}=T_{b}(Y_i),\) \(i=1,2,\ldots ,N\), donde
  \(Y_1,Y_2,\ldots ,Y_n\) son todas las \(N=\binom{n}{b}\) posibles
  submuestras de tamaño \(b\) (sin reemplazamiento) de la muestra
  original.
\item
  en el caso de datos dependientes, \(S_{n,i}=T_{b}(B_{i,b})\),
  \(i=1,2,\ldots ,N\), donde \(B_{i,b},\) \(i=1,2,\ldots ,N\), con \(N=n-b+1\),
  son todos los posibles bloques de tamaño \(b\).
\end{itemize}

Este método propone usar la función de distribución empírica de los
valores \(\tau _{b}(S_{n,i}-T_n)\),
\[L_n(x)=\frac{1}{N}\sum_{i=1}^{N}1_{\{\tau _{b}(S_{n,i}-T_n)\leq x\}}\]
como aproximación de la distribución en el muestreo de \(\tau_n(T_n-\theta )\).
El resultado demostrado por Politis y Romano (1994b) afirma que siempre que
\(\tau _{b}/\tau _n\rightarrow 0\), \(b \rightarrow \infty\) y
\(b/n\rightarrow 0\), la condición \(\tau _n(T_n-\theta )\overset{d}{\rightarrow}J(\cdot ,F)\) implica que \(L_n(x)\rightarrow J(x,F)\) para cada \(x\), punto de continuidad de \(J(\cdot ,F)\) y
\(\left\Vert L_n(\cdot )-J_n(\cdot ,F)\right\Vert _{\infty }\rightarrow 0\) en
probabilidad (si \(J(\cdot ,F)\) es continua). A grandes rasgos este
resultado garantiza que, bajo condiciones minimales sobre el tamaño del
bloque, el método del submuestreo es siempre asintóticamente válido,
siempre que el estadístico de interés tenga una distribución límite.

\hypertarget{el-bootstrap-para-la-predicciuxf3n-con-datos-dependientes}{%
\section{El bootstrap para la predicción con datos dependientes}\label{el-bootstrap-para-la-predicciuxf3n-con-datos-dependientes}}

Dado un proceso estocástico en tiempo discreto,
\(\{X_{t}\}_{t\in \mathbb{ Z}}\), un problema importante en este contexto es predecir un valor
futuro del proceso. Habiendo observado una trayectoria del proceso,
hasta el tiempo \(n\): \(X_1,X_2,\ldots ,X_n\), la cuestión es
encontrar un predictor, tan preciso como sea posible, para el valor del
proceso a \(k\) retardos,
\(X_{n+k}\). Puede construirse un predictor puntual o un intervalo de
predicción, que es típicamente más informativo.

\hypertarget{modelos-de-dependencia-paramuxe9trica}{%
\subsection{Modelos de dependencia paramétrica}\label{modelos-de-dependencia-paramuxe9trica}}

Al igual que en el caso de estimación, cuando la estructura de
dependencia sigue un modelo paramétrico, esta información puede usarse
para adaptar el bootstrap ordinario al contexto de predicción. La mayor
parte de los mecanismos bootstrap presentados en la sección anterior
para la estimación en el contexto paramétrico son también válidos para
la predicción con muy pocos cambios.

Stine (1987) propone un método bootstrap (ya presentado antes) para
estimar el error cuadrático medio de predicción del mejor predictor
lineal estimado en el contexto de un modelo \(AR(p)\). Usa versiones
bootstrap de los parámetros estimados y la remuestra bootstrap para
obtener
\[\widehat{X}_{n+j}^{\ast}=\widehat{\phi}_1^{\ast}\widehat{X}
_{n+j-1}^{\ast}+\widehat{\phi}_2^{\ast}\widehat{X}_{n+j-2}^{\ast
}+\cdots +\widehat{\phi}_{p}^{\ast}X_{n+j-p}^{\ast},\quad j=1,2,\ldots ,k,\]con
\(\widehat{X}_{t}^{\ast}=X_{t}^{\ast}\) para \(t\leq n\), cuya
distribución bootstrap se usa para estimar la verdadera distribución en
el muestreo del predictor.

Thombs y Schucany (1990) proponen método bootstrap primero hacia atrás y
luego hacia adelante para obtener intervalos de predicción a \(k\)
retardos para procesos \(AR(p)\). El método procede como sigue:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Construir los residuos hacia
  atrás:\[\widehat{e}_i=X_i-\widehat{\phi}_1X_{i+1}-\widehat{\phi}
  _2X_{i+2}-\cdots -\widehat{\phi}_{p}X_{i+p},\quad i=1,2,\ldots ,n-p,\]y
  calcular su versión corregida, \(\widehat{e}_i^{\prime}\) (tal y
  como se hace en el método de Stine en la sección anterior).
\item
  Arrojar errores bootstrap hacia atrás, \(\widehat{e}_i^{\ast}\), de
  la función de distribución empírica de los residuos hacia atrás
  corregidos.
\item
  Definir réplicas bootstrap hacia
  atrás:\[X_i^{\ast}=\widehat{\phi}_1X_{i+1}^{\ast}+\widehat{\phi}
  _2X_{i+2}^{\ast}+\cdots +\widehat{\phi}_{p}X_{i+p}^{\ast}+\widehat{e}
  _i^{\ast},\quad i=n-p,\ldots ,1,\]con \(X_i^{\ast}=X_i\) para
  \(t=n-p+1,n-p+2,\ldots ,n\).
\item
  Calcular versiones bootstrap de los estimadores, \(\widehat{\phi} _1^{\ast},\widehat{\phi}_2^{\ast},\ldots ,\widehat{\phi}_{p}^{\ast}\).
\item
  Construir residuos hacia adelante:{\[\widehat{a}_i=X_i-\widehat{\phi}_1X_{i-1}-\widehat{\phi}
  _2X_{i-2}+\cdots -\widehat{\phi}_{p}X_{i-p},\quad i=p+1,p+2,\ldots ,n,\]}
  y su versión corregida \(\widehat{a}_i^{\prime}\).
\item
  Arrojar errores bootstrap hacia adelante, \(\widehat{a}_i^{\ast}\),
  de la función de distribución empírica de los residuos hacia
  adelante corregidos.
\item
  Definir las réplicas bootstrap hacia adelante:{\[X_{n+j}^{\ast}=\widehat{\phi}_1^{\ast}X_{n+j-1}^{\ast}+\widehat{\phi}
  _2^{\ast}X_{n+j-2}^{\ast}+\cdots +\widehat{\phi}_{p}^{\ast
  }X_{n+j-p}^{\ast}+\widehat{a}_{n+j}^{\ast},\quad j=1,2,\ldots ,k.\]}
\end{enumerate}

Thombs y Schucany (1990) prueban la validez asintótica del bootstrap
demostrando que, cuando el tamaño muestral, \(n\), tiende a infinito,
\[P^{\ast}\left( X_{n+k}^{\ast}\leq x \right) -P\left( X_{n+k}\leq
x|_{X_{n-p+1},X_{n-p+2},\ldots ,X_n} \right) \rightarrow 0,\]de forma
casi segura, para casi todo \(x\). Este resultado implica la validez
asintótica del intervalo de predicción bootstrap \((x_{\alpha /2}^{\ast},x_{1-\alpha /2}^{\ast})\), donde \(x_{\beta }^{\ast}\) se
define mediante
\(P^{\ast}\left( X_{n+k}^{\ast}\leq x_{\beta }^{\ast} \right) =\beta\). Algunos estudios de simulación muestran los beneficios de este
método sobre los métodos clásicos cuando la distribución del error no es
normal.

García-Jurado, González-Manteiga, Prada-Sánchez, Febrero-Bande y Cao
(1995) demuestran la validez del bootstrap de Thombs y Schucany para
modelos \(ARI(p,d)\). Supongamos que \(X_{t}\sim ARI(p,d)\), la idea
principal de esta extensión es la siguiente:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Construir la serie de diferencias, \(Y_{t}=\nabla^{d}X_{t},\) donde

  \(\nabla\) es el operador diferencia definido por
  \(\nabla X_{t}=X_{t}-X_{t-1}\). Obviamente \(Y_{t}\) tiene una
  estructura \(AR(p)\).
\item
  Aplicar el bootstrap de Thombs y Schucany a esta serie para obtener
  la serie bootstrap \(\{Y_{t}^{\ast}\}\).
\item
  Calcular réplicas bootstrap \(X_{t}^{\ast}\) mediante \(d\)
  integraciones de la serie \(Y_{t}^{\ast}\), fijando las primeras
  observaciones bootstrap: \(X_{t}^{\ast}=X_{t}\) para \(t\leq n\).
\end{enumerate}

Cao, Febrero-Bande, González-Manteiga, Prada-Sánchez y García-Jurado
(1997) estudian un método bootstrap, alternativo al de Thombs y
Schucany, que es computacionalmente más rápido y también consistente.
Puede resumirse en los siguientes pasos:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Construir la distribución empírica de los residuos hacia adelante
  corregidos, \(F_n^{\widehat{a}^{\prime}}\).
\item
  Generar \(\widehat{a}_i^{\ast}\) con distribución \(F_n^{ \widehat{a}^{\prime}}\).
\item
  Construir réplicas bootstrap futuras
  \[X_{n+j}^{\ast}=\widehat{\phi}_1X_{n+j-1}^{\ast}
  +\widehat{\phi}_2X_{n+j-2}^{\ast} + \cdots +\widehat{\phi}_{p}X_{n+j-p}^{\ast}
  +\widehat{a}_{n+j}^{\ast},\quad j=1,2,\ldots ,k,\]
  donde \(X_i^{\ast}=X_i\) para \(i=n,n-1,\ldots ,n-p+1.\)
\end{enumerate}

Estos autores demuestran la validez asintótica de este método bootstrap
(en el mismo sentido que Thombs y Schucany) y de una versión suavizada
en la cual se reemplaza \(F_n^{\widehat{a}^{\prime}}\) por
\(K_{h}\ast F_n^{\widehat{a}^{\prime}}\), en el paso 2. Pascual, Romo y
Ruiz (2001) proponen una variante de este método en la que se incorpora
la variabilidad en la estimación de los parámetros de la serie.

\hypertarget{situaciones-de-dependencia-general-2}{%
\subsection{Situaciones de dependencia general}\label{situaciones-de-dependencia-general-2}}

Cuando la estructura de dependencia de la serie no es explícita los
métodos bootstrap existentes para la estimación (como el MBB, el SB o el
método de submuestreo) no funcionan para la predicción. El motivo es que
estos métodos no estiman consistentemente la distribución condictional
\[X_{n+k}|_{X_1,X_2,\ldots ,X_n}.\]
Esta situación es
completamente diferente del caso en que la dependencia se modeliza
paramétricamente, ya que en ese otro caso los métodos bootstrap usados
para la estimación permanecen válidos, en general, en el contexto de
predicción.

Es poco menos que imposible estimar la distribución condicional anterior
sin hacer ninguna suposición sobre el tipo de dependencia. Sin embargo
se puede llevar a cabo una estimación cuando se supone que el proceso
estocástico es markoviano de orden \(p\), porque entonces,
\[X_{n+k}|_{X_1,X_2,\ldots ,X_n}{=}^{\mathrm{d}
}X_{n+k}|_{X_{n-p+1},X_{n-p+2},\ldots ,X_n}\]
y, por tanto,
\[F_{k}(y|_{\mathbf{x}})=F_{k}(y|_{x_1,x_2,\ldots ,x_{p}})=P\left(
X_{n+k}\leq y|_{X_{n-p+1}=x_1,X_{n-p+2}=x_2,\ldots ,X_n=x_{p}} \right)\]
puede estimarse por medio de un estimador no paramétrico de la distribución
condicional, basado en estimadores no paramétricos de la regresión,
como, por ejemplo, mediante el estimador tipo núcleo:

\[\widehat{F}_{k,H}(y|_{\mathbf{x}})=\frac{\sum_{i=1}^{q-k}K_{H}(\mathbf{x}
-B_{i,p})\cdot 1_{\{X_{i+p+k-1}\leq y\}}}{\sum_{i=1}^{q-k}K_{H}(\mathbf{x}
-B_{i,p})},\]

donde \(q=n-p+1,\) \(K_{H}(\mathbf{u})=\det (H)^{-1}K(H^{-1}\mathbf{z})\), \(K\) es
una función núcleo, \(H\) es una matriz ventana diagonal definida positiva
y \(B_{i,p},\) \(i=1,2,\ldots ,q\) son los bloques muestrales de tamaño \(p\).
Este estimador podría usarse para calcular intervalos predicción
aproximados para \(X_{n+k}\) dados los valores observados del proceso
hasta el instante \(n\).

En el caso \(p=1\) (\(\{X_{t}\}\) es un proceso de Markov) el estimador
núcleo puede escribirse como
\[\widehat{F}_{k,h}(y|_{\mathbf{x}})=\frac{\sum_{i=1}^{n-k}K_{h}(x-X_i)\cdot
1_{\{X_{i+k}\leq y\}}}{\sum_{i=1}^{n-k}K_{h}(x-X_i)},\]

donde \(K_{h}(u)=h^{-1}K(u/h)\) y \(h>0\). Usar este estimador para calcular
el intervalo de predicción de nivel \(\alpha\):
\[\left( \widehat{F}
_{k,h}^{-1}(\alpha /2|_{\mathbf{x}}),\widehat{F}_{k,h}^{-1}(1-\alpha /2|_{\mathbf{x}}) \right),\]
es equivalente a llevar a cabo un método bootstrap de forma que
\[P\left( X_{n+k}^{\ast}=X_{i+k} \right) =\widehat{p}_i=\frac{
K_{h}(X_n-X_i)}{\sum_{j=1}^{n-k}K_{h}(X_n-X_j)}\mathrm{,}
i=1,2,\ldots ,n-k.\]

Teniendo esto en cuenta ese mecanismo bootstrap puede describirse como
sigue:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Construir los bloques muestrales de tamaño \(k+1\): \(B_{i,k+1}\),
  \(i=1,2,\ldots ,n-k\).
\item
  Calcular los valores \(\widehat{p}_i\), \(i=1,2,\ldots ,n-k\).
\item
  Arrojar un bloque del conjunto \(\{B_{1,k+1},B_{2,k+1},\ldots ,B_{n-k,k+1}\}\) con probabilidades \(\widehat{p}_i\),
  \(i=1,2,\ldots ,n-k\) y definir \(X_{n+k}^{\ast}\) como la última
  observación de los bloques generados.
\end{enumerate}

Está claro que la precisión de este mecanismo bootstrap depende de las
propiedades del estimador tipo núcleo de la distribución condicional.
Así, por ejemplo, bajo las condiciones impuestas en el Teorema 1 de
Gannoun (1990) se obtiene que
\[\sup_{x\in C}\sup_{y\in \mathbb{R}}\left\vert P\left( X_{n+k}^{\ast}\leq
y|_{X_n=x} \right) -P\left( X_{n+k}\leq y|_{X_n=x} \right) \right\vert
\rightarrow 0\]
en probabilidad.

Como consecuencia los intervalos de predicción bootstrap tienen
probabilidad de cobertura asintóticamente correcta, uniformemente, en
probabilidad, sobre la última observación de la muestra. Este resultado
puede extenderse fácilmente para procesos de Markov de orden
\(p>1\).

\hypertarget{implementaciuxf3n-en-r}{%
\section{\texorpdfstring{Implementación en \texttt{R}}{Implementación en R}}\label{implementaciuxf3n-en-r}}

Para simular una serie de tiempo en \texttt{R}
se puede emplear la función \texttt{arima.sim()} del paquete base \texttt{stats}.
Por ejemplo, podemos generar una serie autoregresiva con:
{[}Figura \ref{fig:arima-sim}{]}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Parámetros}
\NormalTok{nsim }\OtherTok{\textless{}{-}} \DecValTok{200}   \CommentTok{\# Numero de simulaciones}
\NormalTok{xvar }\OtherTok{\textless{}{-}} \DecValTok{1}     \CommentTok{\# Varianza}
\NormalTok{xmed }\OtherTok{\textless{}{-}} \DecValTok{0}     \CommentTok{\# Media}
\NormalTok{rho }\OtherTok{\textless{}{-}} \FloatTok{0.5}    \CommentTok{\# Coeficiente AR}
\NormalTok{nburn }\OtherTok{\textless{}{-}} \DecValTok{10}   \CommentTok{\# Periodo de calentamiento (burn{-}in)}
\NormalTok{evar }\OtherTok{\textless{}{-}}\NormalTok{ xvar}\SpecialCharTok{*}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ rho}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\CommentTok{\# Varianza del error}
\CommentTok{\# Simulación}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{ry }\OtherTok{\textless{}{-}} \FunctionTok{arima.sim}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{order =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{), }\AttributeTok{ar =}\NormalTok{ rho), }
            \AttributeTok{n =}\NormalTok{ nsim, }\AttributeTok{sd =} \FunctionTok{sqrt}\NormalTok{(evar)) }\CommentTok{\# n.start = nburn}
\FunctionTok{plot}\NormalTok{(ry)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[!htb]

{\centering \includegraphics[width=0.7\linewidth]{09-depen_files/figure-latex/arima-sim-1} 

}

\caption{Simulación de un modelo autoregresivo.}\label{fig:arima-sim}
\end{figure}

En este caso el periodo de calentamiento se establece mediante el
parámetro \texttt{n.start} (que se fija automáticamente a un valor adecuado).
La recomendación es fijar la varianza de las series simuladas si se quieren
comparar resultados considerando distintos parámetros de dependencia.

Otras opciones:

\begin{itemize}
\item
  \texttt{rand.gen\ =\ rnorm}
\item
  \texttt{innov\ =\ rand.gen(n,\ ...)}
\item
  \texttt{n.start\ =\ NA}
\item
  \texttt{start.innov\ =\ rand.gen(n.start,\ ...)}
\end{itemize}

Ejemplo (\texttt{?arima.sim}):
{[}Figura \ref{fig:arima-sim2}{]}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ry2 }\OtherTok{\textless{}{-}} \FunctionTok{arima.sim}\NormalTok{(}\AttributeTok{n =} \DecValTok{63}\NormalTok{, }\FunctionTok{list}\NormalTok{(}\AttributeTok{ar =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.8897}\NormalTok{, }\SpecialCharTok{{-}}\FloatTok{0.4858}\NormalTok{), }
          \AttributeTok{ma =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.2279}\NormalTok{, }\FloatTok{0.2488}\NormalTok{)),}
          \AttributeTok{rand.gen =} \ControlFlowTok{function}\NormalTok{(n, ...) }\FunctionTok{sqrt}\NormalTok{(}\FloatTok{0.1796}\NormalTok{) }\SpecialCharTok{*} \FunctionTok{rt}\NormalTok{(n, }\AttributeTok{df =} \DecValTok{5}\NormalTok{))}

\FunctionTok{plot}\NormalTok{(ry2)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[!htb]

{\centering \includegraphics[width=0.7\linewidth]{09-depen_files/figure-latex/arima-sim2-1} 

}

\caption{Simulación de un modelo autoregresivo con errores con distribución *t* de Student.}\label{fig:arima-sim2}
\end{figure}

\hypertarget{implementaciuxf3n-en-r-con-el-paquete-boot}{%
\section{\texorpdfstring{Implementación en \texttt{R} con el paquete \texttt{boot}}{Implementación en R con el paquete boot}}\label{implementaciuxf3n-en-r-con-el-paquete-boot}}

La función \texttt{tsboot()} del paquete \texttt{boot} implementa distintos métodos
de remuestreo para series de tiempo.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(boot)}
\CommentTok{\# ?tsboot}
\end{Highlighting}
\end{Shaded}

\hypertarget{rnews-1}{%
\subsection{Rnews 1}\label{rnews-1}}

Canty, A. J. (2002). \href{http://cran.fhcrc.org/doc/Rnews/Rnews_2002-3.pdf}{Resampling methods in R: the boot package}. Rnews: The Newsletter of the R Project, 2 (3), pp.~2-7.

\begin{quote}
"\texttt{tsboot()} can do either of these methods
by specifying \texttt{sim="fixed"} or \texttt{sim="geom"} respectively.
A simple call to tsboot includes the time
series, a function for the \texttt{statistic} (the first argument
of this function being the time series itself), the number
of bootstrap replicates, the simulation type and
the (mean) block length'\,'.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Datos}
\FunctionTok{data}\NormalTok{(lynx)}
\CommentTok{\# ?lynx}
\CommentTok{\# Boot}
\FunctionTok{library}\NormalTok{(boot)}
\NormalTok{lynx.fun }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(tsb) \{}
\NormalTok{  fit }\OtherTok{\textless{}{-}} \FunctionTok{ar}\NormalTok{(tsb, }\AttributeTok{order.max =} \DecValTok{25}\NormalTok{)}
  \FunctionTok{c}\NormalTok{(fit}\SpecialCharTok{$}\NormalTok{order, }\FunctionTok{mean}\NormalTok{(tsb)) }
\NormalTok{\}}
\CommentTok{\# tsboot}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\FunctionTok{tsboot}\NormalTok{(}\FunctionTok{log}\NormalTok{(lynx), lynx.fun, }\AttributeTok{R =} \DecValTok{199}\NormalTok{, }\AttributeTok{sim =} \StringTok{"geom"}\NormalTok{, }\AttributeTok{l =} \DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## STATIONARY BOOTSTRAP FOR TIME SERIES
## 
## Average Block Length of 20 
## 
## Call:
## tsboot(tseries = log(lynx), statistic = lynx.fun, R = 199, l = 20, 
##     sim = "geom")
## 
## 
## Bootstrap Statistics :
##      original      bias    std. error
## t1* 11.000000 -6.46733668   2.4675036
## t2*  6.685933 -0.01494926   0.1163515
\end{verbatim}

\hypertarget{rnews-2}{%
\subsection{Rnews 2}\label{rnews-2}}

Canty, A. J. (2002). \href{http://cran.fhcrc.org/doc/Rnews/Rnews_2002-3.pdf}{Resampling methods in R: the boot package}. Rnews: The Newsletter of the R Project, 2 (3), pp.~2-7.

\begin{quote}
"An alternative to the block bootstrap is to use
model based resampling. In this case a model is fitted
to the time series so that the errors are i.i.d. The
observed residuals are sampled as an i.i.d. series and
then a bootstrap time series is reconstructed. In constructing
the bootstrap time series from the residuals,
it is recommended to generate a long time series
and then discard the initial burn-in stage. Since
the length of burn-in required is problem specific,
\texttt{tsboot} does not actually do the resampling. Instead
the user should give a function which will return the
bootstrap time series. This function should take three
arguments, the time series as supplied to \texttt{tsboot}, a
value \texttt{n.sim} which is the length of the time series required
and the third argument containing any other
information needed by the random generation function
such as coefficient estimates. When the random
generation function is called it will be passed the arguments
\texttt{data}, \texttt{n.sim} and \texttt{ran.args} passed to \texttt{tsboot}
or their defaults.

One problem with the model-based bootstrap is
that it is critically dependent on the correct model
being fitted to the data. Davison and Hinkley (1997)
suggest post-blackening as a compromise between the
block bootstrap and the model-based bootstrap. In
this method a simple model is fitted and the residuals
are found. These residuals are passed as the dataset
to \texttt{tsboot} and are resampled using the block (or stationary)
bootstrap. To create the bootstrap time series
the resampled residuals should be put back through
the fitted model filter. The function \texttt{ran.gen} can be
used to do this'\,'.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Datos}
\NormalTok{lynx1 }\OtherTok{\textless{}{-}} \FunctionTok{log}\NormalTok{(lynx)}
\CommentTok{\# Modelo}
\NormalTok{lynx.ar }\OtherTok{\textless{}{-}} \FunctionTok{ar}\NormalTok{(lynx1)}
\CommentTok{\# Residuos}
\NormalTok{lynx.res }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(lynx.ar, resid[}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(resid)])}
\NormalTok{lynx.res }\OtherTok{\textless{}{-}}\NormalTok{ lynx.res }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(lynx.res)}
\CommentTok{\# Boot}
\FunctionTok{library}\NormalTok{(boot)}
\NormalTok{lynx.ord }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(lynx.ar}\SpecialCharTok{$}\NormalTok{order, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{lynx.mod }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{order =}\NormalTok{ lynx.ord, }\AttributeTok{ar =}\NormalTok{ lynx.ar}\SpecialCharTok{$}\NormalTok{ar)}
\NormalTok{lynx.args }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{mean =} \FunctionTok{mean}\NormalTok{(lynx1), }\AttributeTok{model =}\NormalTok{ lynx.mod)}
\NormalTok{lynx.black }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(res, n.sim, ran.args) \{}
\NormalTok{    m }\OtherTok{\textless{}{-}}\NormalTok{ ran.args}\SpecialCharTok{$}\NormalTok{mean}
\NormalTok{    ts.mod }\OtherTok{\textless{}{-}}\NormalTok{ ran.args}\SpecialCharTok{$}\NormalTok{model}
\NormalTok{    m }\SpecialCharTok{+} \FunctionTok{filter}\NormalTok{(res, ts.mod}\SpecialCharTok{$}\NormalTok{ar, }\AttributeTok{method =} \StringTok{"recursive"}\NormalTok{) }
\NormalTok{\}}
\CommentTok{\# tsboot}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\FunctionTok{tsboot}\NormalTok{(lynx.res, lynx.fun, }\AttributeTok{R =} \DecValTok{199}\NormalTok{, }\AttributeTok{l =} \DecValTok{20}\NormalTok{, }
        \AttributeTok{sim =} \StringTok{"fixed"}\NormalTok{, }\AttributeTok{n.sim =} \DecValTok{114}\NormalTok{,}
        \AttributeTok{ran.gen =}\NormalTok{ lynx.black, }\AttributeTok{ran.args =}\NormalTok{ lynx.args)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## POST-BLACKENED BLOCK BOOTSTRAP FOR TIME SERIES
## 
## Fixed Block Length of 20 
## 
## Call:
## tsboot(tseries = lynx.res, statistic = lynx.fun, R = 199, l = 20, 
##     sim = "fixed", n.sim = 114, ran.gen = lynx.black, ran.args = lynx.args)
## 
## 
## Bootstrap Statistics :
##       original   bias    std. error
## t1* 0.0000e+00 9.819095  3.46664295
## t2* 6.1989e-18 6.683323  0.09551445
\end{verbatim}

\hypertarget{ejercicios-1}{%
\section{Ejercicios}\label{ejercicios-1}}

\begin{exercise}[Practical 8.1, Lynx data: Davison y Hinkley, 1997]
\protect\hypertarget{exr:tsboot-lynx}{}{\label{exr:tsboot-lynx} \iffalse (Practical 8.1, Lynx data: Davison y Hinkley, 1997) \fi{} }
Reproducir el ``Practical 8.1 (Lynx data)'' en Davison, A. C., y Hinkley, D. V. (1997). Bootstrap methods and their application. Cambridge university press, \url{http://statwww.epfl.ch/davison/BMA}
(\href{http://webcache.googleusercontent.com/search?q=cache:a4nFL5ymMMoJ:statwww.epfl.ch/davison/BMA/+\&cd=1\&hl=gl\&ct=clnk\&gl=es}{caché}):
\end{exercise}

\begin{quote}
"Dataframe lynx contains the Canadian lynx data,
to the logarithm of which we fit the autoregressive
model that minimizes AIC:
\end{quote}

{[}Figura \ref{fig:lynx-data}{]}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ts.plot}\NormalTok{(}\FunctionTok{log}\NormalTok{(lynx))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[!htb]

{\centering \includegraphics[width=0.7\linewidth]{09-depen_files/figure-latex/lynx-data-1} 

}

\caption{Lynx data (logarithmic scale).}\label{fig:lynx-data}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lynx.ar }\OtherTok{\textless{}{-}} \FunctionTok{ar}\NormalTok{(}\FunctionTok{log}\NormalTok{(lynx))}
\NormalTok{lynx.ar}\SpecialCharTok{$}\NormalTok{order}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 11
\end{verbatim}

\begin{quote}
The best model is AR(11). How well determined is this,
and what is the variance of the series average?
We bootstrap to see, using \texttt{lynx.fun} (given below),
which calculates the order of the fitted autoregressive
model, the series average, and saves the series itself.
\end{quote}

\begin{quote}
Here are results for fixed-block bootstraps
with block length \(l = 20\):
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(DNI)}
\NormalTok{lynx.fun }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(tsb) \{ }
\NormalTok{  ar.fit }\OtherTok{\textless{}{-}} \FunctionTok{ar}\NormalTok{(tsb, }\AttributeTok{order.max=}\DecValTok{25}\NormalTok{)}
  \FunctionTok{c}\NormalTok{(ar.fit}\SpecialCharTok{$}\NormalTok{order, }\FunctionTok{mean}\NormalTok{(tsb), tsb) }
\NormalTok{\}}
\NormalTok{lynx}\FloatTok{.1} \OtherTok{\textless{}{-}} \FunctionTok{tsboot}\NormalTok{(}\FunctionTok{log}\NormalTok{(lynx), lynx.fun, }\AttributeTok{R=}\DecValTok{99}\NormalTok{, }\AttributeTok{l=}\DecValTok{20}\NormalTok{, }\AttributeTok{sim=}\StringTok{"fixed"}\NormalTok{)}
\NormalTok{lynx}\FloatTok{.1}
\FunctionTok{ts.plot}\NormalTok{(}\FunctionTok{ts}\NormalTok{(lynx}\FloatTok{.1}\SpecialCharTok{$}\NormalTok{t[}\DecValTok{1}\NormalTok{,}\DecValTok{3}\SpecialCharTok{:}\DecValTok{116}\NormalTok{],}\AttributeTok{start=}\FunctionTok{c}\NormalTok{(}\DecValTok{1821}\NormalTok{,}\DecValTok{1}\NormalTok{)),}
        \AttributeTok{main=}\StringTok{"Block simulation, l=20"}\NormalTok{)}
\FunctionTok{boot.array}\NormalTok{(lynx}\FloatTok{.1}\NormalTok{)[}\DecValTok{1}\NormalTok{,]}
\FunctionTok{table}\NormalTok{(lynx}\FloatTok{.1}\SpecialCharTok{$}\NormalTok{t[,}\DecValTok{1}\NormalTok{])}
\FunctionTok{var}\NormalTok{(lynx}\FloatTok{.1}\SpecialCharTok{$}\NormalTok{t[,}\DecValTok{2}\NormalTok{])}
\FunctionTok{qqnorm}\NormalTok{(lynx}\FloatTok{.1}\SpecialCharTok{$}\NormalTok{t[,}\DecValTok{2}\NormalTok{]);}
\FunctionTok{abline}\NormalTok{(}\FunctionTok{mean}\NormalTok{(lynx}\FloatTok{.1}\SpecialCharTok{$}\NormalTok{t[,}\DecValTok{2}\NormalTok{]),}\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{var}\NormalTok{(lynx}\FloatTok{.1}\SpecialCharTok{$}\NormalTok{t[,}\DecValTok{2}\NormalTok{])),}\AttributeTok{lty=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{quote}
To obtain similar results for the stationary
bootstrap with mean block length \(l = 20\):
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{.Random.seed }\OtherTok{\textless{}{-}}\NormalTok{ lynx}\FloatTok{.1}\SpecialCharTok{$}\NormalTok{seed}
\NormalTok{lynx}\FloatTok{.2} \OtherTok{\textless{}{-}} \FunctionTok{tsboot}\NormalTok{(}\FunctionTok{log}\NormalTok{(lynx), lynx.fun, ...}
\CommentTok{\# lynx.2}
\end{Highlighting}
\end{Shaded}

\begin{quote}
See if the results look different from those above.
Do the simulated series using blocks look like the original?
Compare the estimated variances under the two resampling schemes.
Try different block lengths, and see how
the variances of the series average change.
\end{quote}

\begin{quote}
For model-based resampling we need to store results from the original model:
\end{quote}

\begin{quote}
Check the orders of the fitted models for this scheme.
\end{quote}

\begin{quote}
For post-blackening we need to define yet another function:
\end{quote}

\begin{quote}
Compare these results with those above, and try the post-blackened bootstrap with \texttt{sim="geom"}.
(Sections 8.2.2, 8.2.3)'\,'.
\end{quote}

\begin{exercise}[Practical 8.2, Beaver data: Davison y Hinkley, 1997]
\protect\hypertarget{exr:tsboot-beaver}{}{\label{exr:tsboot-beaver} \iffalse (Practical 8.2, Beaver data: Davison y Hinkley, 1997) \fi{} }
Reproducir el ``Practical 8.2 (Beaver data)'' en Davison, A. C., y Hinkley, D. V. (1997). Bootstrap methods and their application. Cambridge university press, \url{http://statwww.epfl.ch/davison/BMA}
(\href{http://webcache.googleusercontent.com/search?q=cache:a4nFL5ymMMoJ:statwww.epfl.ch/davison/BMA/+\&cd=1\&hl=gl\&ct=clnk\&gl=es}{caché}):
\end{exercise}

\begin{quote}
"The data in beaver consist of a time series of \(n = 100\)
observations on the body temperature \(y_1, \ldots, y_n\)
and an indicator \(x_1, \ldots, x_n\) of activity of
a female beaver, Castor canadensis.
\end{quote}

{[}Figura \ref{fig:beaver-data}{]}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# ?beaver}
\FunctionTok{plot}\NormalTok{(beaver)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[!htb]

{\centering \includegraphics[width=0.7\linewidth]{09-depen_files/figure-latex/beaver-data-1} 

}

\caption{Beaver data}\label{fig:beaver-data}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(beaver)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "mts" "ts"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(beaver) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  Time-Series [1:100, 1:4] from 1 to 100: 307 307 307 307 307 307 307 307 307 307 ...
##  - attr(*, "dimnames")=List of 2
##   ..$ : NULL
##   ..$ : chr [1:4] "day" "time" "temp" "activ"
\end{verbatim}

\begin{quote}
We want to estimate and give an uncertainty measure
for the body temperature of the beaver.
The simplest model that allows for the clear
autocorrelation of the series is \ldots{}
\end{quote}

\begin{quote}
To fit the original model and to generate a new series:
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OtherTok{\textless{}{-}}\NormalTok{ beaver}
\CommentTok{\# fit \textless{}{-} function( data )\{ }
\CommentTok{\#   \#  X \textless{}{-} cbind(rep(1,100),data$activ) \# ErrorR}
\CommentTok{\#   X \textless{}{-} cbind(rep(1, 100), data[,"activ"])}
\CommentTok{\#   para \textless{}{-} list(X = X, data = data)}
\CommentTok{\#   \# assign("para",para, frame=1) \# ErrorR}
\CommentTok{\#   \# assign("para", para, envir = .GlobalEnv)}
\CommentTok{\#   para \textless{}\textless{}{-} para}
\CommentTok{\#   \# arima.mle \# ErrorR}
\CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#   d \textless{}{-} arima.mle(x = para$data$temp,}
\CommentTok{\#               model = list(ar = c(0.8)), xreg = para$X)}
\CommentTok{\#   res \textless{}{-} arima.diag(d, plot = F, std.resid = T)$std.resid}
\CommentTok{\#   res \textless{}{-} res[!is.na(res)]}
\CommentTok{\#   list(}
\CommentTok{\#     paras = c(d$model$ar, d$reg.coef, sqrt(d$sigma2)),}
\CommentTok{\#     res = res {-} mean(res),}
\CommentTok{\#     fit = X \%*\% d$reg.coef)  }
\CommentTok{\# \}}
\CommentTok{\# }
\CommentTok{\# beaver.args \textless{}{-} fit(beaver)}
\CommentTok{\# white.noise \textless{}{-} function(n.sim, ts) }
\CommentTok{\#     sample(ts, size = n.sim, replace = TRUE)}
\CommentTok{\# beaver.gen \textless{}{-} function(ts, n.sim, ran.args)\{}
\CommentTok{\#     tsb \textless{}{-} ran.args$res}
\CommentTok{\#     fit \textless{}{-} ran.args$fit}
\CommentTok{\#     coeff \textless{}{-} ran.args$paras}
\CommentTok{\#     ts$temp \textless{}{-} fit + coeff[4] * arima.sim(model = list(ar = coeff[1]),}
\CommentTok{\#         n = n.sim, rand.gen = white.noise, ts = tsb)}
\CommentTok{\#     ts}
\CommentTok{\# \}}
\CommentTok{\# new.beaver \textless{}{-} beaver.gen( beaver, 100, beaver.args )}
\end{Highlighting}
\end{Shaded}

\begin{quote}
Now we are able to generate data,
we can bootstrap and see the results of \texttt{beaver.boot} as follows:
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# beaver.fun \textless{}{-} function(ts) fit(ts)$paras}
\CommentTok{\# beaver.boot \textless{}{-} tsboot( beaver, beaver.fun, R=99,sim="model", }
\CommentTok{\#         n.sim=100,ran.gen=beaver.gen,ran.args=beaver.args) }
\CommentTok{\# names(beaver.boot)}
\CommentTok{\# beaver.boot$t0}
\CommentTok{\# beaver.boot$t[1:10,]}
\end{Highlighting}
\end{Shaded}

\begin{quote}
showing the original value of \texttt{beaver.fun}
and its value for the first 10 replicate series.
Are the estimated mean temperatures for the \texttt{R\ =\ 99}
simulations normal?
Use \texttt{boot.ci} to obtain normal and basic bootstrap
confidence intervals for the resting and active temperatures.
In this analysis we have assumed that
the linear model with AR(1) errors is appropriate.
How would you proceed if it were not?
(Section 8.2; Reynolds, 1994)'\,'.
\end{quote}

\begin{exercise}[Practical 8.3, Sunspot data: Davison y Hinkley, 1997]
\protect\hypertarget{exr:tsboot-sunspot}{}{\label{exr:tsboot-sunspot} \iffalse (Practical 8.3, Sunspot data: Davison y Hinkley, 1997) \fi{} }
Reproducir el ``Practical 8.3 (Sunspot data)'' en Davison, A. C., y Hinkley, D. V. (1997). Bootstrap methods and their application. Cambridge university press, \url{http://statwww.epfl.ch/davison/BMA}
(\href{http://webcache.googleusercontent.com/search?q=cache:a4nFL5ymMMoJ:statwww.epfl.ch/davison/BMA/+\&cd=1\&hl=gl\&ct=clnk\&gl=es}{caché}):
\end{exercise}

\begin{quote}
"Consider scrambling the phases of the sunspot data.
To see the original data,
\end{quote}

{[}Figura \ref{fig:sunspot}{]}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(sunspot.year)  }\CommentTok{\# WarningR: data set \textquotesingle{}sunspot\textquotesingle{} not found}
\CommentTok{\# ?sunspot.year}
\NormalTok{yl }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{50}\NormalTok{, }\DecValTok{200}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(sunspot.year, }\AttributeTok{ylim =}\NormalTok{ yl)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{h =} \DecValTok{0}\NormalTok{, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[!htb]

{\centering \includegraphics[width=0.7\linewidth]{09-depen_files/figure-latex/sunspot-1} 

}

\caption{Sunspot data (yearly numbers).}\label{fig:sunspot}
\end{figure}

\begin{quote}
two replicates generated using ordinary phase scrambling,
and two phase scrambled series whose marginal distribution is the same as that of the original data:
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(DNI)}
\NormalTok{sunspot}\FloatTok{.1} \OtherTok{\textless{}{-}} \FunctionTok{tsboot}\NormalTok{(sunspot.year, ...}
\NormalTok{.Random.seed }\OtherTok{\textless{}{-}}\NormalTok{ sunspot}\FloatTok{.1}\SpecialCharTok{$}\NormalTok{seed }\CommentTok{\# set.seed(DNI)}
\NormalTok{sunspot}\FloatTok{.2} \OtherTok{\textless{}{-}} \FunctionTok{tsboot}\NormalTok{(sunspot.year, ...}
\end{Highlighting}
\end{Shaded}

\begin{quote}
What features of the original data are preserved
by the two algorithms?
(You may find it helpful to experiment with
different shapes for the figures.)
(Section 8.2.4; Problem 8.4; Theiler et al, 1992)'\,'.
\end{quote}

\hypertarget{implementaciuxf3n-en-r-con-el-paquete-forecast}{%
\section{\texorpdfstring{Implementación en \texttt{R} con el paquete \texttt{forecast}}{Implementación en R con el paquete forecast}}\label{implementaciuxf3n-en-r-con-el-paquete-forecast}}

\hypertarget{bootstrap-condicional-a-partir-de-un-modelo-ajustado}{%
\subsection{Bootstrap condicional (a partir de un modelo ajustado)}\label{bootstrap-condicional-a-partir-de-un-modelo-ajustado}}

En la práctica normalmente se ajusta un modelo a los datos observados y
posteriormente se obtienen las simulaciones condicionadas empleando
el modelo ajustado.

Por ejemplo, en el caso de series de tiempo, se puede emplear la función \texttt{simulate}
del paquete \texttt{forecast}: {[}Figura \ref{fig:co2}{]}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(forecast)}
\CommentTok{\# ?co2}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{window}\NormalTok{(co2, }\DecValTok{1990}\NormalTok{) }\CommentTok{\# datos de co2 desde 1990 (hasta 1997)}
\FunctionTok{plot}\NormalTok{(data, }\AttributeTok{ylab =} \FunctionTok{expression}\NormalTok{(}\StringTok{"Atmospheric concentration of CO"}\NormalTok{[}\DecValTok{2}\NormalTok{]), }
     \AttributeTok{xlim=}\FunctionTok{c}\NormalTok{(}\DecValTok{1990}\NormalTok{,}\DecValTok{2000}\NormalTok{), }\AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\DecValTok{350}\NormalTok{, }\DecValTok{375}\NormalTok{))}
\NormalTok{data2 }\OtherTok{\textless{}{-}} \FunctionTok{window}\NormalTok{(co2, }\DecValTok{1990}\NormalTok{, }\DecValTok{1996}\NormalTok{) }\CommentTok{\# datos de co2 desde 1990 hasta 1996}
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{ets}\NormalTok{(data2)}
\CommentTok{\# Simulación condicional}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{ry }\OtherTok{\textless{}{-}} \FunctionTok{simulate}\NormalTok{(fit, }\DecValTok{12}\SpecialCharTok{*}\DecValTok{4}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(ry, }\AttributeTok{col=}\StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[!htb]

{\centering \includegraphics[width=0.7\linewidth]{09-depen_files/figure-latex/co2-1} 

}

\caption{Datos de co2 (1990-1997) y simulación condicional (a partir de las observaciones desde 1990 hasta 1996).}\label{fig:co2}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(}\FunctionTok{forecast}\NormalTok{(fit, }\AttributeTok{h=}\DecValTok{12}\SpecialCharTok{*}\DecValTok{4}\NormalTok{), }\AttributeTok{col=}\StringTok{"blue"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(ry, }\AttributeTok{col=}\StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[!htb]

{\centering \includegraphics[width=0.7\linewidth]{09-depen_files/figure-latex/co22-1} 

}

\caption{Predicción de los valores de co2 y simulación condicional (ambas a partir de las observaciones entre 1990 y 1996).}\label{fig:co22}
\end{figure}

{[}Figura \ref{fig:co22}{]}

Ver enlaces en apéndice \ref{forecast-links}.

\hypertarget{spatial-data}{%
\section{Spatial data}\label{spatial-data}}

Ver enlaces en apéndice \ref{spatial-links}.

\hypertarget{referencias}{%
\chapter*{Referencias}\label{referencias}}
\addcontentsline{toc}{chapter}{Referencias}

Akritas, M. G. (1986). \href{https://www.tandfonline.com/doi/abs/10.1080/01621459.1986.10478369}{Bootstrapping the Kaplan--Meier estimator}.
\emph{J. Amer. Stat. Assoc.} \textbf{81}, 1032--1038.

Beran, R. (1987). Prepivoting to reduce level error of confidence sets.
\emph{Biometrika} \textbf{74}, 457-468.

Bhattacharya, R.N. and Ghosh, J.K. (1978). On the validity of the formal
Edgeworth expansion. \emph{Ann. Statist.} \textbf{6}, 434--451.

Bickel, P.J. and Freedman, D.A. (1981). Some Asymptotic theory for the
bootstrap. \emph{Ann. Statist.} \textbf{12} 2, 470-482.

Bock M., Bowman A.W. and Ismail B. (2007). Estimation and inference for
error variance in bivariate nonparametric regression. \emph{Statistics and Computing}
\textbf{17}, 39-47.

Bowman A.W. and Azzalini A. (2019). \emph{R package `sm': nonparametric smoothing methods}
(version 2.2). \url{http://www.stats.gla.ac.uk/~adrian/sm}.

Bowman, A., Hall, P. and Prvan, T. (1998). Bandwidth selection for the
smoothing of distribution functions. \emph{Biometrika} \textbf{85} 4, 799-808.

Breslow, N. and Crowley, J. (1974). A large sample study of the life
table and product limit estimates under random censorship. \emph{Ann.
Statist.} \textbf{2}, 437--453.

Bühlmann, P. (1994). Blockwise bootstrap empirical processes for
stationary sequences. \emph{Ann. Statist.} \textbf{22}, 995-1012.

Bühlmann, P. (1997). Sieve bootstrap for time series. \emph{Bernoulli} \textbf{3},
123-148.

Bühlmann, P. (1998). Sieve bootstrap for smoothing in nonstationary time
series. \emph{Ann. Statist.} \textbf{26}, 48-83.

Canty, A. J. (2002). \href{http://cran.fhcrc.org/doc/Rnews/Rnews_2002-3.pdf}{Resampling methods in R: the boot package}.
\emph{The Newsletter of the R Project} \textbf{2}, 2-7.

Cao, R. (1990). Órdenes de convergencia para las aproximaciones normal y
bootstrap en la estimación no paramétrica de la función de densidad.
\emph{Trabajos de Estadística}, vol.~\textbf{5}, 2, 23-32.

Cao, R. (1991). Rate of convergence for the wild bootstrap in
nonparametric regression. \emph{Ann. Statist} \textbf{19}, 2226-2231.

Cao, R. (1993). Bootstrapping the mean integrated squared error.
\emph{Jr.~Mult. Anal.} \textbf{45}, 137--160.

Cao, R. (1999). An overview of bootstrap methods for estimating and
predicting in time series. \emph{Test} \textbf{8}, 95-116.

Cao, R., Cuevas, A. and González-Manteiga, W. (1993). A comparative
study of several smoothing methods in density estimation.
\emph{Comp. Statist. Data Anal.} \textbf{17}, 153--176.

Cao, R., Febrero-Bande, M., González-Manteiga, W., Prada-Sánchez, J.M.
and García-Jurado, I. (1997). Saving computer time in constructing
consistent bootstrap prediction intervals for autoregressive processes.
\emph{Commun. Statist. Simula.} \textbf{26}, 961-978.

Cao, R. and González-Manteiga, W. (1993). Bootstrap methods in
regression smoothing. \emph{Journal of Nonparametric Statistics} \textbf{2},
379-388.

Cao, R. and Prada-Sánchez, J.M. (1993). Bootstrapping the mean of a
symmetric population. \emph{Statistics \& Probability Letters} \textbf{17}, 43-48.

Castillo-Páez, S., Fernández-Casal, R. and García-Soidán, P. (2019).
\href{https://www.sciencedirect.com/science/article/pii/S0167947319300325?via\%3Dihub}{A nonparametric bootstrap method for spatial data},
\emph{Computational Statistics and Data Analysis} \textbf{137}, 1-15.

Castillo-Páez S., Fernández-Casal R. and García-Soidán P. (2020).
\href{https://doi.org/10.1016/j.spasta.2019.100389}{Nonparametric bootstrap approach for unconditional risk mapping under heteroscedasticity}.
\emph{Spatial Statistics}. In Press.

Carlstein, E., Do, K. A., Hall, P., Hesterberg, T., and Künsch, H. R. (1998). \href{https://projecteuclid.org/euclid.bj/1174324983}{Matched-block bootstrap for dependent data .}
\emph{Bernoulli} \textbf{4} 3, 305-328.

Cox, D. R. (1972). \href{https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1972.tb00899.x}{Regression Models and Life Tables}
(with discussion), \emph{Journal of the Royal Statistical Society series B} \textbf{34}, 187--220.

Davison, A.C. and Hinkley, D.V. (1997).
\emph{Bootstrap Methods and their Application}. Cambridge University Press.

Efron, B. (1979). Bootstrap Methods: Another look at the Jackknife.
\emph{Ann. Statist.} \textbf{7}, 1-26.

Efron, B. (1981). Censored data and the bootstrap.
\emph{J. Amer. Statist. Assoc.} \textbf{76}, 312--319.

Efron, B. (1982). The Jackknife, the Bootstrap and other Resampling
Plans. CBMS-NSF. \emph{Regional Conference series in applied mathematics}.

Efron, B. (1983). Estimating the error rate of a prediction rule:
improvements on cross-validation. \emph{J. Amer. Stat. Assoc.} \textbf{78}, 316-331.

Efron, B. (1987). Better Bootstrap confidence intervals (with
discussion). \emph{J. Amer. Stat. Assoc.} \textbf{82}, 171-200.

Efron, B. (1990). More Efficient Bootstrap Computations.
\emph{J. Amer. Statist. Assoc.} \textbf{85}, 79-89.

Efron, B. and Tibshirani, R. (1986). Bootstrap methods for standard
errors, confidence intervals, and other measures of statistical
accuracy. \emph{Statistical Science} \textbf{1}, 54-77.

Efron, B. and Tibshirani, R.J. (1993). \emph{An Introduction to the Bootstrap}.
Chapman and Hall.

Fan J. and Gijbels I. (1996). \emph{Local Polynomial Modelling and Its Applications}.
Chapman and Hall, London.

Fan J. and Yao Q. (1998). Efficient estimation of conditional variance functions in
stochastic regression. \emph{Biometrika}, \textbf{85}, 645--660.

Faraway, J.J. and Jhun, M. (1990). Bootstrap choice of bandwidth for
density estimation. \emph{Jr.~Amer. Statist. Assoc.} \textbf{85}, 1119--1122.

Fernández-Casal, R. y Cao, R. (2020). \emph{Simulación Estadística}. \url{https://rubenfcasal.github.io/simbook}.

Ferretti, N. and Romo, J. (1996). Unit root bootstrap test for AR(1)
models. \emph{Biometrika} \textbf{83}, 849-860.

Fisher, R.A. (1935). \emph{The design of experiments}. Edinburgh: Oliver and Boyd.

Fox, J., and Weisberg, S. (2018). \emph{An R companion to applied regression}. Sage publications.

Freedman, D.A. (1981). Bootstrapping regression models.
\emph{Ann. Statis.} \textbf{9} 6, 118-1228.

Fuller, W.A. (1976). \emph{Introduction to statistical time series}. New York:
Wiley.

Gannoun, A. (1990). Estimation non paramétrique de la médiane
conditionnelle. Application à la prévision. \emph{C. R. Acad. Sci. Paris}
\textbf{310}, 295-298.

García-Jurado, I. González-Manteiga, W., Prada-Sánchez, J.M.,
Febrero-Bande, M. and Cao, R. (1995). Predicting using Box-Jenkins,
nonparametric and bootstrap techniques. \emph{Technometrics} \textbf{37}, 303-310.

González-Manteiga, W., and Cao, R. (1993). Testing the hypothesis of a general linear model using nonparametric regression estimation. \emph{Test}, 2, 161-188.

González-Manteiga, W. and Prada-Sánchez, J.M. (1985). Una aplicación de
los métodos de suavización no paramétricos en la técnica bootstrap.
\emph{Proceedings Jornadas Hispano-Lusas de Matemáticas}. Murcia, 1985.

González-Manteiga, W., Prada-Sánchez, J.M. and Romo, J. (1994). The
Bootstrap-A Review. \emph{Computational Statistics} \textbf{9}, 165-205.

Hall, P. (1986). On the bootstrap and confidence intervals.
\emph{Ann. Statist.} \textbf{14}, 1431-1452.

Hall, P. (1988-a) Theoretical comparison of bootstrap confidence
intervals. \emph{Ann. Statist.} \textbf{16}, 927-953.

Hall, P. (1988-b). Rate of convergence in bootstrap approximations.
\emph{Ann. Probab.} \textbf{16} 4, 1665-1684.

Hall, P. (1990). Using the bootstrap to estimate mean squared error and
select smoothing parameter in nonparametric problems.
\emph{J. Multivariate Anal.} \textbf{32}, 177--203.

Hall. P. (1992). \href{https://books.google.es/books?hl=es\&lr=\&id=CwLaBwAAQBAJ\&oi=fnd\&pg=PR11\&dq=The+Bootstrap+and+Edgeworth+Expansion}{\emph{The Bootstrap and Edgeworth Expansion}}. Springer Verlag.

Hall, P., Horowitz, J.L. and Jing, B-Y. (1995). On blocking rules for
the bootstrap with dependent data. \emph{Biometrika} \textbf{82}, 561-574.

Hall, P., Marron, J.S. and Park, B. (1992). Smoothed cross-validation.
\emph{Probab. Theor. Rel. Fields} \textbf{92}, 1--20.

Hall, P. and Martin, M.A.~(1988). On bootstrap resampling and iteration.
\emph{Biometrika} \textbf{75}, 661-671.

Härdle, W. and Mammen, E. (1993). Comparing nonparametric versus
parametric regression fits. \emph{Ann. Statist.} \textbf{21}, 1926-1947.

Härdle, W. and Marron, J. S. (1991). Bootstrap simultaneous error bars
for nonparametric regression. \emph{Ann. Statist.} \textbf{19}, 778--796.

Hartigan, J.A. (1969). Using subsample values as typical values. J.
\emph{Amer. Statist. Assoc.} \textbf{64}, 1303-1317.

Heimann, G. and Kreiss, J-P. (1996). Bootstrapping general first order
autoregression. \emph{Statist. Prob. Lett.} \textbf{30}, 87-98.

Kaplan, E. L. and P. Meier, Nonparametric estimation from incomplete
observations, \emph{J. Amer. Stat. Assoc.} \textbf{53} (1958) 457--481.

Kreiss, J-P. and Franke, J. (1992). Bootstrapping stationary
autoregressive moving average models. \emph{J. Time Ser. Anal.} \textbf{13},
297-317.

Künsch, H.R. (1989). The jackknife and the bootstrap for general
stationary observations. \emph{Ann. Statist.} \textbf{17}, 1217-1241.

Liu, R.Y. and Singh, K. (1992). Moving blocks jackknife and bootstrap
capture weak dependence. In \href{https://books.google.es/books?hl=es\&lr=\&id=ZJzIpNZNVLgC\&oi=fnd\&pg=PA3\&dq=Exploring+the+limits+of+the+bootstrap}{\emph{Exploring the limits of bootstrap}}
(R. LePage and L Billard, Eds.), pp.~225-248. New York: Wiley.

Mammen, E. (1992). \href{https://books.google.es/books?hl=es\&lr=\&id=zpDfBwAAQBAJ\&oi=fnd\&pg=PP8\&dq=When+does+Bootstrap+Work\%3F}{\emph{When does Bootstrap Work?}}.
Springer Verlag.

Maritz, J.S. (1979). A note on exact robust confidence intervals for
location. \emph{Biometrika} \textbf{66}, 163-166.

Marron, J.S. (1992). Bootstrap bandwidth selection. In
\emph{Exploring the limits of the bootstrap}, LePage, R. and Billard,
L. eds., pp.~249--262. New York: Wiley.

Nadaraya, E.A. (1964). On estimating regression. \emph{Theor. Probab. Appl.}
\textbf{9,} 141-142.

Naik-Nimbalkar, U.V. and Rajarshi, M.B. (1994). Validity of blockwise
bootstrap for empirical processes with stationary observations.
\emph{Ann. Statist.} \textbf{22}, 980-994.

Navidi, W. (1989). Edgeworth expansions for bootstrapping regression
models. \emph{Ann. Statist.} \textbf{17} 4, 1472-1478.

Paparoditis, E. (1996). Bootstrapping autoregressive and moving average
parameters estimates of infinite order vector autoregressive processes.
\emph{J. Mult. Anal.} \textbf{57}, 277-296.

Parzen, E. (1962). On estimation of a probability density function and
mode. \emph{Ann. Math. Statist.} \textbf{33}, 1065--1076.

Pascual, L., Romo, J. and Ruiz, E. (2001). Effects of parameter
estimation on prediction densities: a bootstrap approach.
\emph{Int. J. Forecasting} \textbf{17}, 83-103

Pitman, E.J.G. (1937). Significance tests which may be applied to samples from any populations. \emph{Journal of the Royal Statistical Society}, Series B, 4, 119--130.

Politis, D.N. and Romano, J.R. (1994a). The stationary bootstrap.
\emph{J. Amer. Statist. Assoc.} \textbf{89}, 1303-1313.

Politis, D.N. and Romano, J.R. (1994b). Large sample confidence regions
based on subsamples under minimal assumptions. \emph{Ann. Statist.} \textbf{22},
2031-2050.

Politis, D.N. and Romano, J.R. (1994c). Limit theorems for weakly
dependent Hilbert space valued random variables with application to the
stationary bootstrap. \emph{Statist. Sin.} \textbf{4}, 461-476.

Politis, D.N., Romano, J.P. and Wolf, M. (1999). \href{https://books.google.es/books?hl=es\&lr=\&id=nGu6rqjE6JoC\&oi=fnd\&pg=PR7\&dq=Subsampling}{\emph{Subsampling}}.
Springer Verlag.

Prada-Sánchez, J.M. and Cotos-Yáñez, T. (1997). A Simulation Study of
Iterated and Non-iterated Bootstrap Methods for Bias Reduction and
Confidence Interval Estimation. \emph{Comm. Statist .-Simula.} \textbf{26} 3, 927-946.

Prada-Sánchez, J.M. and Otero-Cepeda, X.L. (1989). The use of smooth
bootstrap techniques for estimating the error rate of a prediction rule.
\emph{Comm. Statist .-Simula.} \textbf{18} 3, 1169-1186.

Quenouille, M. (1949). Approximate test of correlation in time series.
\emph{J. Roy. Statist. Soc. Ser. B} \textbf{11}, 18-84.

Radulović, D. (1996). The bootstrap for the mean of strong mixing
sequences under minimal conditions. \emph{Statist. Prob. Lett.} \textbf{28},
65-72.

Reid, N. (1981). Estimating the median survival time. \emph{Biometrika}
\textbf{68}, 601--608.

Rizzo, M.L. (2008). \emph{Statistical Computing with R}. Chapman\&Hall/CRC

Rosenblatt, M. (1956). Remarks on some nonparametric estimate of a
density function. \emph{Ann. Math. Statist.} \textbf{27}, 832--837.

Rubin, D.B. (1981). The Bayesian Bootstrap. \emph{Ann. Statist.} \textbf{9} 1,
130-134.

Rubinstein, R.Y. (1981). \href{https://books.google.es/books?hl=es\&lr=\&id=r2VODQAAQBAJ\&oi=fnd\&pg=PR1\&dq=Simulation+and+the+Monte+Carlo+Method}{\emph{Simulation and the Monte Carlo Method}}. Wiley.

Schucany, W., Gray, H. and Owen, O. (1971). On bias reduction in
estimation. \emph{J. Amer. Stat. Assoc.} \textbf{66}, 524-533.

Shao, J. (1999). \href{https://www.springer.com/gp/book/9780387953823}{\emph{Mathematical Statistics}}. Springer.

Shao, J. (2006). \href{http://www.stewartschultz.com/statistics/books/Mathematical\%20Statistics\%20-\%20Exercises\%20and\%20Solutions.pdf}{\emph{Mathematical Statistics: exercises and solutions}}. Springer.

Sheather, S.J. and Jones, M.C. (1991). A reliable data-based bandwidth
selection method for kernel density estimation.
\emph{Jr.~Royal Statist. Soc. Ser. B} \textbf{53}, 683--690.

Silverman, B.W. (1986). \href{http://users.stat.ufl.edu/~rrandles/sta6934/smhandout.pdf}{\emph{Density Estimation}}. Chapman and Hall.

Shao, J. and Tu, D. (1995). \href{https://books.google.es/books?hl=es\&lr=\&id=VO3SBwAAQBAJ\&oi=fnd\&pg=PA1\&dq=The+Jackknife+and+Bootstrap}{\emph{The Jackknife and Bootstrap}}. Springer Verlag.

Sing, K. (1981). On the asymptotic accuracy of Efron's bootstrap.
\emph{Ann. Statist.} \textbf{9} 6, 1187-1195.

Stine, R.A. (1987). Estimating properties of autoregressive forecasts.
\emph{J. Amer. Stat. Assoc.} \textbf{82}, 1072-1078.

Taylor, C. C. (1989). Bootstrap choice of the smoothing parameter in
kernel density estimation. \emph{Biometrika} \textbf{76}, 705--712.

Thombs, L.A. and Schucany, W.R. (1990). Bootstrap prediction intervals
for autoregression. \emph{J. Amer. Stat. Assoc.} \textbf{85}, 486-492.

Tukey, J. (1958). Bias and confidence in not quite large samples,
abstract, \emph{Ann. Math. Statist.} \textbf{29}, 614.

Wand M.P. and Jones M.C. (1995) \emph{Kernel Smoothing}. Chapman and Hall, London.

Watson, G.S. (1964). \href{https://www.jstor.org/stable/25049340?seq=1\#page_scan_tab_contents}{Smooth regression analysis}. \emph{Sankhy{ā}: The Indian Journal of Statistics Ser. A}
\textbf{26}, 359-372.

Welch, B.L. (1937). On the z‐test in randomized blocks and Latin squares. \emph{Biometrika}, 29, 21--52.

Wu, C.-F. J. (1986). Jackknife, bootstrap and other resampling methods
in regression analysis. \emph{Ann. Statist.} \textbf{14}, 1261--1350.

\hypertarget{appendix-apendices}{%
\appendix}


\hypertarget{links}{%
\chapter{Enlaces}\label{links}}

\textbf{Recursos para el aprendizaje de R} ( \url{https://rubenfcasal.github.io/post/ayuda-y-recursos-para-el-aprendizaje-de-r}
): A continuación se muestran algunos recursos que pueden ser útiles para el aprendizaje de R y la obtención de ayuda\ldots{}

\textbf{\emph{Ayuda online}}:

\begin{itemize}
\item
  Ayuda en línea sobre funciones o paquetes: \href{https://www.rdocumentation.org/}{RDocumentation}
\item
  Buscador \href{http://rseek.org/}{RSeek}
\item
  \href{http://stackoverflow.com/questions/tagged/r}{StackOverflow}
\end{itemize}

\textbf{\emph{Cursos}}:
algunos cursos gratuitos:

\begin{itemize}
\item
  \href{https://www.coursera.org/}{Coursera}:

  \begin{itemize}
  \item
    \href{https://www.coursera.org/learn/intro-data-science-programacion-estadistica-r}{Introducción a Data Science: Programación Estadística con R}
  \item
    \href{https://www.coursera.org/specializations/r}{Mastering Software Development in R}
  \end{itemize}
\end{itemize}

\begin{itemize}
\item
  \href{https://www.datacamp.com/courses}{DataCamp}:

  \begin{itemize}
  \tightlist
  \item
    \href{https://www.datacamp.com/courses/introduccion-a-r/}{Introducción a R}
  \end{itemize}
\end{itemize}

\begin{itemize}
\item
  \href{http://online.stanford.edu/courses}{Stanford online}:

  \begin{itemize}
  \tightlist
  \item
    \href{http://online.stanford.edu/course/statistical-learning}{Statistical Learning}
  \end{itemize}
\end{itemize}

\begin{itemize}
\tightlist
\item
  Curso UCA: \href{http://knuth.uca.es/moodle/course/view.php?id=51}{Introducción a R, R-commander y shiny}
\end{itemize}

\begin{itemize}
\tightlist
\item
  Udacity: \href{https://eu.udacity.com/course/data-analysis-with-r--ud651}{Data Analysis with R}
\end{itemize}

\begin{itemize}
\tightlist
\item
  \href{https://swirlstats.com/scn/title.html}{Swirl Courses}:
  se pueden hacer cursos desde el propio R con el paquete
  \href{https://swirlstats.com}{swirl}.
\end{itemize}

Para información sobre cursos en castellano se puede recurrir a la web de \href{http://r-es.org/}{R-Hispano} en el apartado \href{http://r-es.org/category/formacion}{formación}. Algunos de los cursos que aparecen en entradas antiguas son gratuitos.
Ver: \href{http://r-es.org/2016/02/12/cursos-masivos-y-otra-formacion-on-line-sobre-r/}{Cursos MOOC relacionados con R}.

\textbf{\emph{Libros}}

\begin{itemize}
\item
  \textbf{\emph{Iniciación}}:

  \begin{itemize}
  \item
    2011 - The Art of R Programming. A Tour of Statistical Software Design,
    (\href{https://www.nostarch.com/artofr.htm}{No Starch Press})
  \item
    R for Data Science
    (\href{http://r4ds.had.co.nz/}{online}, \href{http://shop.oreilly.com/product/0636920034407.do}{O'Reilly})
  \item
    Hands-On Programming with R: Write Your Own Functions and Simulations,
    by Garrett Grolemund
    (\href{http://shop.oreilly.com/product/0636920028574.do}{O'Reilly})
  \end{itemize}
\item
  \textbf{\emph{Avanzados}}:

  \begin{itemize}
  \item
    2008 - Software for Data Analysis: Programming with R - Chambers
    (\href{http://www.springer.com/la/book/9780387759357}{Springer})
  \item
    Advanced R by Hadley Wickham
    (online: \href{http://adv-r.had.co.nz/}{1ª ed},
    \href{https://adv-r.hadley.nz/}{2ª ed},
    \href{https://www.amazon.com/dp/1466586966}{Chapman \& Hall})
  \item
    R packages by Hadley Wickham
    (\href{http://r-pkgs.had.co.nz/}{online},
    \href{http://shop.oreilly.com/product/0636920034421.do}{O'Reilly})
  \end{itemize}
\item
  \textbf{\emph{Bookdown}}:
  el paquete \href{https://bookdown.org}{\texttt{bookdown}} de R permite escribir libros empleando
  \href{http://rmarkdown.rstudio.com}{R Markdown} y compartirlos.
  En \url{https://bookdown.org} está disponible una selección de libros escritos con este paquete
  (un listado más completo está disponible \href{https://bookdown.org/home/archive/}{aquí}).
  Algunos libros en este formato en castellano son:

  \begin{itemize}
  \item
    \href{https://rubenfcasal.github.io/simbook}{Prácticas de Simulación}
    (disponible en el repositorio de GitHub
    \href{https://github.com/rubenfcasal/simbook}{rubenfcasal/simbook}).
  \item
    \href{https://rubenfcasal.github.io/bookdown_intro/}{Escritura de libros con bookdown}
    (disponible en el repositorio de GitHub
    \href{https://github.com/rubenfcasal/bookdown_intro}{rubenfcasal/bookdown\_intro}).
  \item
    \href{https://www.datanalytics.com/libro_r/index.html}{R para profesionales de los datos: una introducción}.
  \item
    \href{https://bookdown.org/aquintela/EBE}{Estadística Básica Edulcorada}.
  \end{itemize}
\end{itemize}

\textbf{\emph{Material online}}:
en la web se puede encontrar mucho material adicional, por ejemplo:

\begin{itemize}
\item
  \href{https://www.r-project.org/other-docs.html}{CRAN: Other R documentation}
\item
  \href{https://www.rstudio.com}{RStudio}:
  \href{https://www.rstudio.com/online-learning}{Online learning},
  \href{https://resources.rstudio.com/webinars}{Webinars},
  \href{http://shiny.rstudio.com}{shiny},
  \href{https://www.tidyverse.org/}{tidyverse}.

  \begin{itemize}
  \tightlist
  \item
    \href{https://resources.rstudio.com/rstudio-cheatsheets}{CheatSheets}:
    \href{https://resources.rstudio.com/rstudio-cheatsheets/rmarkdown-2-0-cheat-sheet}{RMarkdown},
    \href{https://resources.rstudio.com/rstudio-cheatsheets/shiny-cheat-sheet}{Shiny},
    \href{https://github.com/rstudio/cheatsheets/blob/master/data-transformation.pdf}{dplyr},
    \href{https://github.com/rstudio/cheatsheets/blob/master/data-import.pdf}{tidyr},
    \href{https://resources.rstudio.com/rstudio-cheatsheets/stringr-cheat-sheet}{stringr}.
  \end{itemize}
\item
  Blogs en inglés:

  \begin{itemize}
  \item
    \url{https://www.r-bloggers.com/}
  \item
    \url{https://www.littlemissdata.com/blog/rstudioconf2019}
  \item
    RStudio: \url{https://blog.rstudio.com}
  \item
    Microsoft Revolutions: \url{https://blog.revolutionanalytics.com}
  \end{itemize}
\item
  Blogs en castellano:

  \begin{itemize}
  \item
    \url{https://www.datanalytics.com}
  \item
    \url{http://oscarperpinan.github.io/R}
  \item
    \url{http://rubenfcasal.github.io}
  \end{itemize}
\item
  Listas de correo:

  \begin{itemize}
  \item
    Listas de distribución de r-project.org: \url{https://stat.ethz.ch/mailman/listinfo}
  \item
    Búsqueda en R-help: \url{http://r.789695.n4.nabble.com/R-help-f789696.html}
  \item
    Búsqueda en R-help-es: \url{https://r-help-es.r-project.narkive.com}

    \url{https://grokbase.com/g/r/r-help-es}
  \item
    Archivos de R-help-es: \url{https://stat.ethz.ch/pipermail/r-help-es}
  \end{itemize}
\end{itemize}

\hypertarget{forecast-links}{%
\section{Forecasting: Principles and Practice}\label{forecast-links}}

\href{https://otexts.com/fpp2}{Forecasting: Principles and Practice, 2ª ed}
by Rob J. Hyndman and George Athanasopoulos:

\begin{itemize}
\item
  \href{https://otexts.com/fpp2/simple-methods.html}{3.1 Some simple forecasting methods}
\item
  \href{https://otexts.com/fpp2/the-forecast-package-in-r.html}{3.6 The forecast package in R}
\item
  \href{https://otexts.com/fpp2/bootstrap.html}{11.4 Bootstrapping and bagging}
\item
  \href{https://otexts.com/fpp2/appendix-for-instructors.html}{Appendix: For instructors}
\end{itemize}

\hypertarget{spatial-links}{%
\section{Spatial data}\label{spatial-links}}

\begin{itemize}
\item
  \href{https://rubenfcasal.github.io/simbook}{Apuntes de simulación}:

  \begin{itemize}
  \item
    \href{https://rubenfcasal.github.io/simbook/simulacion-de-distribuciones-multidimensionales.html}{9 Simulación de Distribuciones Multidimensionales}
  \item
    \href{https://rubenfcasal.github.io/simbook/simulacion-de-distribuciones-multidimensionales.html\#simulacion-condicional-e-incondicional}{9.3 Simulación condicional e incondicional}
  \end{itemize}
\item
  Castillo-Páez, S., Fernández-Casal, R., García-Soidán, P.
  \href{https://www.sciencedirect.com/science/article/pii/S0167947319300325?via\%3Dihub}{A nonparametric bootstrap method for spatial data},
  Computational Statistics and Data Analysis, 137 (2019) 1-15.
\item
  \href{./Poster_METMA9_2.pdf}{Poster bootstrap condicional (pdf)}
\item
  \href{https://rubenfcasal.github.io/npsp/index.html}{npsp}, \href{https://rubenfcasal.github.io/post/geoestadistica-no-parametrica-con-el-paquete-npsp/}{post en castellano}.
\item
  \href{https://rubenfcasal.github.io/Geoestadistica_espacio-temporal.pdf}{Tesis geoestadística espacio-temporal (pdf)}
\end{itemize}

\hypertarget{intro-hpc}{%
\chapter{Introducción al procesamiento en paralelo en R}\label{intro-hpc}}

En este apéndice se pretenden mostrar las principales herramientas para el procesamiento en paralelo disponibles en R y dar una idea de su funcionamiento.
Para más detalles se recomienda ver \href{https://cran.r-project.org/view=HighPerformanceComputing}{CRAN Task View: High-Performance and Parallel Computing with R}
(además de HPC, High Performance Computing, también incluye herramientas
para computación distribuida)\footnote{También puede ser de interés la presentación \href{https://www.r-users.gal/sites/default/files/10_aurelio_rodriguez.pdf}{R y HPC (uso de R en el CESGA)} de Aurelio Rodríguez en las \href{https://www.r-users.gal/pagina/programa-2018}{VI Xornada de Usuarios de R en Galicia}}.

\hypertarget{introducciuxf3n-1}{%
\section{Introducción}\label{introducciuxf3n-1}}

Emplearemos la siguiente terminología:

\begin{itemize}
\item
  \textbf{Núcleo}: término empleado para referirse a un procesador lógico de un equipo
  (un equipo puede tener un único procesador con múltiples núcleos que pueden
  realizar operaciones en paralelo). También podría referirse aquí a un
  equipo (nodo) de una red (clúster de equipos; en la práctica cada uno puede tener
  múltiples núcleos). \emph{Un núcleo puede ejecutar procesos en serie}.
\item
  \textbf{Clúster}: colección de núcleos en un equipo o red de equipos.
  \emph{Un clúster puede ejecutar varios procesos en paralelo}.
\end{itemize}

Por defecto la versión oficial de R emplea un único núcleo, aunque se puede compilar
de forma que realice cálculos en paralelo (e.g.~librería LAPACK).
También están disponibles otras versiones de R que ya implementan por defecto
procesamiento en paralelo (\href{https://mran.revolutionanalytics.com/documents/rro/multithread}{multithread}):

\begin{itemize}
\tightlist
\item
  \href{https://mran.revolutionanalytics.com}{Microsoft R Open}: versión de R con rendimiento mejorado.
\end{itemize}

Métodos simples de paralelización\footnote{Realmente las herramientas estándar son
  \emph{OpenMP} para el procesamiento en paralelo con memoria compartida en un único equipo
  y \emph{MPI} para la computación distribuida en múltiples nodos.}:

\begin{itemize}
\item
  \emph{Forking}: Copia el proceso de R a un nuevo núcleo (se comparte el entorno de trabajo).
  Es el más simple y eficiente pero \textbf{no está disponible en Windows}.
\item
  \emph{Socket}: Lanza una nueva versión de R en cada núcleo, como si se tratase de un cluster
  de equipos comunicados a traves de red (hay que crear un entorno de trabajo en cada núcleo).
  Disponible en todos los sistemas operativos.
\end{itemize}

\hypertarget{paquetes-en-r}{%
\section{Paquetes en R}\label{paquetes-en-r}}

Hay varios paquetes que se pueden usar para el procesamiento paralelo en R,
entre ellos podríamos destacar:

\begin{itemize}
\item
  \texttt{parallel}: forma parte de la instalación base de R y fusiona los paquetes
  \texttt{multicore} (forking) y \texttt{snow} (sockets; Simple Network of Workstations).
  Además incluye herramientas para la generación de números aleatorios en paralelo
  (cada proceso empleara una secuencia y los resultados serán reproducibles).

  Incluye versiones ``paralelizadas'' de la familia \texttt{*apply()}:
  \texttt{mclapply()} (forking), \texttt{parLapply()} (socket), \ldots{}
\item
  \texttt{foreach}: permite realizar iteraciones y admite paralelización con el operador \texttt{\%dopar\%},
  aunque requiere paquetes adicionales como \texttt{doSNOW} o \texttt{doParallel} (recomendado).
\item
  \texttt{rslurm}: permite la ejecución distribuida en clústeres Linux que implementen
  SLURM (Simple Linux Utility for Resource Management),
  un gestor de recursos de código abierto muy empleado.
\end{itemize}

\hypertarget{ejemplos-3}{%
\section{Ejemplos}\label{ejemplos-3}}

Si se emplea el paquete \texttt{parallel} en sistemas tipo Unix (Linux, Mac OS X, \ldots), se podría
evaluar en paralelo una función llamando directamente a \texttt{mclapply()}.
Por defecto empleará todos los núcleos disponibles, pero se puede especificar un número menor
mediante el argumento \texttt{mc.cores}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(parallel)}
\NormalTok{ncores }\OtherTok{\textless{}{-}} \FunctionTok{detectCores}\NormalTok{()}
\NormalTok{ncores}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 8
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{func }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(k) \{}
\NormalTok{  i\_boot }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(iris), }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}
  \FunctionTok{lm}\NormalTok{(Petal.Width }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Petal.Length, }\AttributeTok{data =}\NormalTok{ iris[i\_boot, ])}\SpecialCharTok{$}\NormalTok{coefficients}
\NormalTok{\}}

\FunctionTok{RNGkind}\NormalTok{(}\StringTok{"L\textquotesingle{}Ecuyer{-}CMRG"}\NormalTok{) }\CommentTok{\# Establecemos Pierre L\textquotesingle{}Ecuyer\textquotesingle{}s RngStreams...}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\FunctionTok{system.time}\NormalTok{(res.boot }\OtherTok{\textless{}{-}} \FunctionTok{mclapply}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{100}\NormalTok{, func)) }\CommentTok{\# En Windows llama a lapply() (mc.cores = 1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    user  system elapsed 
##    0.06    0.00    0.06
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# res.boot \textless{}{-} mclapply(1:100, func, mc.cores = ncores {-} 1) \# En Windows genera un error si mc.cores \textgreater{} 1}
\end{Highlighting}
\end{Shaded}

En Windows habría que crear previamente un cluster, llamar a una de las funciones
\texttt{par*apply()} y finalizar el cluster:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cl }\OtherTok{\textless{}{-}} \FunctionTok{makeCluster}\NormalTok{(ncores }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{, }\AttributeTok{type =} \StringTok{"PSOCK"}\NormalTok{)}
\FunctionTok{clusterSetRNGStream}\NormalTok{(cl, }\DecValTok{1}\NormalTok{) }\CommentTok{\# Establecemos Pierre L\textquotesingle{}Ecuyer\textquotesingle{}s RngStreams con semilla 1...}

\FunctionTok{system.time}\NormalTok{(res.boot }\OtherTok{\textless{}{-}} \FunctionTok{parSapply}\NormalTok{(cl, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{100}\NormalTok{, func))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    user  system elapsed 
##    0.01    0.00    0.06
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# stopCluster(cl)}

\FunctionTok{str}\NormalTok{(res.boot)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  num [1:2, 1:100] -0.415 0.429 -0.363 0.42 -0.342 ...
##  - attr(*, "dimnames")=List of 2
##   ..$ : chr [1:2] "(Intercept)" "Petal.Length"
##   ..$ : NULL
\end{verbatim}

Esto también se puede realizar en Linux (\texttt{type\ =\ "FORK"}),
aunque podríamos estar trabajando ya en un cluster de equipos\ldots{}

También podríamos emplear balance de carga si el tiempo de computación es variable
(e.g.~\texttt{parLapplyLB()} o \texttt{clusterApplyLB()}) pero no sería recomendable si se emplean
números pseudo-aleatorios (los resultados no serían reproducibles).

Además, empleando las herramientas del paquete \texttt{snow} se puede representar el uso
del cluster (\emph{experimental} en Windows):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# library(snow)}
\NormalTok{ctime }\OtherTok{\textless{}{-}}\NormalTok{ snow}\SpecialCharTok{::}\FunctionTok{snow.time}\NormalTok{(snow}\SpecialCharTok{::}\FunctionTok{parSapply}\NormalTok{(cl, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{100}\NormalTok{, func))}
\NormalTok{ctime}
\FunctionTok{plot}\NormalTok{(ctime)}
\end{Highlighting}
\end{Shaded}

Hay que tener en cuenta la sobrecarga adicional debida a la comunicación entre nodos
al paralelizar (especialmente con el enfoque de socket).

\hypertarget{procesamiento-en-paralelo-con-la-funciuxf3n-boot}{%
\subsection{\texorpdfstring{Procesamiento en paralelo con la función \texttt{boot()}}{Procesamiento en paralelo con la función boot()}}\label{procesamiento-en-paralelo-con-la-funciuxf3n-boot}}

La función \texttt{boot::boot()} incluye parámetros para el procesamiento en paralelo:
\texttt{parallel\ =\ c("no",\ "multicore",\ "snow")}, \texttt{ncpus}, \texttt{cl}.
Si \texttt{parallel\ =\ "snow"} se crea un clúster en la máquina local durante la ejecución,
salvo que se establezca con el parámetro \texttt{cl}.

Veamos un ejemplo empleando una muestra simulada:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{100}
\NormalTok{rate }\OtherTok{\textless{}{-}} \FloatTok{0.01}
\NormalTok{mu }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{/}\NormalTok{rate}
\NormalTok{muestra }\OtherTok{\textless{}{-}} \FunctionTok{rexp}\NormalTok{(n, }\AttributeTok{rate =}\NormalTok{ rate)}
\NormalTok{media }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(muestra)}
\NormalTok{desv }\OtherTok{\textless{}{-}} \FunctionTok{sd}\NormalTok{(muestra)}

\FunctionTok{library}\NormalTok{(boot)}
\NormalTok{statistic }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, i)\{}
\NormalTok{  remuestra }\OtherTok{\textless{}{-}}\NormalTok{ data[i]}
  \FunctionTok{c}\NormalTok{(}\FunctionTok{mean}\NormalTok{(remuestra), }\FunctionTok{var}\NormalTok{(remuestra)}\SpecialCharTok{/}\FunctionTok{length}\NormalTok{(remuestra))}
\NormalTok{\}}
\NormalTok{B }\OtherTok{\textless{}{-}} \DecValTok{2000}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\FunctionTok{system.time}\NormalTok{(res.boot }\OtherTok{\textless{}{-}} \FunctionTok{boot}\NormalTok{(muestra, statistic, }\AttributeTok{R =}\NormalTok{ B))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    user  system elapsed 
##    0.07    0.00    0.08
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# system.time(res.boot \textless{}{-} boot(muestra, statistic, R = B, parallel = "snow"))}
\FunctionTok{system.time}\NormalTok{(res.boot }\OtherTok{\textless{}{-}} \FunctionTok{boot}\NormalTok{(muestra, statistic, }\AttributeTok{R =}\NormalTok{ B, }\AttributeTok{parallel =} \StringTok{"snow"}\NormalTok{, }\AttributeTok{cl =}\NormalTok{ cl))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    user  system elapsed 
##    0.05    0.00    0.04
\end{verbatim}

\hypertarget{estudio-sim-boot}{%
\subsection{Estudio de simulación}\label{estudio-sim-boot}}

Si se trata de un estudio más complejo, como por ejemplo un estudio de simulación
en el que se emplea bootstrap, la recomendación sería tratar de paralelizar
en el nivel superior para minimizar la sobrecarga debida a la comunicación
entre nodos.

Por ejemplo, a continuación se realiza un estudio similar al mostrado en la Sección \ref{estudio-sim-exp}
pero comparando las probabilidades de cobertura y las longitudes de los
intervalos de confianza implementados en la función \texttt{boot.ci()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{t.ini }\OtherTok{\textless{}{-}} \FunctionTok{proc.time}\NormalTok{()}

\NormalTok{nsim }\OtherTok{\textless{}{-}} \DecValTok{500}

\NormalTok{getSimulation }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(isim, }\AttributeTok{B =} \DecValTok{2000}\NormalTok{, }\AttributeTok{n =} \DecValTok{30}\NormalTok{, }\AttributeTok{alfa =} \FloatTok{0.1}\NormalTok{, }\AttributeTok{mu =} \DecValTok{100}\NormalTok{) \{}
\NormalTok{    rate }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{/}\NormalTok{mu }\CommentTok{\# 0.01}
\NormalTok{    resnames }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Cobertura"}\NormalTok{, }\StringTok{"Longitud"}\NormalTok{)}
    \CommentTok{\# intervals \textless{}{-} c("Normal", "Percentil", "Percentil{-}t", "Percentil{-}t simetrizado")}
\NormalTok{    intervals }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Normal"}\NormalTok{, }\StringTok{"Basic"}\NormalTok{, }\StringTok{"Studentized"}\NormalTok{, }\StringTok{"Percentil"}\NormalTok{, }\StringTok{"BCa"}\NormalTok{)}
    \FunctionTok{names}\NormalTok{(intervals) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"normal"}\NormalTok{,}\StringTok{"basic"}\NormalTok{, }\StringTok{"student"}\NormalTok{, }\StringTok{"percent"}\NormalTok{, }\StringTok{"bca"}\NormalTok{)}
\NormalTok{    intervals }\OtherTok{\textless{}{-}}\NormalTok{ intervals[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{]}
\NormalTok{    resultados }\OtherTok{\textless{}{-}} \FunctionTok{array}\NormalTok{(}\AttributeTok{dim =} \FunctionTok{c}\NormalTok{(}\FunctionTok{length}\NormalTok{(resnames), }\FunctionTok{length}\NormalTok{(intervals)))}
    \FunctionTok{dimnames}\NormalTok{(resultados) }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(resnames, intervals)}
    \CommentTok{\# for (isim in 1:nsim) \{ \# isim \textless{}{-} 1}
\NormalTok{    muestra }\OtherTok{\textless{}{-}} \FunctionTok{rexp}\NormalTok{(n, }\AttributeTok{rate =} \FloatTok{0.01}\NormalTok{)}
\NormalTok{    media }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(muestra)}
\NormalTok{    desv }\OtherTok{\textless{}{-}} \FunctionTok{sd}\NormalTok{(muestra)}
    \CommentTok{\# boot()}
    \FunctionTok{library}\NormalTok{(boot)}
\NormalTok{    statistic }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, i)\{}
\NormalTok{      remuestra }\OtherTok{\textless{}{-}}\NormalTok{ data[i]}
      \FunctionTok{c}\NormalTok{(}\FunctionTok{mean}\NormalTok{(remuestra), }\FunctionTok{var}\NormalTok{(remuestra)}\SpecialCharTok{/}\FunctionTok{length}\NormalTok{(remuestra))}
\NormalTok{    \}}
\NormalTok{    res.boot }\OtherTok{\textless{}{-}} \FunctionTok{boot}\NormalTok{(muestra, statistic, }\AttributeTok{R =}\NormalTok{ B)}
\NormalTok{    res }\OtherTok{\textless{}{-}} \FunctionTok{boot.ci}\NormalTok{(res.boot, }\AttributeTok{conf =} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alfa)}
    \CommentTok{\# Intervalos}
\NormalTok{    res }\OtherTok{\textless{}{-}} \FunctionTok{sapply}\NormalTok{(res[}\FunctionTok{names}\NormalTok{(intervals)], }\ControlFlowTok{function}\NormalTok{(x) \{}
\NormalTok{      l }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(x)}
\NormalTok{      x[}\FunctionTok{c}\NormalTok{(l}\DecValTok{{-}1}\NormalTok{, l)]}
\NormalTok{    \})}
    \CommentTok{\# resultados}
\NormalTok{    resultados[}\DecValTok{1}\NormalTok{, ] }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(res, }\DecValTok{2}\NormalTok{,}
                                   \ControlFlowTok{function}\NormalTok{(ic) (ic[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{\textless{}}\NormalTok{ mu) }\SpecialCharTok{\&\&}\NormalTok{ (mu }\SpecialCharTok{\textless{}}\NormalTok{ ic[}\DecValTok{2}\NormalTok{])) }\CommentTok{\# Cobertura}
\NormalTok{    resultados[}\DecValTok{2}\NormalTok{, ] }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(res, }\DecValTok{2}\NormalTok{, diff) }\CommentTok{\# Longitud}
\NormalTok{    resultados}
\NormalTok{\}}

\NormalTok{parallel}\SpecialCharTok{::}\FunctionTok{clusterSetRNGStream}\NormalTok{(cl)}
\NormalTok{result }\OtherTok{\textless{}{-}} \FunctionTok{parLapply}\NormalTok{(cl, }\DecValTok{1}\SpecialCharTok{:}\NormalTok{nsim, getSimulation)}
\CommentTok{\# stopCluster(cl)}

\CommentTok{\# result}
\NormalTok{t.fin }\OtherTok{\textless{}{-}} \FunctionTok{proc.time}\NormalTok{() }\SpecialCharTok{{-}}\NormalTok{ t.ini}
\FunctionTok{print}\NormalTok{(t.fin)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    user  system elapsed 
##    0.03    0.00    9.80
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{resnames }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Cobertura"}\NormalTok{, }\StringTok{"Longitud"}\NormalTok{)}
\NormalTok{intervals }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Normal"}\NormalTok{, }\StringTok{"Basic"}\NormalTok{, }\StringTok{"Studentized"}\NormalTok{, }\StringTok{"Percentil"}\NormalTok{, }\StringTok{"BCa"}\NormalTok{)}
\FunctionTok{names}\NormalTok{(intervals) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"normal"}\NormalTok{,}\StringTok{"basic"}\NormalTok{, }\StringTok{"student"}\NormalTok{, }\StringTok{"percent"}\NormalTok{, }\StringTok{"bca"}\NormalTok{)}
\NormalTok{intervals }\OtherTok{\textless{}{-}}\NormalTok{ intervals[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{]}
\NormalTok{resultados }\OtherTok{\textless{}{-}} \FunctionTok{sapply}\NormalTok{(result, }\ControlFlowTok{function}\NormalTok{(x) x)}
\FunctionTok{dim}\NormalTok{(resultados) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{length}\NormalTok{(resnames), }\FunctionTok{length}\NormalTok{(intervals), nsim)}
\FunctionTok{dimnames}\NormalTok{(resultados) }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(resnames, intervals, }\ConstantTok{NULL}\NormalTok{)}

\NormalTok{res }\OtherTok{\textless{}{-}} \FunctionTok{t}\NormalTok{(}\FunctionTok{apply}\NormalTok{(resultados, }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{), mean))}
\NormalTok{res}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             Cobertura Longitud
## Normal          0.866 57.05639
## Basic           0.860 56.97389
## Studentized     0.900 65.72609
## Percentil       0.868 56.97389
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(res, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|r|r}
\hline
  & Cobertura & Longitud\\
\hline
Normal & 0.866 & 57.056\\
\hline
Basic & 0.860 & 56.974\\
\hline
Studentized & 0.900 & 65.726\\
\hline
Percentil & 0.868 & 56.974\\
\hline
\end{tabular}

El último paso es finalizar el cluster:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{stopCluster}\NormalTok{(cl)}
\end{Highlighting}
\end{Shaded}


  \bibliography{book.bib,packages.bib}

\end{document}
